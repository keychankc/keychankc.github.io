<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":272,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. 从早期跨模态学习到 CLIP：视觉和语言对齐是如何发展的在 CLIP 出现之前，让计算机同时“看懂图像、理解文字”，其实已经是一个被研究了很多年的问题。只要稍微想一想就能明白：如果模型既能处理图像，又能处理语言，那它就有可能完成图文搜索、图片描述、视觉问答等各种任务，这听起来非常有吸引力。 也正因为如此，计算机视觉和自然语言处理领域的研究者，早早就开始尝试“跨模态学习”，也就是把图像和文本放">
<meta property="og:type" content="article">
<meta property="og:title" content="视觉多模态：CLIP、ALIGN 与视觉语言对齐">
<meta property="og:url" content="https://www.keychan.xyz/2026/02/10/056-visual-multimodal/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. 从早期跨模态学习到 CLIP：视觉和语言对齐是如何发展的在 CLIP 出现之前，让计算机同时“看懂图像、理解文字”，其实已经是一个被研究了很多年的问题。只要稍微想一想就能明白：如果模型既能处理图像，又能处理语言，那它就有可能完成图文搜索、图片描述、视觉问答等各种任务，这听起来非常有吸引力。 也正因为如此，计算机视觉和自然语言处理领域的研究者，早早就开始尝试“跨模态学习”，也就是把图像和文本放">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/IQ8kvH.png">
<meta property="article:published_time" content="2026-02-10T13:20:12.000Z">
<meta property="article:modified_time" content="2026-02-10T13:48:36.145Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="对比学习">
<meta property="article:tag" content="CLIP">
<meta property="article:tag" content="ALIGN">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="RAG-Vision">
<meta property="article:tag" content="系统级接口">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/IQ8kvH.png">


<link rel="canonical" href="https://www.keychan.xyz/2026/02/10/056-visual-multimodal/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2026/02/10/056-visual-multimodal/","path":"2026/02/10/056-visual-multimodal/","title":"视觉多模态：CLIP、ALIGN 与视觉语言对齐"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>视觉多模态：CLIP、ALIGN 与视觉语言对齐 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%8E%E6%97%A9%E6%9C%9F%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E5%88%B0-CLIP%EF%BC%9A%E8%A7%86%E8%A7%89%E5%92%8C%E8%AF%AD%E8%A8%80%E5%AF%B9%E9%BD%90%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%91%E5%B1%95%E7%9A%84"><span class="nav-text">1. 从早期跨模态学习到 CLIP：视觉和语言对齐是如何发展的</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-CLIP-%E5%87%BA%E7%8E%B0%E4%B9%8B%E5%89%8D%EF%BC%8C%E5%A6%82%E4%BD%95%E6%8A%8A%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%85%B3%E8%81%94%E8%B5%B7%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-text">1.1 CLIP 出现之前，如何把图像和文字关联起来的？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%E8%B7%AF%E7%BA%BF%EF%BC%9A%E6%8A%8A%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E2%80%9C%E6%94%BE%E8%BF%9B%E5%90%8C%E4%B8%80%E4%B8%AA%E7%A9%BA%E9%97%B4%E2%80%9D"><span class="nav-text">1.1.1 双编码器路线：把图像和文字“放进同一个空间”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-%E8%9E%8D%E5%90%88%E4%BA%A4%E4%BA%92%E8%B7%AF%E7%BA%BF%EF%BC%9A%E8%AE%A9%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%9C%A8%E6%A8%A1%E5%9E%8B%E5%86%85%E9%83%A8%E2%80%9C%E6%B7%B1%E5%BA%A6%E4%BA%A4%E4%BA%92%E2%80%9D"><span class="nav-text">1.1.2 融合交互路线：让图像和文字在模型内部“深度交互”</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9B%E6%97%A9%E6%9C%9F%E6%96%B9%E6%B3%95%EF%BC%8C%E5%A7%8B%E7%BB%88%E6%B2%A1%E8%83%BD%E5%8F%98%E6%88%90%E2%80%9C%E9%80%9A%E7%94%A8%E8%83%BD%E5%8A%9B%E2%80%9D%EF%BC%9F"><span class="nav-text">1.2 为什么这些早期方法，始终没能变成“通用能力”？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-%E6%96%87%E6%9C%AC%E6%9B%B4%E5%83%8F%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E8%AF%AD%E8%A8%80%E6%9C%AC%E8%BA%AB"><span class="nav-text">1.2.1 文本更像“标签”，而不是语言本身</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-%E5%AD%A6%E5%88%B0%E7%9A%84%E8%A1%A8%E7%A4%BA%E4%B8%8D%E5%A4%9F%E7%A8%B3%E5%AE%9A%EF%BC%8C%E9%9A%BE%E4%BB%A5%E8%B7%A8%E4%BB%BB%E5%8A%A1%E5%A4%8D%E7%94%A8"><span class="nav-text">1.2.2 学到的表示不够稳定，难以跨任务复用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-3-%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E2%80%9C%E5%B0%81%E9%97%AD%E4%B8%96%E7%95%8C%E2%80%9D%E7%9A%84"><span class="nav-text">1.2.3 训练目标本质上是“封闭世界”的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-4-%E7%B3%BB%E7%BB%9F%E5%B1%82%E9%9D%A2%E5%B9%B6%E4%B8%8D%E5%8F%8B%E5%A5%BD"><span class="nav-text">1.2.4 系统层面并不友好</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-CLIP-%E5%87%BA%E7%8E%B0%E7%9A%84%E8%83%8C%E6%99%AF%EF%BC%9A%E4%B8%8D%E6%98%AF%E9%87%8D%E6%96%B0%E5%8F%91%E6%98%8E%E8%BD%AE%E5%AD%90%EF%BC%8C%E8%80%8C%E6%98%AF%E6%8D%A2%E4%BA%86%E4%B8%80%E7%A7%8D%E9%97%AE%E6%B3%95"><span class="nav-text">1.3 CLIP 出现的背景：不是重新发明轮子，而是换了一种问法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%9ACLIP-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%97%A2%E7%AE%80%E5%8D%95%E5%8F%88%E5%BC%BA%E5%A4%A7%EF%BC%9F"><span class="nav-text">1.4 双塔结构 + 对比学习：CLIP 为什么既简单又强大？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-CLIP-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E5%9B%BE%E5%83%8F%E2%80%93%E8%AF%AD%E8%A8%80%E5%AF%B9%E9%BD%90%E2%80%9D%E8%83%BD%E5%8F%98%E6%88%90%E9%80%9A%E7%94%A8%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="nav-text">2. CLIP 的核心机制：为什么“图像–语言对齐”能变成通用表示？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E4%BB%8E%E2%80%9C%E9%80%89%E7%AD%94%E6%A1%88%E2%80%9D%E5%88%B0%E2%80%9C%E7%9C%8B%E9%85%8D%E4%B8%8D%E9%85%8D%E2%80%9D%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%E5%8F%98%E5%8C%96%EF%BC%9F"><span class="nav-text">2.1 从“选答案”到“看配不配”：训练目标发生了什么变化？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E5%8C%B9%E9%85%8D%E2%80%9D%E6%AF%94%E2%80%9C%E5%88%86%E7%B1%BB%E2%80%9D%E6%9B%B4%E5%BC%80%E6%94%BE%EF%BC%9F"><span class="nav-text">为什么“匹配”比“分类”更开放？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E5%B0%91%E4%BA%A4%E4%BA%92%E2%80%9D%E5%8F%8D%E8%80%8C%E6%9B%B4%E9%80%9A%E7%94%A8%EF%BC%9F"><span class="nav-text">2.2 双塔结构：为什么“少交互”反而更通用？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-text">双塔结构解决了什么问题？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%9ACLIP-%E6%98%AF%E5%A6%82%E4%BD%95%E2%80%9C%E5%A1%91%E9%80%A0%E2%80%9D%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4%E7%9A%84%EF%BC%9F"><span class="nav-text">2.3 对比学习：CLIP 是如何“塑造”语义空间的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%92%8C%E6%B8%A9%E5%BA%A6%EF%BC%9A%E7%9C%8B%E4%BC%BC%E7%BB%86%E8%8A%82%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%BE%88%E5%85%B3%E9%94%AE"><span class="nav-text">2.4 相似度和温度：看似细节，其实很关键</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Zero-shot-%E8%83%BD%E5%8A%9B%EF%BC%9A%E4%B8%8D%E6%98%AF%E9%AD%94%E6%B3%95%EF%BC%8C%E8%80%8C%E6%98%AF%E7%BB%93%E6%9E%84%E7%9A%84%E8%87%AA%E7%84%B6%E7%BB%93%E6%9E%9C"><span class="nav-text">2.5 Zero-shot 能力：不是魔法，而是结构的自然结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Zero-shot-%E6%B3%9B%E5%8C%96%EF%BC%9A%E8%AF%AD%E8%A8%80%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%8F%98%E6%88%90%E5%BC%80%E6%94%BE%E7%9A%84%E2%80%9C%E8%AF%AD%E4%B9%89%E6%8E%A5%E5%8F%A3%E2%80%9D%EF%BC%9F"><span class="nav-text">3. Zero-shot 泛化：语言为什么能变成开放的“语义接口”？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Zero-shot-%E5%88%86%E7%B1%BB%E4%B8%8D%E6%98%AF%E2%80%9C%E9%A2%9D%E5%A4%96%E7%9A%84%E6%8A%80%E8%83%BD%E2%80%9D"><span class="nav-text">3.1 Zero-shot 分类不是“额外的技能”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%AD%E8%A8%80%E8%83%BD%E5%BD%93%E2%80%9C%E5%BC%80%E6%94%BE%E8%AF%AD%E4%B9%89%E6%8E%A5%E5%8F%A3%E2%80%9D%EF%BC%9F"><span class="nav-text">3.2 为什么语言能当“开放语义接口”？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E8%AF%AD%E8%A8%80%E5%8F%AF%E4%BB%A5%E6%97%A0%E9%99%90%E7%BB%84%E5%90%88"><span class="nav-text">3.2.1 语言可以无限组合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-%E5%90%8C%E4%B8%80%E4%BB%B6%E4%BA%8B%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%BE%88%E5%A4%9A%E7%A7%8D%E8%AF%B4%E6%B3%95"><span class="nav-text">3.2.2 同一件事可以有很多种说法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-%E8%AF%AD%E8%A8%80%E8%83%BD%E6%8F%8F%E8%BF%B0%E7%9A%84%E5%B1%82%E6%AC%A1%E5%BE%88%E4%B8%B0%E5%AF%8C"><span class="nav-text">3.2.3 语言能描述的层次很丰富</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Prompt%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8D%A2%E4%B8%AA%E8%AF%B4%E6%B3%95%EF%BC%8C%E6%95%88%E6%9E%9C%E5%B0%B1%E5%8F%AF%E8%83%BD%E5%8F%98%EF%BC%9F"><span class="nav-text">3.3 Prompt：为什么换个说法，效果就可能变？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Zero-shot-%E7%9A%84%E8%BE%B9%E7%95%8C%EF%BC%9A%E5%AE%83%E5%BC%BA%E5%9C%A8%E5%93%AA%EF%BC%8C%E4%B9%9F%E5%BC%B1%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="nav-text">3.4 Zero-shot 的边界：它强在哪，也弱在哪？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-%E7%A9%BA%E9%97%B4%E5%85%B3%E7%B3%BB%E5%92%8C%E7%BB%93%E6%9E%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E6%9C%89%E9%99%90"><span class="nav-text">3.4.1 空间关系和结构推理能力有限</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E7%9A%84%E5%81%8F%E5%B7%AE%E4%BC%9A%E5%8E%9F%E6%A0%B7%E5%8F%8D%E6%98%A0%E5%88%B0-zero-shot-%E4%B8%8A"><span class="nav-text">3.4.2 训练数据的偏差会原样反映到 zero-shot 上</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-%E2%80%9C%E8%83%BD%E5%AF%B9%E9%BD%90%E2%80%9D%E4%B8%8D%E7%AD%89%E4%BA%8E%E2%80%9C%E7%9C%9F%E7%90%86%E8%A7%A3%E2%80%9D"><span class="nav-text">3.4.3 “能对齐”不等于“真理解”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%A7%84%E6%A8%A1%E5%8C%96%E5%AF%B9%E9%BD%90%EF%BC%9AALIGN-%E4%B8%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E6%9D%A1%E4%BB%B6%E4%B8%8B%E7%9A%84%E7%8E%B0%E5%AE%9E%E8%B7%AF%E5%BE%84"><span class="nav-text">4. 规模化对齐：ALIGN 与弱监督条件下的现实路径</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%BB%8E-CLIP-%E5%88%B0-ALIGN%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E2%80%9C%E8%A7%84%E6%A8%A1%E4%BC%98%E5%85%88%E2%80%9D"><span class="nav-text">4.1 从 CLIP 到 ALIGN：为什么选择“规模优先”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%99%AA%E5%A3%B0%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B2%A1%E6%9C%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E5%A4%B1%E6%95%88"><span class="nav-text">4.2 噪声为什么没有让模型失效</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E8%A7%84%E6%A8%A1%E4%BD%9C%E4%B8%BA%E9%9A%90%E5%BC%8F%E7%BA%A6%E6%9D%9F%EF%BC%9A%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8D%87%E4%B8%8E%E7%8E%B0%E5%AE%9E%E4%BB%A3%E4%BB%B7"><span class="nav-text">4.3 规模作为隐式约束：能力提升与现实代价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E8%A7%84%E6%A8%A1%E5%8C%96%E5%AF%B9%E9%BD%90%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%AF%E7%A4%BA"><span class="nav-text">4.4 规模化对齐带来的方法启示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E4%BB%8E%E8%A1%A8%E7%A4%BA%E5%88%B0%E6%8E%A5%E5%8F%A3%EF%BC%9ABLIP2-%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BD%A2%E6%88%90"><span class="nav-text">5. 从表示到接口：BLIP2 与多模态系统的形成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E4%BB%8E%E5%AF%B9%E9%BD%90%E5%88%B0%E7%94%9F%E6%88%90%EF%BC%9A%E9%97%AE%E9%A2%98%E5%8F%91%E7%94%9F%E4%BA%86%E5%8F%98%E5%8C%96"><span class="nav-text">5.1 从对齐到生成：问题发生了变化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-BLIP2%EF%BC%9A%E6%8A%8A%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA%E5%BD%93%E4%BD%9C%E6%8E%A5%E5%8F%A3%E6%9D%A5%E7%94%A8"><span class="nav-text">5.2 BLIP2：把对齐表示当作接口来用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E4%BB%8E%E5%90%91%E9%87%8F%E5%AF%B9%E9%BD%90%E5%88%B0%E8%AF%AD%E8%A8%80%E5%8F%AF%E7%94%A8%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="nav-text">5.3 从向量对齐到语言可用的信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-RAG-Vision%EF%BC%9A%E6%8A%8A%E5%AF%B9%E9%BD%90%E7%A9%BA%E9%97%B4%E5%8F%98%E6%88%90%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90"><span class="nav-text">5.4 RAG-Vision：把对齐空间变成系统资源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-%E5%AF%B9%E9%BD%90%E8%8C%83%E5%BC%8F%E5%9C%A8%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%96%B0%E4%BD%8D%E7%BD%AE"><span class="nav-text">5.5 对齐范式在系统中的新位置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E5%AF%B9%E9%BD%90%E8%8C%83%E5%BC%8F%E7%9A%84%E5%A4%96%E6%BA%A2%EF%BC%9A%E9%9D%9E%E8%AF%AD%E8%A8%80%E5%A4%9A%E8%A7%86%E8%A7%92%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E7%BB%9F%E4%B8%80%E8%A1%A8%E7%A4%BA"><span class="nav-text">6. 对齐范式的外溢：非语言多视角场景中的统一表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E4%BB%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%88%B0%E5%A4%9A%E8%A7%86%E8%A7%92%EF%BC%9A%E9%97%AE%E9%A2%98%E5%B9%B6%E6%B2%A1%E6%9C%89%E6%9C%AC%E8%B4%A8%E5%8F%98%E5%8C%96"><span class="nav-text">6.1 从多模态到多视角：问题并没有本质变化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-BEV-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%B8%80%E7%A7%8D%E6%B2%A1%E6%9C%89%E8%AF%AD%E8%A8%80%E7%9A%84%E5%AF%B9%E9%BD%90%E6%96%B9%E5%BC%8F"><span class="nav-text">6.2 BEV 表示学习：一种没有语言的对齐方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-%E6%8A%8A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%BA%E4%BF%A1%E6%81%AF%E8%A7%86%E8%A7%92"><span class="nav-text">6.4 把模态理解为信息视角</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0%E4%B8%80%E4%B8%AA-CLIP-like-%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E5%AF%B9%E9%BD%90%E6%A8%A1%E5%9E%8B"><span class="nav-text">7. 训练与评估一个 CLIP-like 视觉语言对齐模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E5%AE%9E%E8%B7%B5%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%88%A4%E6%96%AD%E8%B7%AF%E5%BE%84"><span class="nav-text">7.1 实践目标与判断路径</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%EF%BC%9A%E8%B4%B4%E8%BF%91%E7%8E%B0%E5%AE%9E%E8%80%8C%E9%9D%9E%E5%88%B6%E9%80%A0%E7%90%86%E6%83%B3%E6%9D%A1%E4%BB%B6"><span class="nav-text">7.2 数据构建：贴近现实而非制造理想条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE%EF%BC%9A%E5%93%AA%E4%BA%9B%E9%80%89%E6%8B%A9%E6%9C%80%E5%80%BC%E5%BE%97%E6%8A%95%E5%85%A5%E7%B2%BE%E5%8A%9B"><span class="nav-text">7.3 模型结构与训练配置：哪些选择最值得投入精力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%93%AA%E4%BA%9B%E4%BF%A1%E5%8F%B7%E5%80%BC%E5%BE%97%E7%9B%B8%E4%BF%A1"><span class="nav-text">7.4 训练过程中哪些信号值得相信</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-%E5%9B%BE%E6%96%87%E6%A3%80%E7%B4%A2%EF%BC%9A%E5%88%A4%E6%96%AD%E5%AF%B9%E9%BD%90%E6%98%AF%E5%90%A6%E5%8F%91%E7%94%9F%E7%9A%84%E9%A6%96%E8%A6%81%E6%89%8B%E6%AE%B5"><span class="nav-text">7.5 图文检索：判断对齐是否发生的首要手段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-Zero-shot-%E5%88%86%E7%B1%BB%EF%BC%9A%E6%8E%A5%E5%8F%A3%E4%BB%B7%E5%80%BC%E7%9A%84%E7%9B%B4%E6%8E%A5%E4%BD%93%E7%8E%B0"><span class="nav-text">7.6 Zero-shot 分类：接口价值的直接体现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-CLIPScore-%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7%E5%88%A4%E6%96%AD%E7%9A%84%E8%BE%B9%E7%95%8C"><span class="nav-text">7.7 CLIPScore 与一致性判断的边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-8-%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%EF%BC%9A%E5%9C%A8%E6%9C%89%E9%99%90%E6%9D%A1%E4%BB%B6%E4%B8%8B%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E7%9C%8B%E5%BE%85%E5%AF%B9%E9%BD%90"><span class="nav-text">7.8 实践总结：在有限条件下如何正确看待对齐</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B%EF%BC%9A%E5%AF%B9%E9%BD%90%E8%8C%83%E5%BC%8F%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%8C%E5%8F%88%E5%B0%9A%E6%9C%AA%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88"><span class="nav-text">8. 总结与展望：对齐范式改变了什么，又尚未解决什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E5%AF%B9%E9%BD%90%E8%8C%83%E5%BC%8F%E7%9C%9F%E6%AD%A3%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%94%B9%E5%8F%98"><span class="nav-text">8.1 对齐范式真正带来的改变</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E5%AF%B9%E9%BD%90%E8%8C%83%E5%BC%8F%E7%9A%84%E7%8E%B0%E5%AE%9E%E8%BE%B9%E7%95%8C"><span class="nav-text">8.2 对齐范式的现实边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E4%BB%8E%E5%AF%B9%E9%BD%90%E8%B5%B0%E5%90%91%E6%9B%B4%E5%AE%8C%E6%95%B4%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%91%E6%88%98"><span class="nav-text">8.3 从对齐走向更完整世界模型的挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-%E7%BB%93%E8%AF%AD%EF%BC%9A%E5%AF%B9%E9%BD%90%E6%98%AF%E8%B5%B7%E7%82%B9%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E7%BB%88%E7%82%B9"><span class="nav-text">8.4 结语：对齐是起点，而不是终点</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">145</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2026/02/10/056-visual-multimodal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="视觉多模态：CLIP、ALIGN 与视觉语言对齐 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          视觉多模态：CLIP、ALIGN 与视觉语言对齐
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2026-02-10 21:20:12 / 修改时间：21:48:36" itemprop="dateCreated datePublished" datetime="2026-02-10T21:20:12+08:00">2026-02-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2026/02/10/056-visual-multimodal/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2026/02/10/056-visual-multimodal/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:13</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-从早期跨模态学习到-CLIP：视觉和语言对齐是如何发展的"><a href="#1-从早期跨模态学习到-CLIP：视觉和语言对齐是如何发展的" class="headerlink" title="1. 从早期跨模态学习到 CLIP：视觉和语言对齐是如何发展的"></a>1. 从早期跨模态学习到 CLIP：视觉和语言对齐是如何发展的</h2><p>在 CLIP 出现之前，让计算机同时“看懂图像、理解文字”，其实已经是一个被研究了很多年的问题。只要稍微想一想就能明白：如果模型既能处理图像，又能处理语言，那它就有可能完成图文搜索、图片描述、视觉问答等各种任务，这听起来非常有吸引力。</p>
<p>也正因为如此，计算机视觉和自然语言处理领域的研究者，早早就开始尝试“跨模态学习”，也就是把图像和文本放在同一个模型里一起建模。然而，有些尴尬的是：这些方法虽然在论文里能跑出不错的结果，但很难真正变成一种<strong>稳定、通用、可以长期复用的基础能力</strong>。模型往往是“为某个任务量身定做的”，而不是“可以反复拿来用的工具”。</p>
<span id="more"></span>
<p>理解 CLIP 为什么重要，关键不在于它某个指标提升了多少，而在于：它第一次让“视觉–语言对齐”这件事，形成了一种<strong>清晰、简单、可扩展的通用范式</strong>。要看清这一点，我们需要先回顾一下 CLIP 出现之前，这个领域大致是怎么发展的。</p>
<h3 id="1-1-CLIP-出现之前，如何把图像和文字关联起来的？"><a href="#1-1-CLIP-出现之前，如何把图像和文字关联起来的？" class="headerlink" title="1.1 CLIP 出现之前，如何把图像和文字关联起来的？"></a>1.1 CLIP 出现之前，如何把图像和文字关联起来的？</h3><p>早期视觉语言研究的核心目标，其实可以用一句话概括：<strong>让模型知道“一张图”和“一段话”在说的是不是同一件事</strong>。围绕这个目标，研究者主要走了两条不同的技术路线。</p>
<h4 id="1-1-1-双编码器路线：把图像和文字“放进同一个空间”"><a href="#1-1-1-双编码器路线：把图像和文字“放进同一个空间”" class="headerlink" title="1.1.1 双编码器路线：把图像和文字“放进同一个空间”"></a>1.1.1 双编码器路线：把图像和文字“放进同一个空间”</h4><p>第一类方法的想法相对直观：既然文字已经可以用词向量来表示语义关系（比如“猫”和“狗”比“猫”和“飞机”更接近），那能不能把图像也变成类似的向量，然后让图像和文字在同一个空间里“对齐”？</p>
<p>DeViSE 是比较早的一项代表性工作。它的基本思路是：</p>
<ul>
<li>先用神经网络把图像变成一个向量</li>
<li>再把这个向量“映射”到词向量所在的空间里</li>
<li>希望图像“猫”的向量，能靠近“cat”这个词向量</li>
</ul>
<p>后来出现的 VSE、VSE++ 等方法，把这个思路进一步系统化，采用了<strong>双编码器结构</strong>：</p>
<ul>
<li>一个编码器专门处理图像</li>
<li>另一个编码器专门处理文本</li>
<li>对应的图文对在向量空间中拉近，不匹配的拉远</li>
</ul>
<p>这种方法在图文检索任务中表现不错，比如“给我一张和这段话最匹配的图片”。同时，它也确立了一个重要的基本设想：<strong>图像和文字可以共享一个语义空间</strong>。</p>
<h4 id="1-1-2-融合交互路线：让图像和文字在模型内部“深度交互”"><a href="#1-1-2-融合交互路线：让图像和文字在模型内部“深度交互”" class="headerlink" title="1.1.2 融合交互路线：让图像和文字在模型内部“深度交互”"></a>1.1.2 融合交互路线：让图像和文字在模型内部“深度交互”</h4><p>另一类方法的思路则更加“激进”。它们认为：只是把图像和文本各自编码成一个向量，可能太粗糙了；真正的理解，应该发生在更细粒度的层面，比如：</p>
<ul>
<li>图像里的某个区域，对应文本里的哪个词</li>
<li>一句话中的不同成分，如何分别关联到画面中的不同对象</li>
</ul>
<p>于是，ViLBERT、LXMERT、UNITER 等模型引入了跨模态 Transformer，通过 <strong>cross-attention</strong> 机制，让图像特征和文本特征在模型内部反复交互、彼此影响。</p>
<p>这类模型在视觉问答、多模态推理等复杂任务上确实非常强，能捕捉到细致的语义关系。但代价也很明显：</p>
<ul>
<li>模型结构复杂</li>
<li>计算成本高</li>
<li>强烈依赖具体任务和数据集</li>
</ul>
<p>回头看，这一阶段的研究已经包含了很多后来方法的“零部件”：双编码器、相似度学习、预训练+微调等。但它们始终没有组合成一个<strong>真正面向开放世界的统一学习框架</strong>。</p>
<pre class="mermaid">flowchart LR
    %% 融合交互结构
    subgraph R[融合交互 / Cross-Attention 路线]
        direction LR
        R1[图像特征<br/>Patch / Region]
        R2[文本特征<br/>Token Embedding]
        R1 --> R3[跨模态编码器<br/>Cross-Attention]
        R2 --> R3
        R3 --> R4[融合表示]
        R4 --> R5[下游任务<br/>生成 / 推理 / VQA]
    end


    %% 双编码器结构
    subgraph L[双编码器 / Joint Embedding 路线]
        direction LR
        L1[图像编码器<br/>CNN / ViT] --> L3[图像嵌入向量]
        L2[文本编码器<br/>Transformer] --> L4[文本嵌入向量]
        L3 --> L5[相似度计算<br/>对比学习目标]
        L4 --> L5
    end</pre>
<center>图1-1 视觉语言模型的两类典型结构：双编码器路线与融合交互路线</center>

<p>如图上侧双编码器路线强调图像与文本在嵌入空间中的对齐与复用，结构简洁、效率高；下侧融合交互路线强调模态间的深度耦合，更适合复杂生成与推理任务，但系统成本更高。该对照为后文围绕 CLIP 双塔结构展开讨论提供了直观背景。</p>
<h3 id="1-2-为什么这些早期方法，始终没能变成“通用能力”？"><a href="#1-2-为什么这些早期方法，始终没能变成“通用能力”？" class="headerlink" title="1.2 为什么这些早期方法，始终没能变成“通用能力”？"></a>1.2 为什么这些早期方法，始终没能变成“通用能力”？</h3><p>如果只看论文结果，CLIP 之前的视觉语言模型其实已经不弱了。但问题在于：它们很难被当作一个长期可靠的“基础模块”来使用，原因主要有以下几个方面。</p>
<h4 id="1-2-1-文本更像“标签”，而不是语言本身"><a href="#1-2-1-文本更像“标签”，而不是语言本身" class="headerlink" title="1.2.1 文本更像“标签”，而不是语言本身"></a>1.2.1 文本更像“标签”，而不是语言本身</h4><p>在很多早期模型中，文本的角色其实非常像一种“高级标签”。模型的目标往往是给定一张图，找到对应的那条描述，或者在一组候选答案里选对一个。</p>
<p>在这种设定下，文字并不是一个可以自由组合、不断扩展的语义系统，而只是监督信号的一部分。这使得模型更擅长“在已有数据里做选择”，却不太擅长面对全新的描述方式或概念组合。</p>
<h4 id="1-2-2-学到的表示不够稳定，难以跨任务复用"><a href="#1-2-2-学到的表示不够稳定，难以跨任务复用" class="headerlink" title="1.2.2 学到的表示不够稳定，难以跨任务复用"></a>1.2.2 学到的表示不够稳定，难以跨任务复用</h4><p>很多模型在换一个任务或数据集之后，都需要重新训练，或者进行大幅度微调。换句话说同一个模型，在不同任务下，内部表示会发生明显变化，表示本身很难作为一个“中间接口”被固定下来。</p>
<p>这在系统层面是个大问题，因为一旦表示不稳定，就很难围绕它构建长期演进的应用。</p>
<h4 id="1-2-3-训练目标本质上是“封闭世界”的"><a href="#1-2-3-训练目标本质上是“封闭世界”的" class="headerlink" title="1.2.3 训练目标本质上是“封闭世界”的"></a>1.2.3 训练目标本质上是“封闭世界”的</h4><p>早期跨模态模型通常在规模有限的数据集上训练，负样本也来自同一个数据集。在这种情况下，模型更多是在学：“如何区分这堆已知样本里，哪些配对是对的，哪些是错的”。</p>
<p>但现实世界是开放的，新的概念、新的组合不断出现。当模型规模变大、应用场景变复杂时，这种“封闭世界假设”的局限就会越来越明显。</p>
<h4 id="1-2-4-系统层面并不友好"><a href="#1-2-4-系统层面并不友好" class="headerlink" title="1.2.4 系统层面并不友好"></a>1.2.4 系统层面并不友好</h4><p>尤其是深度融合模型，在推理时需要复杂的跨模态交互，计算开销大、延迟高。这使得它们不适合做大规模检索，也不适合部署在对响应速度要求高的系统中。</p>
<p>综合来看，这些问题并不是某个技巧的小缺陷，而是<strong>范式层面的限制</strong>。</p>
<h3 id="1-3-CLIP-出现的背景：不是重新发明轮子，而是换了一种问法"><a href="#1-3-CLIP-出现的背景：不是重新发明轮子，而是换了一种问法" class="headerlink" title="1.3 CLIP 出现的背景：不是重新发明轮子，而是换了一种问法"></a>1.3 CLIP 出现的背景：不是重新发明轮子，而是换了一种问法</h3><p>正是在这样的背景下，CLIP 才显得格外重要。CLIP 的名字是 Contrastive Language–Image Pretraining，直译过来就是“通过对比学习进行图像–语言预训练”。但真正关键的是它对问题本身的重新定义。</p>
<p>CLIP 做了一件非常“克制”的事：它不去针对某个具体下游任务设计目标，而是只问一个问题——<strong>“这张图像，和这段文字，在语义上是不是匹配的？”</strong> 为了回答这个问题，CLIP 使用了海量的图像–文本对数据，让模型在视觉和语言之间学会一个共享的语义空间。</p>
<p>在这里，文本不再是固定的标签，而是自然语言形式的描述；模型不再学习“图像属于哪个类别”，而是学习“这张图像是否符合这段描述”。正是这种问题形式的改变，使训练目标天然具有开放性，也为后来的 zero-shot 能力打下了基础。</p>
<p><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/IQ8kvH.png" alt="IQ8kvH"></p>
<center>图 1-2　CLIP 的图像–文本共享语义空间示意</center>

<p>如上图图像与自然语言分别经由独立编码器映射到同一语义嵌入空间中。模型不再预测固定类别，而是通过比较图像与文本表示之间的相似度，判断二者在语义层面是否匹配。匹配的图像–文本对在嵌入空间中距离更近，不匹配的对则被拉远。</p>
<h3 id="1-4-双塔结构-对比学习：CLIP-为什么既简单又强大？"><a href="#1-4-双塔结构-对比学习：CLIP-为什么既简单又强大？" class="headerlink" title="1.4 双塔结构 + 对比学习：CLIP 为什么既简单又强大？"></a>1.4 双塔结构 + 对比学习：CLIP 为什么既简单又强大？</h3><p>在结构上，CLIP 采用的是一种非常清晰的<strong>双塔架构</strong>：一侧是图像编码器；另一侧是文本编码器。</p>
<p>两者在各自的通道里独立工作，最后只在向量空间中通过相似度进行“对齐”。这种设计有两个非常实际的好处。一方面，它避免了复杂的跨模态注意力计算，使得推理速度快、扩展性好；另一方面，图像和文本的向量可以提前算好、缓存起来，天然适合做检索和索引。</p>
<p>在训练时，CLIP 使用的是对比学习目标。可以把一个 batch 想象成一组“配对好的图片和文字”：</p>
<ul>
<li>每张图像只和自己的那条文字是“正样本”</li>
<li>和 batch 里其他文字都是“负样本”</li>
</ul>
<p>模型的目标就是：让正确配对的图文相似度最高，其余配对尽可能低。通过这种方式，CLIP 在大规模数据上逐步拉齐了视觉和语言的语义空间。</p>
<p><strong>小结：为什么说 CLIP 是一次范式转折？</strong><br>回顾整个发展过程可以发现：CLIP 并没有发明全新的技术组件，它用到的很多想法，在早期工作中都能找到影子。但 CLIP 的真正贡献，在于把<strong>问题设定、模型结构和数据规模</strong>这三件事，组合成了一个高度一致的整体。</p>
<p>从这一刻起，视觉语言学习不再只是“为某个任务服务的模型”，而第一次具备了成为<strong>通用表示接口</strong>的可能。这一转变，也直接引出了后续的 zero-shot 泛化、多模态系统构建等一系列重要进展。</p>
<p>理解这一点，是理解后续所有 CLIP 相关工作的前提。下一章，我们将进一步深入 CLIP 的训练目标与能力特性，看看这种新范式究竟“强”在什么地方，又“限制”在哪里。</p>
<h2 id="2-CLIP-的核心机制：为什么“图像–语言对齐”能变成通用表示？"><a href="#2-CLIP-的核心机制：为什么“图像–语言对齐”能变成通用表示？" class="headerlink" title="2. CLIP 的核心机制：为什么“图像–语言对齐”能变成通用表示？"></a>2. CLIP 的核心机制：为什么“图像–语言对齐”能变成通用表示？</h2><p>在上一章中，我们已经看到CLIP 并不是靠某一个惊艳的小技巧横空出世的，而是在已有研究基础上，通过一整套彼此配合的设计选择，真正改变了视觉语言学习的方向。</p>
<p>这一章我们不再从历史角度展开，而是直接走进 CLIP 的内部，看看它到底是<strong>怎么运作的</strong>。核心问题只有一个：<strong>为什么“让图像和文字对齐”这件事，最终能变成一种可以反复使用的通用表示方式？</strong></p>
<p>要回答这个问题，我们需要依次理解它在训练目标、模型结构和表示方式上的几个关键选择。</p>
<h3 id="2-1-从“选答案”到“看配不配”：训练目标发生了什么变化？"><a href="#2-1-从“选答案”到“看配不配”：训练目标发生了什么变化？" class="headerlink" title="2.1 从“选答案”到“看配不配”：训练目标发生了什么变化？"></a>2.1 从“选答案”到“看配不配”：训练目标发生了什么变化？</h3><p>如果我们接触过传统的图像分类模型，比如在 ImageNet 上训练的网络，它们解决的问题通常是这样的：</p>
<blockquote>
<p>给一张图片，从一堆预先定义好的标签里，选出一个正确答案。</p>
</blockquote>
<p>在这种训练方式下，世界是被提前“分好类”的。模型要做的事情，本质上就是学会这些类别之间的区分规则。因此，它学到的特征往往和这套标签体系紧密绑定：</p>
<ul>
<li>标签没定义的概念，模型很难处理</li>
<li>类别之外的信息，往往会被忽略</li>
</ul>
<p>CLIP 对这个问题的处理方式，并不是再加更多标签，而是<strong>干脆换了一道题</strong>。它不再问“这是什么类别？”，而是问：<strong>“这张图，和这段话，说的是不是同一件事？”</strong> 这个变化看起来很简单，但影响非常大。</p>
<h4 id="为什么“匹配”比“分类”更开放？"><a href="#为什么“匹配”比“分类”更开放？" class="headerlink" title="为什么“匹配”比“分类”更开放？"></a>为什么“匹配”比“分类”更开放？</h4><p>首先，文本本身是开放的。自然语言不像标签那样固定，它可以描述：</p>
<ul>
<li>外观（“一只毛茸茸的小动物”）</li>
<li>行为（“正在奔跑”）</li>
<li>风格（“像油画一样”）</li>
<li>甚至抽象概念（“孤独”“混乱”）</li>
</ul>
<p>当训练目标变成“图像是否符合描述”，模型从一开始就被迫面对一个更大、更灵活的语义空间，而不是一个封闭的类别列表。其次，这种设定让图像和语言处在<strong>对等的位置</strong>。</p>
<p>文字不再只是告诉模型“正确答案是什么”，而是和图像一样，作为需要被理解和对齐的对象存在。模型真正要学的，是<strong>两种模态之间稳定的对应关系</strong>。也正因为如此，CLIP 的训练目标天然就不依赖具体任务，这为后面 zero-shot 使用打下了基础。</p>
<h3 id="2-2-双塔结构：为什么“少交互”反而更通用？"><a href="#2-2-双塔结构：为什么“少交互”反而更通用？" class="headerlink" title="2.2 双塔结构：为什么“少交互”反而更通用？"></a>2.2 双塔结构：为什么“少交互”反而更通用？</h3><p>从结构上看，CLIP 采用的是一种被称为“双塔”的设计：</p>
<ul>
<li>一边是图像编码器，只负责把图片变成向量</li>
<li>另一边是文本编码器，只负责把文字变成向量</li>
</ul>
<p>两者在各自的“通道”里独立工作，直到最后一步，才通过相似度计算联系起来。如果我们之前了解过一些多模态模型，可能会觉得这种结构“太简单了”。毕竟，还有很多模型会让图像和文字在中间层反复交互，看起来理解得更“深入”。</p>
<p>但 CLIP 在这里做了一个非常明确的取舍：<strong>它优先保证表示本身的独立性和可复用性，而不是追求单个任务上的极致效果。</strong></p>
<h4 id="双塔结构解决了什么问题？"><a href="#双塔结构解决了什么问题？" class="headerlink" title="双塔结构解决了什么问题？"></a>双塔结构解决了什么问题？</h4><ul>
<li>它让表示变成了“可以保存和复用的东西”。<br>  图像和文本被编码成向量后，就可以被缓存、索引、反复使用，而不需要每次都重新计算复杂的跨模态交互。这对检索系统和大规模应用非常关键。</li>
<li>它把“理解”和“对齐”分开了。<br>  编码器只关心：如何在各自模态里提取稳定、有意义的特征；对齐只发生在向量空间中，通过相似度来完成</li>
</ul>
<p>这种清晰的分工，让模型结构更容易扩展，也更容易在大数据上训练。从这个角度看，双塔结构并不是“能力不够”，而是一种<strong>为通用性服务的设计选择</strong>。</p>
<pre class="mermaid">flowchart LR
  subgraph A[双塔结构（CLIP）<br/>编码解耦，最后对齐]
    direction LR
    A1[图像输入] --> A2[图像编码器]
    A2 --> A3[图像嵌入向量]
    A4[文本输入] --> A5[文本编码器]
    A5 --> A6[文本嵌入向量]
    A3 --> A7[相似度计算<br/>检索 / zero-shot]
    A6 --> A7
  end

  subgraph B[融合结构（Cross-Attention）<br/>编码阶段深度交互]
    direction LR
    B1[图像输入] --> B2[图像特征提取]
    B3[文本输入] --> B4[文本特征提取]
    B2 --> B5[跨模态融合模块<br/>Cross-Attention]
    B4 --> B5
    B5 --> B6[融合表示]
    B6 --> B7[下游任务输出<br/>VQA / 推理 / 生成]
  end</pre>
<center>图2-1 CLIP 双塔结构与融合结构的推理流程对比示意</center>

<p>该图对比了双塔结构与融合结构在推理流程上的关键差异。双塔结构中，图像与文本在编码阶段完全解耦，分别生成可独立使用的嵌入向量，直到最后才通过相似度计算完成对齐，因此嵌入可以被缓存、索引并在系统中重复调用。融合结构则在编码阶段引入跨注意力机制，使两种模态在内部进行深度交互，能够捕捉更细粒度的对应关系，但推理路径更重、复用性更弱。</p>
<h3 id="2-3-对比学习：CLIP-是如何“塑造”语义空间的？"><a href="#2-3-对比学习：CLIP-是如何“塑造”语义空间的？" class="headerlink" title="2.3 对比学习：CLIP 是如何“塑造”语义空间的？"></a>2.3 对比学习：CLIP 是如何“塑造”语义空间的？</h3><p>CLIP 的训练核心，是一种对比学习方式。我们可以用一个直观的场景来理解。假设一个训练批次里有很多组“图片 + 对应文字”：</p>
<ul>
<li>每张图片，只有一段文字是真正匹配的</li>
<li>其余文字，都是“不太对”的描述</li>
</ul>
<p>CLIP 会做的事情是：</p>
<ul>
<li>计算所有图片和所有文字之间的相似度</li>
<li>让正确配对的相似度最高</li>
<li>其余组合的相似度尽量低</li>
</ul>
<p>长期训练下来，这个过程会在向量空间中产生一种整体结构：</p>
<ul>
<li>语义接近的图像和文字会聚在一起</li>
<li>不相关的内容会被自然拉开</li>
<li>整个空间不会挤在某几个方向上，而是相对均匀地展开</li>
</ul>
<p>一个值得注意的点是：CLIP 并不会精细区分“哪些负样本其实也有点像”。在大规模数据条件下，这种“粗粒度”的对比，反而有助于模型学到更稳健、更全局的语义结构。</p>
<p>因此，这里的对比学习，目的并不是追求局部关系的精确，而是<strong>构建一个稳定、可查询的语义空间</strong>。</p>
<h3 id="2-4-相似度和温度：看似细节，其实很关键"><a href="#2-4-相似度和温度：看似细节，其实很关键" class="headerlink" title="2.4 相似度和温度：看似细节，其实很关键"></a>2.4 相似度和温度：看似细节，其实很关键</h3><p>在 CLIP 的训练过程中，图像与文本之间如何比较相似程度，并不是一个无关紧要的实现细节，而是直接决定对齐是否稳定、表示是否可用的关键设计。<strong>相似度函数与温度参数</strong>，正是这一设计中的两个核心要素。</p>
<p>在相似度计算上，CLIP 通常采用余弦相似度。直观来看，这一选择体现了一种非常明确的判断标准：在比较两个表示时，不关心向量“有多大”，而只关心它们“指向的方向是否接近”。可以将嵌入向量理解为指向某个语义概念的箭头，箭头的方向代表语义内容，而长度本身并不携带稳定的语义信息。通过只比较方向，模型能够在嵌入空间中形成更加稳定、可解释的几何结构，从而减少训练过程中由于尺度变化带来的干扰。</p>
<p>如果说余弦相似度解决的是“什么算相似”的问题，那么<strong>温度参数</strong>则决定了“相似度差异有多重要”。温度可以被理解为一个调节模型判断强度的旋钮，用来控制相似度分布在 softmax 归一化后的形态。</p>
<p>当温度较低时，相似度差异会被显著放大。模型对最相似的匹配关系非常敏感，往往迅速形成高度集中的对齐结果。这种设置有助于增强区分度，使模型更果断地拉近正确配对、拉远错误配对，但也更容易受到噪声样本或 batch 波动的影响，训练过程可能变得不稳定。</p>
<p>当温度较高时，相似度分布则更加平缓。模型在不同候选之间保留更多不确定性，训练过程更加稳定，对噪声的鲁棒性也更强，但代价是对齐关系不够“尖锐”，收敛速度和区分能力可能受到一定影响。</p>
<p>CLIP 的一个重要设计选择，是<strong>将温度参数设为可学习形式</strong>，而非固定常数。这一做法的本质，是将“区分得足够清楚”和“不过度自信”之间的权衡，交由模型在训练过程中自行调整。在不同数据规模、batch 设置和噪声水平下，最合适的温度并不相同，让模型通过梯度学习这一参数，有助于在多种训练条件下维持嵌入空间的合理结构。</p>
<p>在实践中可以观察到，温度参数的设置与收敛状态，往往对检索性能、zero-shot 表现以及整体训练稳定性产生显著影响。这也再次说明，在视觉语言对齐中，<strong>表示质量并不只取决于模型结构或数据规模</strong>，嵌入空间的几何组织方式同样至关重要。</p>
<pre class="mermaid">flowchart LR
    subgraph A[低温度（T 较小）高区分度]
        direction LR
        A1[相似度分布<br/>差异被放大] --> A2[Softmax 输出<br/>高度集中]
        A2 --> A3[对齐关系明确<br/>但易不稳定]
    end

    subgraph B[中等温度（T 适中）平衡状态]
        direction LR
        B1[相似度分布<br/>差异适中] --> B2[Softmax 输出<br/>分布合理]
        B2 --> B3[区分度与稳定性<br/>取得平衡]
    end

    subgraph C[高温度（T 较大）高平滑度]
        direction LR
        C1[相似度分布<br/>差异被压缩] --> C2[Softmax 输出<br/>更为平缓]
        C2 --> C3[训练更稳定<br/>但对齐变弱]
    end</pre>
<center>图2-2 温度参数对相似度分布与对齐强度的影响示意</center>

<p>该图示意了温度参数在相似度计算与 softmax 归一化过程中所起的调节作用。温度较低时，相似度差异被显著放大，softmax 输出高度集中，模型对匹配关系的判断更为果断，但也更容易受到噪声与 batch 波动影响。温度较高时，相似度分布被压缩，softmax 输出更加平滑，有助于提升训练稳定性，但可能削弱对齐强度。</p>
<p>将温度设为可学习参数，使模型能够在训练过程中动态调整这一权衡，在不同数据规模与训练配置下维持嵌入空间的合理结构，是 CLIP 在实践中表现稳定的重要原因之一。</p>
<h3 id="2-5-Zero-shot-能力：不是魔法，而是结构的自然结果"><a href="#2-5-Zero-shot-能力：不是魔法，而是结构的自然结果" class="headerlink" title="2.5 Zero-shot 能力：不是魔法，而是结构的自然结果"></a>2.5 Zero-shot 能力：不是魔法，而是结构的自然结果</h3><p>CLIP 最引人注目的能力之一，是它可以在<strong>不重新训练的情况下</strong>完成新任务，比如直接用文字描述来做分类。这种 zero-shot 能力，其实并不是额外加上去的，而是整个表示结构的自然结果。</p>
<p><strong>因为图像和文本被放在同一个语义空间中，任何可以用语言描述的概念，都可以先变成一个文本向量，再和图像向量进行相似度比较</strong>。从这个角度看，分类本身就变成了一次“找谁最像”的问题。</p>
<p>当然，这并不意味着 CLIP 真正“理解”了世界。更准确地说，它提供的是一种<strong>可扩展的语义接口</strong>：</p>
<ul>
<li>新概念不需要重新定义类别</li>
<li>新任务不需要重新训练模型</li>
<li>只要能用语言描述，就可以尝试对齐</li>
</ul>
<p>这正是 CLIP 能够跨任务使用的根本原因。</p>
<p><strong>小结：为什么对齐，最终变成了通用表示？</strong><br>回顾这一章可以发现，CLIP 的每一个设计选择单独看都不复杂，但它们在目标、结构和规模上的高度一致，使整体效果发生了质变。</p>
<ul>
<li>匹配式训练目标，让模型一开始就面向开放语义</li>
<li>双塔结构，保证了表示的独立性和可复用性</li>
<li>对比学习，塑造了稳定、可查询的语义空间</li>
<li>相似度与温度设计，确保训练过程可控、鲁棒</li>
</ul>
<p>正是在这些因素共同作用下，视觉语言对齐不再只是“解决某个任务的技巧”，而成为了一种可以被反复使用的通用表示方式。</p>
<p>下一章将进一步聚焦 CLIP 所展现的 zero-shot 泛化能力，深入讨论语言作为“语义接口”在开放世界中的优势，以及它仍然存在的边界。</p>
<h2 id="3-Zero-shot-泛化：语言为什么能变成开放的“语义接口”？"><a href="#3-Zero-shot-泛化：语言为什么能变成开放的“语义接口”？" class="headerlink" title="3. Zero-shot 泛化：语言为什么能变成开放的“语义接口”？"></a>3. Zero-shot 泛化：语言为什么能变成开放的“语义接口”？</h2><p>在前一章我们已经看到，CLIP 做了一件很关键的事：它把图像和文本都映射到同一个“语义空间”里。我们可以把这个空间想象成一张大地图——图片和句子都会被放到这张地图上的某个位置，越“像”的东西，位置越接近。</p>
<p>这种设计带来一个非常直接的结果：模型即使没有针对某个具体任务再训练，也能做分类、检索等事情。看起来就像多了一种“即插即用”的能力。</p>
<p>这一章我们专门围绕这个现象回答三个问题：</p>
<ol>
<li>CLIP 的 zero-shot 分类到底是怎么实现的？</li>
<li>为什么语言可以充当开放的语义接口？</li>
<li>这种能力好用到什么程度，它的边界又在哪里？</li>
</ol>
<h3 id="3-1-Zero-shot-分类不是“额外的技能”"><a href="#3-1-Zero-shot-分类不是“额外的技能”" class="headerlink" title="3.1 Zero-shot 分类不是“额外的技能”"></a>3.1 Zero-shot 分类不是“额外的技能”</h3><p>在传统视觉模型里，“分类”往往和“特定任务”绑得很死。比如我们要做猫狗分类，就得用猫狗数据训练；要做花卉分类，就得换数据重新训练或至少微调。原因很简单：模型训练时面对的是一个固定的类别列表，它学到的能力也就围绕这个列表展开。</p>
<p>CLIP 的做法完全不同。它在做 zero-shot 分类时，不需要再训练任何参数，而是走了这样一条路：</p>
<ol>
<li><strong>把每个类别写成一句话</strong>（或一段描述）<br> 比如不是只写“cat”，而是写成“a photo of a cat”（一张猫的照片）这类自然语言描述。</li>
<li><strong>用文本编码器把这些描述变成向量</strong><br> 每个类别对应一个“文本向量”。</li>
<li><strong>把图片也变成向量</strong><br> 图片经过图像编码器得到“图像向量”。</li>
<li><strong>比较相似度，谁最像就选谁</strong><br> 图像向量和所有类别的文本向量逐个比一遍，最相近的那个类别就是预测结果。</li>
</ol>
<p>如果用生活类比，就像不用再背一张“标签表”了，而是拿着图片去对照一堆“文字说明”，看哪个说明最符合这张图。</p>
<p>关键点在于：这不是 CLIP 专门为“分类”加的一个模块。它只是利用了“图像与文本在同一空间可比较”这一事实。换句话说，在 CLIP 的框架里：<strong>分类被改写成了“语义匹配”问题。</strong> 只要图片和类别描述在语义空间中能稳定对应，分类就自然成立。因此，zero-shot 在 CLIP 里不是特例，而更像一种“默认用法”。</p>
<pre class="mermaid">flowchart LR
    A[输入图像] --> B[图像编码器<br/>Image Encoder]
    B --> C[图像嵌入<br/>v_img]

    subgraph D[类别文本提示（Prompts）]
        direction TB
        D1[类别1文本描述<br/>例如：a photo of a cat] --> E1[文本编码器] --> F1[类别嵌入 t1]
        D2[类别2文本描述<br/>例如：a photo of a dog] --> E2[文本编码器] --> F2[类别嵌入 t2]
        D3[类别3文本描述<br/>例如：a photo of a airplane] --> E3[文本编码器] --> F3[类别嵌入 t3]
        D4[...更多类别] --> E4[文本编码器] --> F4[类别嵌入 tn]
    end

    C --> G[相似度计算]
    F1 --> G
    F2 --> G
    F3 --> G
    F4 --> G

    G --> H[选择相似度最高的类别]
    H --> I[输出预测类别]</pre>
<center>图3-1 CLIP 的 zero-shot 分类：在共享嵌入空间中做相似度匹配</center>

<p>该图展示了 CLIP 的 zero-shot 分类流程：首先将输入图像编码为嵌入向量，同时将每个类别写成自然语言描述并编码为类别向量。随后在共享嵌入空间中计算图像向量与各类别向量的相似度，并选择相似度最高的类别作为预测结果。整个过程无需额外训练，本质上等价于在语义空间中进行一次最近邻式的匹配检索。</p>
<h3 id="3-2-为什么语言能当“开放语义接口”？"><a href="#3-2-为什么语言能当“开放语义接口”？" class="headerlink" title="3.2 为什么语言能当“开放语义接口”？"></a>3.2 为什么语言能当“开放语义接口”？</h3><p>很多人第一次看到 CLIP 的 zero-shot，会以为它的本质是“见过更多类别”。但更准确的说法是：<strong>CLIP 借助语言，把类别这件事从“固定标签”变成了“可以随时写出来的描述”。</strong> 语言之所以能做到这一点，主要因为它有三个天然优势。  </p>
<h4 id="3-2-1-语言可以无限组合"><a href="#3-2-1-语言可以无限组合" class="headerlink" title="3.2.1 语言可以无限组合"></a>3.2.1 语言可以无限组合</h4><p>标签体系很有限：我们预先定义多少类，就只能识别多少类。但语言不是这样。有限的词汇和语法规则，就能拼出几乎无限的描述。</p>
<p>比如我们没训练过“穿雨衣的狗”，但训练过“狗”“雨衣”“穿着”，语言可以把这些片段组合起来。于是模型即使没见过“这个完整概念”，也可能通过组合语义去“猜个大概”。这就是语言的<strong>组合性</strong>带来的开放空间。</p>
<h4 id="3-2-2-同一件事可以有很多种说法"><a href="#3-2-2-同一件事可以有很多种说法" class="headerlink" title="3.2.2 同一件事可以有很多种说法"></a>3.2.2 同一件事可以有很多种说法</h4><p>“同一张图”可以被描述成：</p>
<ul>
<li>“a photo of a cat”</li>
<li>“a picture of a cat”</li>
<li>“a close-up of a cat”</li>
</ul>
<p>这种“说法很多”的特性，在训练时反而有好处：它迫使模型不要死记某一种表述，而要学更稳定、更本质的对应关系。你可以把它理解成一种“自然的纠错机制”：模型不能靠背答案，只能靠理解大意。这就是语言的<strong>冗余性</strong>带来的稳定性。</p>
<h4 id="3-2-3-语言能描述的层次很丰富"><a href="#3-2-3-语言能描述的层次很丰富" class="headerlink" title="3.2.3 语言能描述的层次很丰富"></a>3.2.3 语言能描述的层次很丰富</h4><p>传统标签往往只描述“是什么”（类别）。但语言还能描述：</p>
<ul>
<li>属性：红色的、毛茸茸的</li>
<li>动作：在跳、在奔跑</li>
<li>风格：卡通风、油画风</li>
<li>关系：左边的杯子、桌子上的书</li>
<li>甚至感受：温暖的、压抑的（虽然会更主观）</li>
</ul>
<p>这让语言天然适合当一个“接口”，因为它不止能提供类别名，还能提供额外信息，帮助定义任务。在 CLIP 的训练中，文本不是被当作“标签名”用的，而是被当作完整语义对象参与对齐。模型学到的不是“图像→标签”，而是“视觉模式↔语言语义”的对应关系。</p>
<p>正因为角色变了，语言在推理阶段才有资格当“开放接口”。</p>
<h3 id="3-3-Prompt：为什么换个说法，效果就可能变？"><a href="#3-3-Prompt：为什么换个说法，效果就可能变？" class="headerlink" title="3.3 Prompt：为什么换个说法，效果就可能变？"></a>3.3 Prompt：为什么换个说法，效果就可能变？</h3><p>zero-shot 分类听起来很直接，但真正用起来会很快发现：<strong>同一个类别，怎么写那句话，准确率可能差很多。</strong></p>
<p>比如 “a photo of a cat” 和 “a picture of a cat” 语义差不多，但在 CLIP 的语义空间里，它们的位置可能并不完全一样，于是分类结果会出现差异。这种现象常被称为 <strong>prompt 敏感性</strong>。</p>
<p>这并不一定意味着模型“很不稳定”，而是因为在 CLIP 里，文本不是装饰，而是分类过程的一部分：</p>
<ul>
<li>文本向量就是“类别的表示”</li>
<li>你换一种说法，相当于在语义空间里换了一个“查询点”</li>
<li>查询点变了，相似度排序自然也会变</li>
</ul>
<p>从直觉上说，prompt 更像一个“检索时的搜索关键词”。同样在网上搜东西，我们用不同关键词，搜出来的结果也可能不同。CLIP 的 zero-shot 分类其实就是一种“语义检索”，prompt 就是我们写的“检索语句”。</p>
<p>因此，很多实践会用<strong>多种 prompt 模板</strong>（多种说法）来描述同一类别，然后把结果做平均或投票。这样做并不是临时补丁，而是语言接口“开放性”的自然代价：开放意味着表达方式多，也就带来表达方式的差异。</p>
<p>但 prompt 敏感性也提醒我们一件事：zero-shot 的效果依赖于“我们的描述方式”和“模型训练时见过的表达习惯”是否接近。描述一旦偏离训练分布，性能下降就很常见。</p>
<h3 id="3-4-Zero-shot-的边界：它强在哪，也弱在哪？"><a href="#3-4-Zero-shot-的边界：它强在哪，也弱在哪？" class="headerlink" title="3.4 Zero-shot 的边界：它强在哪，也弱在哪？"></a>3.4 Zero-shot 的边界：它强在哪，也弱在哪？</h3><p>CLIP 的 zero-shot 很强，但它不是万能钥匙。它的边界主要体现在三个方面。</p>
<h4 id="3-4-1-空间关系和结构推理能力有限"><a href="#3-4-1-空间关系和结构推理能力有限" class="headerlink" title="3.4.1 空间关系和结构推理能力有限"></a>3.4.1 空间关系和结构推理能力有限</h4><p>CLIP 更擅长做“整体语义匹配”，比如识别是什么物体、是什么场景、整体风格如何。但如果任务需要更细的结构理解，比如：</p>
<ul>
<li>“红球在蓝球左边吗？”</li>
<li>“图里有几个人分别在做什么？”</li>
<li>“先找到A，再根据A的位置判断B的关系”</li>
</ul>
<p>这类复杂空间关系或多步骤推理，CLIP 往往不如专门为定位&#x2F;推理设计的模型。原因也很直观：CLIP 的训练目标主要在学“图文是否匹配”的全局对齐，而不是训练它做精细定位或严密推理。</p>
<h4 id="3-4-2-训练数据的偏差会原样反映到-zero-shot-上"><a href="#3-4-2-训练数据的偏差会原样反映到-zero-shot-上" class="headerlink" title="3.4.2 训练数据的偏差会原样反映到 zero-shot 上"></a>3.4.2 训练数据的偏差会原样反映到 zero-shot 上</h4><p>CLIP 依赖大规模网络图文数据训练。网络数据覆盖面很广，但也不可避免带有偏好和缺口：</p>
<ul>
<li>某些领域数据少</li>
<li>某些风格、文化背景不均衡</li>
<li>某些描述方式更常见</li>
</ul>
<p>于是你会看到：在和训练分布相近的场景里，zero-shot 很强；但在特定领域、特定风格（例如医学影像、工业缺陷、一些小语种描述）上，效果可能明显下降。这不是 CLIP “突然变笨”，而是它学到的对齐关系本来就来自这些数据。</p>
<h4 id="3-4-3-“能对齐”不等于“真理解”"><a href="#3-4-3-“能对齐”不等于“真理解”" class="headerlink" title="3.4.3 “能对齐”不等于“真理解”"></a>3.4.3 “能对齐”不等于“真理解”</h4><p>CLIP 很擅长判断“这段话像不像这张图”，但这不等同于它理解了概念背后的因果、物理规律或逻辑结构。zero-shot 更准确的含义是：<strong>语义空间可扩展、可查询、可迁移</strong>，而不是模型具备人类式的理解与推理。</p>
<p><strong>小结：Zero-shot 是“接口能力””</strong><br>综合来看，CLIP 的 zero-shot 泛化并不是一个额外模块，而是视觉–语言对齐范式带来的直接结果：</p>
<ul>
<li>共享语义空间让分类变成匹配</li>
<li>语言的开放性让类别可被“随写随用”</li>
<li>prompt 的变化影响查询方式，从而影响结果</li>
<li>能力边界则来自训练目标与数据分布本身</li>
</ul>
<p>因此，zero-shot 更像是一种<strong>接口能力的体现</strong>：它让模型摆脱固定标签体系，让视觉表示第一次具备跨任务、跨类别复用的可能性。但与此同时，它也提醒我们：zero-shot 的灵活性来自语言接口，而语言接口的开放性也带来了敏感性与偏差；而对齐的强大，并不等价于真正的深层理解。</p>
<p>在明确这些优势与限制之后，后续研究才会进一步探索：如何在大规模弱监督条件下扩展对齐范式、如何减少偏差、如何增强推理与结构理解能力。下一章将围绕这些方向，讨论对齐范式在更大规模、更复杂系统中的扩展形式。</p>
<h2 id="4-规模化对齐：ALIGN-与弱监督条件下的现实路径"><a href="#4-规模化对齐：ALIGN-与弱监督条件下的现实路径" class="headerlink" title="4. 规模化对齐：ALIGN 与弱监督条件下的现实路径"></a>4. 规模化对齐：ALIGN 与弱监督条件下的现实路径</h2><p>在讨论 CLIP 时，我们已经看到一种颇具吸引力的能力：模型可以在没有针对具体任务进行训练的情况下，直接理解图像与自然语言之间的关系，并完成分类、检索等任务。这种能力的核心在于视觉与语言表示的对齐。然而，另一个问题随之出现：这种方法是否依赖于高质量、人工精心整理的数据？</p>
<p>现实世界中的数据往往并不“干净”。网络图片常常配有模糊、片面甚至错误的文字描述。如果训练数据中充满噪声，模型还能否学会可靠的视觉语言对应关系？ALIGN（A Large-scale Image and Noisy-text embedding）正是在这一背景下提出的一项工作。它在整体结构上延续了 CLIP 的双塔对齐范式，但在数据层面采取了更为激进的策略，直接利用大规模网络图像及其伴随文本，在弱监督甚至高噪声条件下检验视觉语言对齐是否仍然成立。本章将围绕这一问题展开，重点介绍 ALIGN 所采用的规模化对齐思路，说明在弱监督甚至嘈杂数据条件下，对齐方法为何仍然可行，以及这种选择背后的现实代价。</p>
<h3 id="4-1-从-CLIP-到-ALIGN：为什么选择“规模优先”"><a href="#4-1-从-CLIP-到-ALIGN：为什么选择“规模优先”" class="headerlink" title="4.1 从 CLIP 到 ALIGN：为什么选择“规模优先”"></a>4.1 从 CLIP 到 ALIGN：为什么选择“规模优先”</h3><p>从方法框架上看，ALIGN 与 CLIP 非常相似。两者都使用双塔结构，分别对图像和文本进行编码，并通过对比学习让相关的图文在表示空间中彼此接近，不相关的样本相互远离。它们真正的差异，并不在模型结构本身，而在对训练数据的态度上。</p>
<p>CLIP 使用的是经过一定清洗和筛选的图文数据，尽量保证图片与文字之间存在较为明确的对应关系。而 ALIGN 采取了一条更为激进的路径：直接使用海量网络图像及其伴随文本，几乎不对文本质量进行严格过滤。</p>
<p>这种选择并非忽视噪声，而是基于一种更现实的判断。在互联网尺度的数据中，噪声并不是偶发现象，而是常态。与其投入巨大成本去追求“完全正确”的标注，不如接受数据本身的混乱性，通过足够大的规模，让模型在整体统计意义上学到稳定的语义关系。</p>
<p>在这一思路下，ALIGN 并不执着于每一张图片是否与文字精确匹配，而更关注整体语义空间是否成形。换句话说，模型是否能够在大范围内区分“相关”和“不相关”，比单个样本是否完美更加重要。</p>
<pre class="mermaid">flowchart LR
    subgraph C[CLIP：相对“干净”的对齐数据]
        direction LR
        C1[数据来源<br/>筛选后的图文对] --> C2[清洗策略<br/>过滤/去噪/质量控制]
        C2 --> C3[训练集特性<br/>噪声较低、匹配更明确]
        C3 --> C4[训练目标<br/>对比学习对齐]
    end

    subgraph A[ALIGN：规模优先的弱监督数据]
        direction LR
        A1[数据来源<br/>大规模网页图像+伴随文本] --> A2[清洗策略<br/>最小过滤或几乎不过滤]
        A2 --> A3[训练集特性<br/>噪声更高、匹配更松散]
        A3 --> A4[训练目标<br/>对比学习对齐]
    end

    C4 --> D[对齐结果<br/>共享语义嵌入空间]
    A4 --> D

    D --> E[关键差异<br/>CLIP：质量优先<br/>ALIGN：规模优先]</pre>
<center>图4-1 CLIP 与 ALIGN：数据来源与清洗策略对比示意</center>

<p>该图对比了 CLIP 与 ALIGN 在数据策略上的核心差异。CLIP 以相对筛选、清洗后的图文对为训练基础，强调样本匹配的明确性；ALIGN 则直接使用大规模网页图像及其伴随文本，仅进行最小过滤，以“规模”对抗噪声，并在统计意义上塑造稳定的语义嵌入空间。两者在结构与目标上相近，但在数据与规模的取舍上体现出不同的设计哲学。</p>
<h3 id="4-2-噪声为什么没有让模型失效"><a href="#4-2-噪声为什么没有让模型失效" class="headerlink" title="4.2 噪声为什么没有让模型失效"></a>4.2 噪声为什么没有让模型失效</h3><p>从直觉上看，大量不准确的图文配对似乎会严重干扰学习过程。但在对比学习的框架下，噪声的影响方式并不像想象中那样直接和致命。</p>
<p>首先，对比学习关注的是“相对关系”，而不是每个样本是否绝对正确。在训练时，模型会同时看到一大批图文对，其目标是让真实关联的配对在整体相似度排序中靠前。即便其中存在一些错误配对，只要大多数样本在统计上是合理的，模型依然可以学到稳定的语义方向。</p>
<p>其次，大规模数据自然带来了丰富的负样本。在数据量较小的情况下，少量噪声样本就可能对训练产生明显干扰；而在超大规模数据中，噪声更像是局部扰动，其影响会被大量正常样本“冲淡”。从这个角度看，规模本身起到了一种隐式约束和稳定作用。</p>
<p>此外，语言具有天然的冗余性。即使文本描述并未精确对应图像内容，往往仍然包含部分相关信息。例如，一张街景照片配上“城市风景”的描述，虽然不具体，但并非完全错误。这种“部分正确”的信号在大规模训练中依然能够为表示空间的形成提供帮助。</p>
<pre class="mermaid">flowchart LR
    A[图文训练数据<br/>包含一定比例噪声配对] --> B[对比学习训练<br/>大 batch 相似度矩阵]

    subgraph M[噪声被“稀释”的三条机制]
        direction TB

        M1[机制1：相对排序目标<br/>关注“谁更像”而非绝对正确]
        M2[机制2：负样本空间扩展<br/>噪声梯度被大量样本淹没]
        M3[机制3：语言冗余性<br/>文本常含部分相关语义信号]
    end

    B --> M1
    B --> M2
    A --> M3

    M1 --> C[整体语义方向仍可学习]
    M2 --> C
    M3 --> C

    C --> D[训练后嵌入空间<br/>仍形成稳定结构]

    subgraph N[不同噪声比例下的现象对比]
        direction TB
        N1[低噪声<br/>簇更紧、边界更清晰]
        N2[中噪声<br/>簇略松，但结构仍保留]
        N3[高噪声<br/>结构变弱，仍可能维持全局可分性]
    end

    D --> N1
    D --> N2
    D --> N3</pre>
<center>图4-2 噪声为何未摧毁对齐：规模化对比学习中的“稀释效应”示意</center>

<p>该图概括了弱监督与噪声条件下，对比学习仍能维持对齐的原因：其一，对比学习优化的是批内相似度的相对排序，少量错误配对不会线性累积为致命误差；其二，在大规模数据与大 batch 条件下，负样本空间自然扩展，噪声样本的梯度贡献更容易被整体统计结构“平均”掉，规模因此表现为一种隐式正则化；其三，文本描述往往具备语义冗余，即便不完全匹配也可能提供部分正确的语义信号，从而对嵌入空间形成产生持续的正向约束。</p>
<h3 id="4-3-规模作为隐式约束：能力提升与现实代价"><a href="#4-3-规模作为隐式约束：能力提升与现实代价" class="headerlink" title="4.3 规模作为隐式约束：能力提升与现实代价"></a>4.3 规模作为隐式约束：能力提升与现实代价</h3><p>ALIGN 的实验结果表明，只要数据规模足够大，即便在弱监督条件下，视觉语言对齐依然能够成立，并在多种任务上展现出良好的泛化能力。但这种能力并非“免费获得”。</p>
<p>从积极的一面看，规模化训练显著提升了模型对开放语义空间的覆盖能力。模型不再依赖精细的人工标注，而是通过大量样本中反复出现的统计规律，逐步建立起图像与语言之间的对应关系。这使得模型在面对新概念或新场景时，仍然能够保持一定的鲁棒性。</p>
<p>但与此同时，这一路径也对计算资源和工程系统提出了极高要求。大规模数据意味着更大的 batch、更长的训练时间，以及复杂的分布式训练系统。此外，虽然噪声在整体上被“平均”掉，但由数据分布本身带来的偏差并不会自动消失，它们可能在某些特定任务或领域中以不易察觉的方式显现出来。</p>
<p>因此，ALIGN 并不是一条绕开问题的捷径，而是一种务实的权衡选择。它接受现实世界数据的不完美，用规模和结构设计换取更强的通用表示能力。</p>
<h3 id="4-4-规模化对齐带来的方法启示"><a href="#4-4-规模化对齐带来的方法启示" class="headerlink" title="4.4 规模化对齐带来的方法启示"></a>4.4 规模化对齐带来的方法启示</h3><p>ALIGN 的价值，并不在于提供了一种唯一正确的训练方案，而在于验证了一个重要事实：视觉语言对齐并不一定依赖高质量、人工精标的数据。在弱监督甚至噪声广泛存在的条件下，只要规模足够大，对齐依然可以实现。</p>
<p>这一结论为后续多模态模型的发展奠定了现实基础。在此之后，研究的重点逐渐从“能不能做到”转向“怎样做得更好”。如何降低训练成本，如何控制和缓解噪声带来的偏差，如何让对齐表示更好地服务于生成、推理等更复杂任务，成为新的关注方向。</p>
<p><strong>小结：在真实世界中实现对齐</strong><br>本章以 ALIGN 为核心，说明了视觉语言对齐在弱监督和大规模数据条件下的可行性。通过规模优先的策略，对比学习目标在嘈杂环境中依然能够塑造稳定的语义空间，使对齐方法从理想化设想走向现实应用。</p>
<p>同时，这一路径也清晰地揭示了规模化训练的边界。规模并不能自动消除偏差，对齐本身也并不意味着模型已经具备可直接使用的能力。正是在认识这些限制的基础上，后续工作开始探索如何将对齐表示进一步融入更完整的多模态系统中，使模型不仅能够建立联系，还能够在复杂任务中发挥作用。下一章将围绕这一演进过程展开讨论。</p>
<h2 id="5-从表示到接口：BLIP2-与多模态系统的形成"><a href="#5-从表示到接口：BLIP2-与多模态系统的形成" class="headerlink" title="5. 从表示到接口：BLIP2 与多模态系统的形成"></a>5. 从表示到接口：BLIP2 与多模态系统的形成</h2><p>在前面的章节中，视觉语言对齐被反复讨论。无论是 CLIP 还是 ALIGN，它们都展示了一点：只要训练方式得当，图像和文本可以被映射到同一个语义空间中，从而实现跨模态理解。这种能力在分类、检索等任务中已经非常实用。</p>
<p>但当任务进一步升级，例如生成图像描述、回答关于图片的问题，甚至进行多轮多模态对话时，仅有一个“对齐得很好”的表示空间就显得不够了。此时，一个更现实的问题浮现出来：这些对齐后的表示，如何才能真正参与到复杂系统中，被“用起来”而不仅仅是“存在”。</p>
<p>BLIP2 正是在这一背景下提出的。作为一种面向生成式多模态系统的设计方案，BLIP2 并未试图重新训练一个端到端的视觉语言大模型，而是<strong>通过引入中间接口模块，将已有的视觉编码器与大语言模型连接起来，使对齐后的视觉表示能够被语言模型直接消费和利用</strong>。这一设计标志着视觉语言对齐从“表示层能力”走向“系统级接口”的关键一步。</p>
<p>本章将围绕这一转变展开，重点介绍 BLIP2 以及检索增强生成在视觉场景下的应用，说明视觉语言对齐是如何从一种表示学习成果，逐步演化为多模态系统中的关键接口。</p>
<h3 id="5-1-从对齐到生成：问题发生了变化"><a href="#5-1-从对齐到生成：问题发生了变化" class="headerlink" title="5.1 从对齐到生成：问题发生了变化"></a>5.1 从对齐到生成：问题发生了变化</h3><p>CLIP 和 ALIGN 的成功，说明图像和文本可以在同一个语义空间中找到对应关系。但需要注意的是，这种对齐主要发生在向量层面，其输出是连续的数值表示，而不是语言模型可以直接“说出来”的文字。</p>
<p>在检索或分类任务中，这并不是问题。模型只需要判断相似度高低即可。但在生成式任务中，情况就不同了。大语言模型的工作对象是一个个离散的词或子词，而不是连续向量。换句话说，CLIP 学到的视觉表示，并不能直接接到语言模型的输入端。</p>
<p>一种看似直接的做法，是把视觉模型和语言模型整体连起来，进行端到端训练。但在实践中，这种方式往往训练成本极高，而且非常容易不稳定。模型规模越大，这个问题就越突出。</p>
<p>因此，研究的焦点逐渐转向一个更实际的问题：是否可以在保留已有对齐表示优势的前提下，通过某种“中间层”，把视觉信息转换成语言模型能够理解和利用的形式。BLIP2 正是在这样的背景下提出的。</p>
<h3 id="5-2-BLIP2：把对齐表示当作接口来用"><a href="#5-2-BLIP2：把对齐表示当作接口来用" class="headerlink" title="5.2 BLIP2：把对齐表示当作接口来用"></a>5.2 BLIP2：把对齐表示当作接口来用</h3><p>BLIP2 的核心思想，可以理解为对多模态系统进行清晰的分工。整个系统由三部分组成：一个已经训练好的视觉编码器，一个已经训练好的大语言模型，以及夹在两者之间的一个轻量级模块，称为 Q-Former。</p>
<p>在这个设计中，视觉模型负责“看”，语言模型负责“说”，而 Q-Former 的任务是充当翻译者。它从视觉编码器输出的大量特征中，挑选出对语言生成最有价值的信息，并将这些信息整理成语言模型可以接收的形式。</p>
<p>一个关键点在于，视觉模型和语言模型在训练过程中都是冻结的。也就是说，它们本身并不会被重新大规模调整。真正被训练的，主要是中间的 Q-Former。这种做法大大降低了训练成本，也让系统更容易稳定收敛。</p>
<p>更重要的是，这种结构重新定义了视觉语言对齐的角色。对齐后的视觉表示不再需要直接承担生成任务，而是作为一种信息接口，为语言模型提供高质量的视觉线索。</p>
<pre class="mermaid">flowchart LR
    direction LR

    subgraph V[视觉侧（冻结）]
        V1[视觉编码器<br/>ViT / CNN] --> V2[视觉特征序列<br/>Patch Embeddings]
    end

    subgraph Q[中间接口层（可训练）]
        Q1[Q-Former<br/>可学习查询向量] --> Q2[压缩后的视觉表示<br/>Query Outputs]
    end

    subgraph L[语言侧（冻结）]
        L1[大语言模型<br/>LLM] --> L2[文本生成<br/>Tokens]
    end

    V2 --> Q1
    Q2 --> L1

    note1[关键设计取舍<br/>• 冻结视觉模型<br/>• 冻结语言模型<br/>• 仅训练中间接口]
    Q1 --- note1

    note2[核心作用<br/>将连续视觉特征<br/>转化为语言模型<br/>可使用的信息接口]
    Q2 --- note2</pre>
<center>图5-1 BLIP2 分层解耦结构：对齐表示作为生成接口</center>

<h3 id="5-3-从向量对齐到语言可用的信息"><a href="#5-3-从向量对齐到语言可用的信息" class="headerlink" title="5.3 从向量对齐到语言可用的信息"></a>5.3 从向量对齐到语言可用的信息</h3><p>与 CLIP 不同，BLIP2 并不试图在一个统一的向量空间中完成所有跨模态交互。它承认一个现实差异：视觉模型和语言模型的工作方式并不相同，因此需要一个过渡步骤。</p>
<p>在 CLIP 中，对齐的目标是让图像和文本在空间中“靠得近”。而在 BLIP2 中，对齐的标准变成了：这些视觉信息是否真的能被语言模型用来生成合理的输出。</p>
<p>可以把这一变化理解为对齐层级的提升。向量层面的对齐解决的是“语义是否相关”的问题，而面向生成的对齐，则关心“这些语义能否参与推理和表达”。BLIP2 并没有推翻前者，而是在其基础上，增加了一层面向使用的结构。</p>
<h3 id="5-4-RAG-Vision：把对齐空间变成系统资源"><a href="#5-4-RAG-Vision：把对齐空间变成系统资源" class="headerlink" title="5.4 RAG-Vision：把对齐空间变成系统资源"></a>5.4 RAG-Vision：把对齐空间变成系统资源</h3><p>如果说 BLIP2 解决的是“如何把一张图片的信息交给语言模型”，那么检索增强生成在视觉场景下的应用，则回答了另一个问题：如何在系统层面使用已经对齐好的表示。</p>
<p>在这种框架中，对齐模型学到的嵌入空间被当作一种可查询的索引。系统在接收到输入后，首先在这个空间中检索出最相关的图像或文本片段，然后将这些结果作为上下文交给语言模型，用于后续生成。</p>
<p>这种设计的好处在于分工清晰。对齐模型负责找到“哪些内容相关”，语言模型负责“如何组织和表达”。两者通过检索结果连接，而不是在参数层面深度绑定。这使系统更容易扩展，也更容易调试和控制。  </p>
<p>从更高的角度看，这种方式把对齐表示从模型内部的一种能力，提升为整个系统可反复利用的资源。嵌入空间不再只是训练的中间产物，而是成为多模态系统中的长期资产。</p>
<pre class="mermaid">flowchart LR
    direction LR

    A[用户查询<br/>文本 / 图像] --> B[查询编码<br/>CLIP-like 编码器]
    B --> C[查询嵌入向量<br/>q]

    subgraph M[多模态记忆库（可扩充）]
        direction TB
        M1[图像/文本/片段] --> M2[离线编码<br/>CLIP-like]
        M2 --> M3[嵌入索引<br/>向量数据库]
    end

    C --> D[相似度检索<br/>Top-K 近邻]
    M3 --> D

    D --> E[检索结果<br/>相关图像/文本/片段]
    E --> F[上下文构造<br/>拼接/格式化]
    F --> G[大语言模型<br/>生成/推理]
    G --> H[输出结果<br/>回答/描述/决策]

    subgraph K[关键解耦关系]
        direction TB
        K1[对齐模型负责<br/>语义匹配与检索]
        K2[语言模型负责<br/>生成与推理]
    end

    D --- K1
    G --- K2</pre>
<center>图5-2 RAG-Vision 系统流程：对齐嵌入空间作为可查询的多模态记忆</center>

<h3 id="5-5-对齐范式在系统中的新位置"><a href="#5-5-对齐范式在系统中的新位置" class="headerlink" title="5.5 对齐范式在系统中的新位置"></a>5.5 对齐范式在系统中的新位置</h3><p>通过 BLIP2 和 RAG-Vision 可以看到，视觉语言对齐的角色已经发生了明显变化。它不再只是为了提升某个模型的性能，而是逐渐演化为连接不同模块的公共接口。</p>
<p>这种接口化的意义在于灵活性。只要接口保持稳定，视觉模型可以升级，语言模型也可以替换，而整体系统结构不需要推倒重来。这种设计思路对于复杂工程系统尤为重要，也是多模态模型能够快速演进的重要原因之一。</p>
<p><strong>小结：从表示学习走向系统设计</strong><br>本章围绕 BLIP2 和检索增强生成，展示了视觉语言对齐如何从表示层走向系统层。通过引入中间模块和检索机制，对齐表示被成功融入生成式多模态系统，成为连接视觉感知与语言推理的关键桥梁。</p>
<p>这一阶段的探索表明，对齐范式的真正价值，并不仅在于学习到一个“好看的表示空间”，而在于为多模态系统提供一种稳定、可扩展、可复用的语义接口。正是在这样的基础上，研究开始进一步思考对齐思想是否可以推广到更广泛的信息视角之中。下一章将围绕这一延展方向展开讨论。</p>
<h2 id="6-对齐范式的外溢：非语言多视角场景中的统一表示"><a href="#6-对齐范式的外溢：非语言多视角场景中的统一表示" class="headerlink" title="6. 对齐范式的外溢：非语言多视角场景中的统一表示"></a>6. 对齐范式的外溢：非语言多视角场景中的统一表示</h2><p>在前面的讨论中，视觉语言对齐始终围绕“图像与文本如何对应”这一问题展开。但如果暂时不去关注具体的输入形式，而是把视角拉高，可以发现一个更本质的共通点：无论是否包含语言，对齐真正要解决的，并不是模态之间的差异，而是<strong>同一个世界在不同观测方式下如何被统一理解</strong>。</p>
<p>换句话说，当同一环境、同一物体或同一状态，被从不同角度、不同传感器、不同表达方式观察到时，如何让模型意识到“它们说的是同一件事”，并在内部形成稳定、可复用的表示，这才是对齐范式的核心。</p>
<p>本章正是从这一更抽象的角度出发，说明对齐思想如何脱离语言语境，在纯感知场景中以另一种形式出现。通过 BEV 表示学习这一代表性案例，可以看到，对齐并不是语言模型的专属技巧，而是一种更广泛的表示学习原则。</p>
<h3 id="6-1-从多模态到多视角：问题并没有本质变化"><a href="#6-1-从多模态到多视角：问题并没有本质变化" class="headerlink" title="6.1 从多模态到多视角：问题并没有本质变化"></a>6.1 从多模态到多视角：问题并没有本质变化</h3><p>在视觉语言模型中，常用“模态”来区分图像和文本。但如果进一步思考，可以发现所谓的模态差异，本质上是一种<strong>信息视角的差异</strong>。图像从像素层面描述世界，语言从抽象语义层面描述世界，它们只是同一现实的不同观察方式。</p>
<p>在自动驾驶、多摄像头感知等场景中，虽然没有语言参与，但问题结构是相似的。多个摄像头从不同位置拍摄同一条道路，画面在角度、遮挡和光照上差异很大，但它们指向的是同一个物理环境。系统真正关心的，并不是“这张图来自哪个摄像头”，而是“环境中发生了什么”。</p>
<p>因此，多视角感知可以被看作一种不含语言的多模态问题。它的目标与视觉语言对齐高度一致：让模型学会忽略视角带来的表面差异，抓住与真实环境状态相关的核心信息。</p>
<h3 id="6-2-BEV-表示学习：一种没有语言的对齐方式"><a href="#6-2-BEV-表示学习：一种没有语言的对齐方式" class="headerlink" title="6.2 BEV 表示学习：一种没有语言的对齐方式"></a>6.2 BEV 表示学习：一种没有语言的对齐方式</h3><p>BEV，也就是鸟瞰视角表示，为这一思想提供了非常直观的实践例子。在多摄像头系统中，每个摄像头看到的画面都不同，但这些画面都可以被映射到同一个俯视坐标系中。</p>
<p>在这个统一空间里，不同摄像头拍到的同一辆车，会被投影到相同或相邻的位置。模型在 BEV 空间中进行特征融合时，不再需要关心“这个目标是从哪个角度看到的”，而只关心它在空间中的位置和状态。</p>
<p>从本质上看，这就是一种对齐操作。不同视角下的观测被强制对应到同一空间结构中，视角差异被显式消除。模型通过这种方式，逐步学会一种对视角不敏感、对环境更稳定的表示。  </p>
<p>虽然 BEV 表示学习并不依赖语言监督，其约束主要来自几何关系和投影一致性，但在思想层面，它与视觉语言对齐非常接近。不同视角的图像，就像不同“表达方式”，而 BEV 空间则扮演了统一语义空间的角色。</p>
<pre class="mermaid">flowchart LR
    direction LR

    subgraph CAM[多摄像头观测（不同视角）]
        direction TB
        C1[相机1图像<br/>前视] --> F1[图像特征提取<br/>Backbone]
        C2[相机2图像<br/>左视] --> F2[图像特征提取<br/>Backbone]
        C3[相机3图像<br/>右视] --> F3[图像特征提取<br/>Backbone]
        C4[相机4图像<br/>后视] --> F4[图像特征提取<br/>Backbone]
    end

    subgraph PROJ[几何对齐（投影到统一坐标系）]
        direction TB
        P1[视角变换/投影<br/>基于标定与几何关系]
        P2[统一 BEV 网格<br/>Bird's-Eye View]
    end

    F1 --> P1
    F2 --> P1
    F3 --> P1
    F4 --> P1
    P1 --> P2

    subgraph BEV[BEV 空间融合与理解（视角不变表示）]
        direction TB
        B1[BEV 特征融合<br/>Fusion] --> B2[全局一致表示<br/>稳定环境理解]
        B2 --> B3[下游任务<br/>检测/跟踪/规划]
    end

    P2 --> B1

    note1[对齐信号来源<br/>几何一致性 + 多视角投影约束]
    P1 --- note1

    note2[核心效果<br/>同一物体在不同视角下<br/>映射到同一 BEV 位置]
    P2 --- note2</pre>
<center>图6-1 多摄像头到 BEV：空间对齐与特征融合示意</center>
### 6.3 对齐信号不同，但核心目标一致
当然，BEV 表示学习和视觉语言对齐在具体实现上存在明显差异。前者依赖的是几何一致性，后者依赖的是语义一致性。一个强调“是不是同一个位置”，另一个强调“是不是同一个意思”。

<p>但如果从更高层次来看，两者都在做同一件事：<strong>为不同观测建立等价关系</strong>。只要这种等价关系存在，模型就可以把多样化的输入映射到一个共享表示空间中。</p>
<p>这种共享表示的价值，在下游任务中体现得尤为明显。在视觉语言模型中，它支持跨任务迁移和零样本能力；在 BEV 系统中，它支撑检测、跟踪和规划等多个模块共享同一中间表示。</p>
<p>因此，对齐并不是某一类数据或某一种任务的专属技巧，而是一种关于“如何构建可复用表示”的通用方法。</p>
<h3 id="6-4-把模态理解为信息视角"><a href="#6-4-把模态理解为信息视角" class="headerlink" title="6.4 把模态理解为信息视角"></a>6.4 把模态理解为信息视角</h3><p>如果将“模态”重新理解为“信息视角”，许多原本分散的表示学习方法就会显得更加统一。无论是图像与文本，还是前视摄像头与侧视摄像头，它们的共同点在于：都以不同方式观察同一个世界。</p>
<p>在这一视角下，对齐学习可以被看作是在刻画“不变性”。模型通过对齐不同视角下的观测，逐渐学会忽略那些只与观察方式有关的变化，而保留与真实环境状态相关的结构信息。</p>
<p>这种思路与传统只关注标签预测的训练方式形成了鲜明对比。它更强调对世界结构本身的建模，而不是对某一个任务目标的拟合。这也是对齐范式能够支持长期复用和跨任务迁移的重要原因。</p>
<p><strong>小结：对齐是一种通用的表示思想</strong><br>通过 BEV 表示学习这一案例，可以清楚地看到，对齐范式并不依赖语言的存在。在不涉及文本的场景中，只要存在多个视角对同一世界的观测，对齐思想就会自然出现。</p>
<p>这一观察进一步说明，对齐并不是视觉语言模型中的“特殊技巧”，而是一种更普遍的表示学习原则。理解这一点，有助于将 CLIP、ALIGN、BLIP2 等工作放在更大的认知框架中，也为进一步探索从对齐走向更完整的世界模型提供了重要线索。下一章将回到实践层面，结合具体实验和工程流程，讨论在有限资源条件下应用这些思想时所面临的现实问题。</p>
<h2 id="7-训练与评估一个-CLIP-like-视觉语言对齐模型"><a href="#7-训练与评估一个-CLIP-like-视觉语言对齐模型" class="headerlink" title="7. 训练与评估一个 CLIP-like 视觉语言对齐模型"></a>7. 训练与评估一个 CLIP-like 视觉语言对齐模型</h2><p>在前面的章节中，视觉语言对齐被反复讨论为一种重要的表示学习范式。然而，一个始终无法回避的现实问题是：这些方法是否只在大公司、超大算力和海量数据条件下才成立？当计算资源有限、数据规模中等时，CLIP 式的视觉语言对齐是否仍然具有实践价值？</p>
<p>本章并不试图给出一个完整可运行的工程实现，也不假设具备工业级的训练条件。相反，这里关注的是<strong>在现实约束下，如何理解和实践 CLIP 式视觉语言对齐，以及如何判断“对齐是否真的发生”</strong>。重点不在于复现最高性能，而在于澄清哪些因素真正重要、哪些信号值得信赖，以及哪些工程现象在有限资源条件下尤为关键。</p>
<p>从这一意义上说，本章更接近一份实践经验与判断逻辑的总结。它尝试回答的问题不是“应该怎样把模型做到最好”，而是“如果需要在受限条件下做这件事，应该如何做出合理取舍，并避免常见误判”。</p>
<h3 id="7-1-实践目标与判断路径"><a href="#7-1-实践目标与判断路径" class="headerlink" title="7.1 实践目标与判断路径"></a>7.1 实践目标与判断路径</h3><p>在资源受限的前提下实践 CLIP 式视觉语言对齐，一个合理的目标并非追求最优性能指标，而是确认模型是否真的学到了稳定、可复用的图文对应关系。因此，实践目标可以被概括为三个层次。</p>
<p>首先，需要判断图像与文本是否在嵌入空间中形成了基本一致的语义结构，而不仅仅是在训练数据上拟合相似度。其次，应通过无需额外训练的方式，例如图文检索或 zero-shot 分类，验证这种结构是否具备实际可用性。最后，在整个过程中，需要识别哪些工程因素对稳定性和结果影响最大，从而将有限的资源投入到最关键的位置。</p>
<p>围绕这一目标，CLIP 式对齐的实践通常遵循一个相对固定的流程：构建图文数据、使用双塔模型进行对比学习训练，并在冻结参数的条件下，通过多种行为层面的评估方式检查对齐是否发生。该流程的关键不在于每一步的复杂度，而在于是否能够为判断表示质量提供清晰、可靠的信号。</p>
<h3 id="7-2-数据构建：贴近现实而非制造理想条件"><a href="#7-2-数据构建：贴近现实而非制造理想条件" class="headerlink" title="7.2 数据构建：贴近现实而非制造理想条件"></a>7.2 数据构建：贴近现实而非制造理想条件</h3><p>在有限资源条件下，数据规模通常只能控制在数万到数十万对图文样本之间。这一规模更接近常见研究或个人工程环境，而非工业级训练设置。数据来源可以是公开数据集的子集，或通过简单规则从网络资源中构建。</p>
<p>在这种情况下，过度追求“干净数据”往往得不偿失。一方面，精细的人工筛选成本高昂；另一方面，也会人为制造一个与真实应用脱节的理想环境。因此，更现实的做法是仅进行最低限度的清洗，例如移除空文本、无法加载的图像，以及明显无关的配对，而接受其余噪声的存在。</p>
<p>在实践中可以观察到，噪声形式非常多样：有些文本描述过于宽泛，有些只与图像部分相关，也有些几乎完全不匹配。但正如前文分析 ALIGN 时所强调的那样，这类噪声并不会自动导致对齐失败。其影响往往取决于数据规模是否足够支撑统计规律的形成，以及训练过程是否具备足够的稳定性。</p>
<h3 id="7-3-模型结构与训练配置：哪些选择最值得投入精力"><a href="#7-3-模型结构与训练配置：哪些选择最值得投入精力" class="headerlink" title="7.3 模型结构与训练配置：哪些选择最值得投入精力"></a>7.3 模型结构与训练配置：哪些选择最值得投入精力</h3><p>在算力和规模受限的条件下，并非所有模型配置都同等重要。实践中更关键的问题是：<strong>哪些选择真正影响对齐是否发生</strong>。</p>
<p>从结构上看，CLIP 式的双塔设计本身已经足够简洁有效。只要图像编码器与文本编码器具备基本的表达能力，其具体规模通常不是主要瓶颈。相比之下，训练配置对嵌入空间结构的影响往往更加直接。</p>
<p>其中，batch size 是最值得优先关注的因素之一。较大的 batch 能够在一次更新中提供更丰富的负样本，有助于稳定相似度排序。在显存受限的情况下，通过梯度累积获得较大的等效 batch，往往比更换模型结构更有效。</p>
<p>另一个关键调节项是温度参数。温度决定了模型区分“相似”与“不相似”的力度，其设置不当会直接导致嵌入分布塌缩或过度分散。在有限资源条件下，优先保证温度行为合理，往往比追求更深或更大的模型更有价值。</p>
<h3 id="7-4-训练过程中哪些信号值得相信"><a href="#7-4-训练过程中哪些信号值得相信" class="headerlink" title="7.4 训练过程中哪些信号值得相信"></a>7.4 训练过程中哪些信号值得相信</h3><p>在 CLIP 式对齐训练中，一个常见误区是过度依赖对比损失的数值变化来判断训练是否成功。实践中需要明确的是，<strong>loss 下降并不等价于对齐发生</strong>。</p>
<p>在资源受限条件下，可能会观察到 loss 持续下降，但图文检索行为几乎没有改善。这通常意味着模型更好地拟合了训练目标，却尚未形成可复用的语义结构。因此，相比 loss 曲线，检索排序的变化往往是更可靠的信号。</p>
<p>此外，嵌入分布的形态也比单一指标更值得关注。当温度或 batch 设置不合理时，嵌入向量可能出现过度集中或无序分散的情况，这些现象通常会直接反映在检索结果的不稳定上。通过观察这些行为层面的信号，可以更早判断训练是否偏离预期方向。</p>
<h3 id="7-5-图文检索：判断对齐是否发生的首要手段"><a href="#7-5-图文检索：判断对齐是否发生的首要手段" class="headerlink" title="7.5 图文检索：判断对齐是否发生的首要手段"></a>7.5 图文检索：判断对齐是否发生的首要手段</h3><p>在所有评估方式中，图文检索是验证视觉语言对齐是否成立的最直接手段。评估时，模型参数保持冻结，仅使用训练完成后的嵌入进行相似度搜索。</p>
<p>实践中通常会在“用图像找文本”和“用文本找图像”两个方向进行检索评估。只要数据规模和 batch 设置合理，模型往往能够在未见样本上找回语义相关的配对。这一现象表明，对齐并非仅在训练集内部成立，而具备一定的泛化能力。</p>
<p>对失败案例的分析同样重要。很多检索错误并不意味着模型完全失效，而是源于文本描述本身的模糊性或多义性。这类现象反而揭示了语言在对齐任务中的复杂角色。</p>
<h3 id="7-6-Zero-shot-分类：接口价值的直接体现"><a href="#7-6-Zero-shot-分类：接口价值的直接体现" class="headerlink" title="7.6 Zero-shot 分类：接口价值的直接体现"></a>7.6 Zero-shot 分类：接口价值的直接体现</h3><p>在 zero-shot 分类中，每个类别会被写成若干自然语言描述，并通过文本编码器生成对应向量。图像嵌入与这些类别向量进行相似度比较，从而完成分类。</p>
<p>即便在有限数据条件下，模型通常也能取得明显优于随机的表现。不同文本描述方式对结果影响显著，使用多种描述并进行简单集成，往往可以提升稳定性。</p>
<p>尽管 zero-shot 分类的性能通常不如专门训练的分类器，但它无需额外训练、可以快速适配新任务。这一特性清晰体现了对齐表示作为通用接口的价值。</p>
<h3 id="7-7-CLIPScore-与一致性判断的边界"><a href="#7-7-CLIPScore-与一致性判断的边界" class="headerlink" title="7.7 CLIPScore 与一致性判断的边界"></a>7.7 CLIPScore 与一致性判断的边界</h3><p>除了检索和分类，还可以使用 CLIPScore 等指标评估图像与文本之间的整体匹配程度。该指标基于嵌入相似度，用于衡量文本描述与图像内容的一致性。</p>
<p>实践中需要注意的是，高 CLIPScore 并不等同于语义完全正确。这一现象再次提醒，对齐能力并不等价于深层理解。但作为一种高效、可复用的一致性判断工具，CLIPScore 在工程实践中仍然具有重要价值。</p>
<h3 id="7-8-实践总结：在有限条件下如何正确看待对齐"><a href="#7-8-实践总结：在有限条件下如何正确看待对齐" class="headerlink" title="7.8 实践总结：在有限条件下如何正确看待对齐"></a>7.8 实践总结：在有限条件下如何正确看待对齐</h3><p>综合以上讨论，可以得出几个清晰结论。首先，CLIP 式视觉语言对齐在有限资源条件下依然是可行的，其核心能力并非完全依赖极端规模。其次，batch size、温度参数和数据噪声，对结果的影响往往超过模型规模本身。</p>
<p>更重要的是，这一实践视角再次印证了前文的核心观点：对齐表示的真正价值，不在于单一任务的最优性能，而在于其作为通用接口的可复用性。即便性能并不突出，一个结构稳定的对齐空间，依然能够支撑多种下游形式。</p>
<p>本章从工程与判断逻辑的角度，为视觉语言对齐范式补上了现实注脚。它既展示了这一方法在资源受限条件下的可行性，也清楚地揭示了其边界。下一章将回到整体视角，对全文进行总结，讨论对齐范式真正改变了什么，又有哪些关键问题仍有待解决。</p>
<h2 id="8-总结与展望：对齐范式改变了什么，又尚未解决什么"><a href="#8-总结与展望：对齐范式改变了什么，又尚未解决什么" class="headerlink" title="8. 总结与展望：对齐范式改变了什么，又尚未解决什么"></a>8. 总结与展望：对齐范式改变了什么，又尚未解决什么</h2><p>在前面的章节中，已经从多个角度梳理了视觉语言对齐的发展脉络。从最初的想法萌芽，到 CLIP 和 ALIGN 的规模化实践，再到 BLIP2 和检索增强生成等系统级应用，对齐范式逐渐从一种表示学习技巧，演变为多模态系统中的关键支撑。</p>
<p>在这一章中，将暂时放下具体模型与实验细节，从更高层次回顾一个核心问题：视觉语言对齐真正改变了什么？与此同时，又有哪些问题并未因此迎刃而解，仍然构成未来研究与工程实践中的挑战。</p>
<h3 id="8-1-对齐范式真正带来的改变"><a href="#8-1-对齐范式真正带来的改变" class="headerlink" title="8.1 对齐范式真正带来的改变"></a>8.1 对齐范式真正带来的改变</h3><p>从整体上看，视觉语言对齐并没有完全否定传统视觉学习的路径，但它显著改变了表示学习的关注重点和组织方式。</p>
<p>首先，对齐范式推动了一种从“任务导向”到“接口导向”的转变。以往的视觉模型，往往围绕某一个明确任务进行训练，表示的价值主要体现在该任务的性能指标上。而对齐模型的目标，则是先构建一个通用的表示空间，再让不同任务围绕这一表示展开。分类、检索、生成等能力，不再需要各自独立训练模型，而是共享同一个语义接口。</p>
<p>其次，对齐范式引入了自然语言作为视觉系统的开放语义空间。与固定、封闭的标签体系相比，自然语言具有高度灵活性和组合性，可以表达更丰富、更抽象的概念。这一变化使视觉模型第一次在结构上具备了 zero-shot 的可能性，即在没有针对某一任务进行专门训练的情况下，也能做出合理判断。</p>
<p>再次，对齐范式改变了模型与系统之间的关系。通过双塔结构和嵌入空间设计，表示不再只是模型内部的中间结果，而是可以被存储、检索和复用的系统资源。在多模态系统中，这种表示逐渐成为连接感知模块与生成模块的关键纽带，使系统更易扩展，也更容易演进。</p>
<p>从这些变化可以看到，对齐范式的影响并不仅限于性能提升，而是重新塑造了多模态系统的整体组织方式。  </p>
<h3 id="8-2-对齐范式的现实边界"><a href="#8-2-对齐范式的现实边界" class="headerlink" title="8.2 对齐范式的现实边界"></a>8.2 对齐范式的现实边界</h3><p>尽管对齐范式带来了显著进展，但其能力边界同样清晰，且不应被忽视。</p>
<p>首先，对齐并不等同于理解。无论是 zero-shot 分类，还是基于相似度的评分指标，本质上都依赖嵌入空间中的“相像程度”。模型可以判断图像和文本是否匹配，但这并不意味着它真正理解了其中的因果关系、物理规律或隐含逻辑。在涉及复杂推理或精细空间关系的场景中，这一差距尤为明显。</p>
<p>其次，对齐能力高度依赖训练数据的分布。大规模弱监督数据可以在统计意义上塑造稳定的表示，但其中包含的偏差同样会被模型继承。当应用场景发生明显变化时，对齐模型的表现往往会出现波动。这类问题并不能简单地通过继续扩大数据规模来彻底解决。</p>
<p>再次，当前对齐范式更擅长建模“相似性”，而对关系、比较和逻辑结构的刻画能力有限。语言中的关系信息，并不会自动转化为模型内部可操作的推理机制。这一限制在现有框架下仍然普遍存在。</p>
<h3 id="8-3-从对齐走向更完整世界模型的挑战"><a href="#8-3-从对齐走向更完整世界模型的挑战" class="headerlink" title="8.3 从对齐走向更完整世界模型的挑战"></a>8.3 从对齐走向更完整世界模型的挑战</h3><p>如果将视觉语言对齐视为多模态学习中的一个重要阶段性成果，那么接下来的问题在于：如何在此基础上，进一步提升模型对世界结构的刻画能力。</p>
<p>其中一个关键挑战，是如何在对齐之后引入更明确的结构表示。当前模型更多关注“是否相关”，而较少关心“如何相关”。将关系建模、因果约束或物理一致性纳入表示学习目标，被认为是向更高层次理解迈进的重要一步。</p>
<p>另一个重要方向是时间和动态建模。现有对齐方法多以静态图像和文本为基本单元，但真实世界的理解往往依赖时间维度上的连续变化。如何在对齐框架下处理视频、事件和长期记忆，是多模态系统走向更复杂能力的必要条件。</p>
<p>此外，在工程层面，还需要在通用性和任务适配之间寻找更好的平衡。模块化接口带来了灵活性，但也可能引入信息瓶颈。如何在解耦结构的同时，保留足够的表达能力，仍然缺乏统一而成熟的解决方案。</p>
<h3 id="8-4-结语：对齐是起点，而不是终点"><a href="#8-4-结语：对齐是起点，而不是终点" class="headerlink" title="8.4 结语：对齐是起点，而不是终点"></a>8.4 结语：对齐是起点，而不是终点</h3><p>综合来看，视觉语言对齐并不是多模态智能的终点形态，而是起点。它通过重新组织表示学习的目标，使模型第一次能够在开放语义空间中获得稳定、可复用的能力，并为多模态系统的工程化落地提供了现实可行的路径。</p>
<p>但需要清醒地认识到，对齐主要解决的是“不同观测如何对应”的问题，而不是“世界本身如何运作”的问题。真正通用的智能系统，仍然需要在对齐表示之上，引入结构、因果和动态建模能力。</p>
<p>正是在这一意义上，对齐范式的价值并不在于给出最终答案，而在于为后续探索提供了一个清晰而坚实的出发点。未来的多模态研究，正是从这一起点出发，逐步尝试从语义对齐，走向对世界更系统、更深入的理解。</p>
<script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';	mermaid.initialize({startOnLoad: true, flowchart: {curve: 'linear'}}); </script>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2026/02/10/056-visual-multimodal/" title="视觉多模态：CLIP、ALIGN 与视觉语言对齐">https://www.keychan.xyz/2026/02/10/056-visual-multimodal/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag"># 对比学习</a>
              <a href="/tags/CLIP/" rel="tag"># CLIP</a>
              <a href="/tags/ALIGN/" rel="tag"># ALIGN</a>
              <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"># 多模态</a>
              <a href="/tags/RAG-Vision/" rel="tag"># RAG-Vision</a>
              <a href="/tags/%E7%B3%BB%E7%BB%9F%E7%BA%A7%E6%8E%A5%E5%8F%A3/" rel="tag"># 系统级接口</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/02/03/055-visual-self-supervised-learning/" rel="prev" title="视觉自监督学习：从对比学习到 MAE，再到通用视觉表征">
                  <i class="fa fa-angle-left"></i> 视觉自监督学习：从对比学习到 MAE，再到通用视觉表征
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2026/02/17/057-the-evolutionary-history-of-gpt/" rel="next" title="GPT 的进化史：从语言建模到世界建模">
                  GPT 的进化史：从语言建模到世界建模 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">533k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2026/02/10/056-visual-multimodal/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
