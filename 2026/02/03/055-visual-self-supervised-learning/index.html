<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":272,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. 引言：自监督学习，真的能“看懂”图像吗？在很长一段时间里，计算机视觉的进步，几乎完全依赖一种方式：监督学习。简单来说，就是给模型大量图片，再告诉它“这是什么”，让它不断做题、纠错、记答案。 ImageNet 分类、COCO 检测与分割，这些经典成果，都是在这种模式下取得的。只要数据够多、标签够准，模型的分数就能不断刷新。但随着模型越来越大、应用场景越来越真实，这条路开始遇到问题了：  标注图">
<meta property="og:type" content="article">
<meta property="og:title" content="视觉自监督学习：从对比学习到 MAE，再到通用视觉表征">
<meta property="og:url" content="https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. 引言：自监督学习，真的能“看懂”图像吗？在很长一段时间里，计算机视觉的进步，几乎完全依赖一种方式：监督学习。简单来说，就是给模型大量图片，再告诉它“这是什么”，让它不断做题、纠错、记答案。 ImageNet 分类、COCO 检测与分割，这些经典成果，都是在这种模式下取得的。只要数据够多、标签够准，模型的分数就能不断刷新。但随着模型越来越大、应用场景越来越真实，这条路开始遇到问题了：  标注图">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2026-02-03T13:19:12.000Z">
<meta property="article:modified_time" content="2026-02-03T13:19:48.876Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="迁移学习">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="自监督学习">
<meta property="article:tag" content="对比学习">
<meta property="article:tag" content="MAE">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/","path":"2026/02/03/055-visual-self-supervised-learning/","title":"视觉自监督学习：从对比学习到 MAE，再到通用视觉表征"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>视觉自监督学习：从对比学习到 MAE，再到通用视觉表征 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80%EF%BC%9A%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%9C%9F%E7%9A%84%E8%83%BD%E2%80%9C%E7%9C%8B%E6%87%82%E2%80%9D%E5%9B%BE%E5%83%8F%E5%90%97%EF%BC%9F"><span class="nav-text">1. 引言：自监督学习，真的能“看懂”图像吗？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%BE%88%E6%88%90%E5%8A%9F%EF%BC%8C%E4%BD%86%E7%9C%9F%E7%9A%84%E6%B2%A1%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="nav-text">1.1 监督学习很成功，但真的没问题吗？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-%E5%9B%BE%E7%89%87%E5%A4%9A%E3%80%81%E6%A0%87%E7%AD%BE%E5%A4%9A%EF%BC%8C%E4%B8%8D%E7%AD%89%E4%BA%8E%E6%A8%A1%E5%9E%8B%E6%87%82%E4%BA%86"><span class="nav-text">1.1.1 图片多、标签多，不等于模型懂了</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-ImageNet-%E7%9A%84%E6%88%90%E5%8A%9F%EF%BC%8C%E4%B8%BA%E4%BD%95%E9%9A%BE%E4%BB%A5%E5%A4%8D%E5%88%B6%EF%BC%9F"><span class="nav-text">1.1.2 ImageNet 的成功，为何难以复制？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%83%B3%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-text">1.2 自监督学习想解决什么问题？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-%E4%B8%8D%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%A0%87%E7%AD%BE%EF%BC%8C%E6%A8%A1%E5%9E%8B%E6%80%8E%E4%B9%88%E5%AD%A6%EF%BC%9F"><span class="nav-text">1.2.1 不用人工标签，模型怎么学？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%8F%AA%E6%98%AF%E2%80%9C%E6%9B%B4%E8%8A%B1%E5%93%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%90%97%E2%80%9D%EF%BC%9F"><span class="nav-text">1.2.2 自监督只是“更花哨的数据增强吗”？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E6%9C%AC%E6%96%87%E6%89%93%E7%AE%97%E6%80%8E%E4%B9%88%E8%AE%A8%E8%AE%BA%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-text">1.3 本文打算怎么讨论这个问题？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-%E4%B8%A4%E6%9D%A1%E4%B8%BB%E6%B5%81%E8%B7%AF%E7%BA%BF%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-vs-%E9%81%AE%E8%94%BD%E9%87%8D%E5%BB%BA"><span class="nav-text">1.3.1 两条主流路线：对比学习 vs 遮蔽重建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-%E4%B8%8D%E8%BF%BD%E9%97%AE%E2%80%9C%E8%B0%81%E6%9B%B4%E5%BC%BA%E2%80%9D%EF%BC%8C%E8%80%8C%E5%85%B3%E6%B3%A8%E2%80%9C%E8%B0%81%E9%80%82%E5%90%88%E4%BB%80%E4%B9%88%E2%80%9D"><span class="nav-text">1.3.2 不追问“谁更强”，而关注“谁适合什么”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%94%A8%E4%B8%80%E4%B8%AA%E7%BB%9F%E4%B8%80%E7%9A%84%E7%9C%BC%E5%85%89%E7%9C%8B%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E5%AD%A6%E4%B9%A0"><span class="nav-text">2. 用一个统一的眼光看自监督视觉学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%B9%B6%E4%B8%8D%E6%98%AF%E2%80%9C%E6%B2%A1%E6%9C%89%E4%BA%BA%E7%AE%A1%E2%80%9D"><span class="nav-text">2.1 自监督学习，其实并不是“没有人管”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%B8%8D%E6%98%AF%E8%87%AA%E7%94%B1%E5%8F%91%E6%8C%A5%EF%BC%8C%E8%80%8C%E6%98%AF%E5%9C%A8%E5%81%9A%E2%80%9C%E6%8C%87%E5%AE%9A%E7%BB%83%E4%B9%A0%E9%A2%98%E2%80%9D"><span class="nav-text">2.1.1 模型并不是自由发挥，而是在做“指定练习题”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-%E5%81%8F%E8%A7%81%E6%B2%A1%E6%9C%89%E6%B6%88%E5%A4%B1%EF%BC%8C%E5%8F%AA%E6%98%AF%E6%8D%A2%E4%BA%86%E4%B8%AA%E5%9C%B0%E6%96%B9"><span class="nav-text">2.1.2 偏见没有消失，只是换了个地方</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E4%B8%A4%E6%9D%A1%E4%B8%BB%E6%B5%81%E8%B7%AF%E7%BA%BF%EF%BC%8C%E6%9C%AC%E8%B4%A8%E4%B8%8A%E5%9C%A8%E2%80%9C%E6%95%99%E6%A8%A1%E5%9E%8B%E6%80%8E%E4%B9%88%E5%AD%A6%E2%80%9D"><span class="nav-text">2.2 两条主流路线，本质上在“教模型怎么学”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9D%A0%E2%80%9C%E5%88%86%E6%B8%85%E6%A5%9A%E2%80%9D%E6%9D%A5%E5%AD%A6%E8%A1%A8%E7%A4%BA"><span class="nav-text">2.2.1 对比学习：靠“分清楚”来学表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-%E9%81%AE%E8%94%BD%E9%87%8D%E5%BB%BA%EF%BC%9A%E9%9D%A0%E2%80%9C%E8%A1%A5%E5%85%A8%E7%BC%BA%E5%A4%B1%E2%80%9D%E6%9D%A5%E7%90%86%E8%A7%A3%E6%95%B4%E4%BD%93"><span class="nav-text">2.2.2 遮蔽重建：靠“补全缺失”来理解整体</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95%EF%BC%8C%E8%B5%B0%E7%9A%84%E6%98%AF%E5%90%8C%E4%B8%80%E6%9D%A1%E2%80%9C%E8%AE%AD%E7%BB%83%E2%80%94%E8%BF%81%E7%A7%BB%E2%80%9D%E8%B7%AF%E7%BA%BF"><span class="nav-text">2.3 不同方法，走的是同一条“训练—迁移”路线</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%85%89%E9%9D%A0%E2%80%9C%E5%88%86%E6%B8%85%E6%A5%9A%E2%80%9D%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9C%9F%E7%9A%84%E6%87%82%E8%AF%AD%E4%B9%89%E5%90%97%EF%BC%9F"><span class="nav-text">3. 对比学习：光靠“分清楚”，模型真的懂语义吗？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%BA%95%E5%9C%A8%E5%B9%B2%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-text">3.1 对比学习到底在干什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-%E6%AD%A3%E6%A0%B7%E6%9C%AC%E3%80%81%E8%B4%9F%E6%A0%B7%E6%9C%AC%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%98%AF%E5%9C%A8%E7%8E%A9%E2%80%9C%E8%AE%A4%E4%BA%BA%E6%B8%B8%E6%88%8F%E2%80%9D"><span class="nav-text">3.1.1 正样本、负样本，其实是在玩“认人游戏”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-%E4%B8%8D%E5%8F%98%E6%80%A7%EF%BC%9A%E6%97%A2%E6%98%AF%E4%BC%98%E7%82%B9%EF%BC%8C%E4%B9%9F%E6%98%AF%E4%BB%A3%E4%BB%B7"><span class="nav-text">3.1.2 不变性：既是优点，也是代价</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E7%94%A8%E4%BF%A1%E6%81%AF%E8%AE%BA%E8%A7%A3%E9%87%8A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%89%E7%94%A8%EF%BC%8C%E4%BD%86%E4%B8%8D%E4%B8%87%E8%83%BD"><span class="nav-text">3.2 用信息论解释对比学习：有用，但不万能</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E4%B8%80%E4%B8%AA%E5%B8%B8%E8%A7%81%E8%AF%B4%E6%B3%95%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%9C%A8%E2%80%9C%E6%9C%80%E5%A4%A7%E5%8C%96%E5%85%B1%E5%90%8C%E4%BF%A1%E6%81%AF%E2%80%9D"><span class="nav-text">3.2.1 一个常见说法：对比学习在“最大化共同信息”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B8%A9%E5%BA%A6%E3%80%81%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F%E8%BF%99%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="nav-text">3.2.2 为什么温度、负样本数量这么重要？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-%E4%BF%A1%E6%81%AF%E8%AE%BA%E8%A7%A3%E9%87%8A%E7%9A%84%E5%B1%80%E9%99%90%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="nav-text">3.2.3 信息论解释的局限在哪里？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E4%B8%89%E4%B8%AA%E7%BB%8F%E5%85%B8%E6%96%B9%E6%B3%95%EF%BC%8C%E4%B8%89%E7%A7%8D%E8%AE%BE%E8%AE%A1%E5%8F%96%E5%90%91"><span class="nav-text">3.3 三个经典方法，三种设计取向</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-SimCLR%EF%BC%9A%E8%B5%84%E6%BA%90%E5%A4%9F%E5%A4%9A%EF%BC%8C%E5%B0%B1%E6%8A%8A%E4%BA%8B%E6%83%85%E5%81%9A%E7%AE%80%E5%8D%95"><span class="nav-text">3.3.1 SimCLR：资源够多，就把事情做简单</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-MoCo%EF%BC%9A%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1%E8%83%BD%E4%B8%8D%E8%83%BD%E6%95%91%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="nav-text">3.3.2 MoCo：工程设计能不能救方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-DINO%EF%BC%9A%E6%B2%A1%E6%9C%89%E8%B4%9F%E6%A0%B7%E6%9C%AC%EF%BC%8C%E4%B9%9F%E8%83%BD%E5%AD%A6%E5%88%B0%E5%A5%BD%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="nav-text">3.3.3 DINO：没有负样本，也能学到好表示？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B1%80%E9%99%90%EF%BC%9A%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%BC%80%E5%A7%8B%E2%80%9C%E4%B8%8D%E5%A4%AA%E5%A5%BD%E7%94%A8%E2%80%9D%EF%BC%9F"><span class="nav-text">3.4 对比学习的局限：什么时候开始“不太好用”？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-%E5%9C%A8%E7%BB%86%E7%B2%92%E5%BA%A6%E4%BB%BB%E5%8A%A1%E4%B8%AD%EF%BC%8C%E9%97%AE%E9%A2%98%E4%BC%9A%E6%9A%B4%E9%9C%B2"><span class="nav-text">3.4.1 在细粒度任务中，问题会暴露</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-%E5%AF%B9%E5%A2%9E%E5%BC%BA%E7%AD%96%E7%95%A5%E7%9A%84%E4%BE%9D%E8%B5%96%EF%BC%8C%E8%AE%A9%E6%96%B9%E6%B3%95%E4%B8%8D%E5%A4%9F%E2%80%9C%E7%9C%81%E5%BF%83%E2%80%9D"><span class="nav-text">3.4.2 对增强策略的依赖，让方法不够“省心”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E9%81%AE%E8%94%BD%E9%87%8D%E5%BB%BA%E4%B8%8E-MAE%EF%BC%9A%E8%AE%A9%E6%A8%A1%E5%9E%8B%E2%80%9C%E8%A1%A5%E5%85%A8%E5%9B%BE%E7%89%87%E2%80%9D%EF%BC%8C%E8%83%BD%E5%AD%A6%E5%88%B0%E6%9B%B4%E7%A8%B3%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%90%97%EF%BC%9F"><span class="nav-text">4. 遮蔽重建与 MAE：让模型“补全图片”，能学到更稳的表示吗？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E9%81%AE%E8%94%BD%E5%BB%BA%E6%A8%A1%E2%80%9D%E5%9C%A8-NLP-%E5%BE%88%E6%88%90%E5%8A%9F%EF%BC%8C%E4%BD%86%E5%9C%A8%E8%A7%86%E8%A7%89%E9%87%8C%E5%BE%88%E6%99%9A%E6%89%8D%E7%81%AB%EF%BC%9F"><span class="nav-text">4.1 为什么“遮蔽建模”在 NLP 很成功，但在视觉里很晚才火？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-%E8%AF%AD%E8%A8%80%E6%98%AF%E2%80%9C%E7%A6%BB%E6%95%A3%E8%AF%8D%E5%9D%97%E2%80%9D%EF%BC%8C%E5%9B%BE%E5%83%8F%E6%98%AF%E2%80%9C%E8%BF%9E%E7%BB%AD%E5%83%8F%E7%B4%A0%E2%80%9D"><span class="nav-text">4.1.1 语言是“离散词块”，图像是“连续像素”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-%E5%85%B3%E9%94%AE%E8%BD%AC%E6%8A%98%EF%BC%9AVision-Transformer-%E8%AE%A9%E2%80%9C%E5%9B%BE%E5%83%8F%E5%83%8F%E5%8F%A5%E5%AD%90%E4%B8%80%E6%A0%B7%E2%80%9D%E5%8F%AF%E5%BB%BA%E6%A8%A1"><span class="nav-text">4.1.2 关键转折：Vision Transformer 让“图像像句子一样”可建模</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-MAE-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E6%88%90%E5%8A%9F%EF%BC%9F%E4%B8%89%E4%B8%AA%E5%85%B3%E9%94%AE%E8%AE%BE%E8%AE%A1"><span class="nav-text">4.2 MAE 为什么能成功？三个关键设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-%E2%80%9C%E9%81%AE%E4%BD%8F-75-%E2%80%9D%E5%8F%8D%E8%80%8C%E6%9B%B4%E5%A5%BD%EF%BC%9A%E9%80%BC%E6%A8%A1%E5%9E%8B%E5%88%AB%E8%B5%B0%E6%8D%B7%E5%BE%84"><span class="nav-text">4.2.1 “遮住 75%”反而更好：逼模型别走捷径</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-%E9%9D%9E%E5%AF%B9%E7%A7%B0%E7%BB%93%E6%9E%84%EF%BC%9A%E7%BC%96%E7%A0%81%E5%99%A8%E5%BE%88%E5%BC%BA%EF%BC%8C%E8%A7%A3%E7%A0%81%E5%99%A8%E5%BE%88%E8%BD%BB"><span class="nav-text">4.2.2 非对称结构：编码器很强，解码器很轻</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-3-%E4%B8%BA%E4%BB%80%E4%B9%88-CNN-%E5%BE%88%E9%9A%BE%E5%A4%8D%E5%88%B6-MAE-%E7%9A%84%E6%95%88%E6%9E%9C%EF%BC%9F"><span class="nav-text">4.2.3 为什么 CNN 很难复制 MAE 的效果？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-MAE-%E5%AD%A6%E5%88%B0%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F%E5%93%AA%E9%87%8C%E5%BC%BA%E3%80%81%E5%93%AA%E9%87%8C%E4%B8%8D%E5%8D%A0%E4%BC%98%EF%BC%9F"><span class="nav-text">4.3 MAE 学到的表示有什么特点？哪里强、哪里不占优？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-%E5%BC%BA%E9%A1%B9%EF%BC%9A%E6%9B%B4%E2%80%9C%E6%87%82%E6%95%B4%E4%BD%93%E2%80%9D%EF%BC%8C%E8%BF%81%E7%A7%BB%E6%9B%B4%E7%A8%B3"><span class="nav-text">4.3.1 强项：更“懂整体”，迁移更稳</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BC%B1%E7%82%B9%EF%BC%9A%E5%88%A4%E5%88%AB%E6%80%A7%E4%B8%8D%E5%A6%82%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E9%82%A3%E4%B9%88%E2%80%9C%E5%A4%A9%E7%94%9F%E5%BC%BA%E2%80%9D"><span class="nav-text">4.3.2 可能的弱点：判别性不如对比学习那么“天生强”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E4%B8%8E-MAE%EF%BC%9A%E4%B8%8D%E6%98%AF%E8%B0%81%E5%8F%96%E4%BB%A3%E8%B0%81%EF%BC%8C%E8%80%8C%E6%98%AF%E5%90%84%E5%B9%B2%E5%90%84%E7%9A%84%E6%B4%BB"><span class="nav-text">5. 对比学习与 MAE：不是谁取代谁，而是各干各的活</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E5%AE%83%E4%BB%AC%E5%AD%A6%E5%88%B0%E7%9A%84%E2%80%9C%E8%A1%A8%E7%A4%BA%E2%80%9D%EF%BC%8C%E6%9C%AC%E8%B4%A8%E4%B8%8A%E5%93%AA%E9%87%8C%E4%B8%8D%E4%B8%80%E6%A0%B7%EF%BC%9F"><span class="nav-text">5.1 它们学到的“表示”，本质上哪里不一样？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-%E4%B8%80%E4%B8%AA%E6%93%85%E9%95%BF%E2%80%9C%E5%88%86%E6%B8%85%E6%A5%9A%E2%80%9D%EF%BC%8C%E4%B8%80%E4%B8%AA%E6%93%85%E9%95%BF%E2%80%9C%E7%9C%8B%E6%95%B4%E4%BD%93%E2%80%9D"><span class="nav-text">5.1.1 一个擅长“分清楚”，一个擅长“看整体”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-%E4%B8%80%E4%B8%AA%E5%9C%A8%E2%80%9C%E4%B8%BB%E5%8A%A8%E4%B8%A2%E4%BF%A1%E6%81%AF%E2%80%9D%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%9C%A8%E2%80%9C%E5%B0%BD%E9%87%8F%E4%BF%9D%E4%BF%A1%E6%81%AF%E2%80%9D"><span class="nav-text">5.1.2 一个在“主动丢信息”，一个在“尽量保信息”</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E5%AE%9E%E9%99%85%E7%94%A8%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%AF%A5%E9%80%89%E5%93%AA%E4%B8%80%E4%B8%AA%EF%BC%9F"><span class="nav-text">5.2 实际用的时候，该选哪一个？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-%E7%9C%8B%E4%B8%89%E4%BB%B6%E4%BA%8B%EF%BC%9A%E6%95%B0%E6%8D%AE%E3%80%81%E4%BB%BB%E5%8A%A1%E3%80%81%E6%A8%A1%E5%9E%8B"><span class="nav-text">5.2.1 看三件事：数据、任务、模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-Few-shot-%E5%92%8C%E5%AF%86%E9%9B%86%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">5.2.2 Few-shot 和密集预测任务的区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%9E%8D%E5%90%88%E8%B7%AF%E7%BA%BF%EF%BC%9A%E5%BD%93%E2%80%9C%E5%88%86%E6%B8%85%E6%A5%9A%E2%80%9D%E5%92%8C%E2%80%9C%E7%9C%8B%E6%95%B4%E4%BD%93%E2%80%9D%E5%BC%80%E5%A7%8B%E4%B8%80%E8%B5%B7%E5%AD%A6"><span class="nav-text">6. 融合路线：当“分清楚”和“看整体”开始一起学</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-BEiT%EF%BC%9A%E6%8A%8A%E2%80%9C%E8%A1%A5%E5%83%8F%E7%B4%A0%E2%80%9D%E5%8D%87%E7%BA%A7%E6%88%90%E2%80%9C%E7%8C%9C%E8%AF%AD%E4%B9%89%E6%A0%87%E7%AD%BE%E2%80%9D"><span class="nav-text">6.1 BEiT：把“补像素”升级成“猜语义标签”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-%E4%B8%80%E4%B8%AA%E5%85%B3%E9%94%AE%E6%83%B3%E6%B3%95%EF%BC%9A%E9%97%AE%E9%A2%98%E5%8F%AF%E8%83%BD%E5%87%BA%E5%9C%A8%E2%80%9C%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87%E5%A4%AA%E4%BD%8E%E7%BA%A7%E2%80%9D"><span class="nav-text">6.1.1 一个关键想法：问题可能出在“预测目标太低级”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A6%BB%E6%95%A3-token-%E6%9C%AC%E8%BA%AB%E5%B0%B1%E5%83%8F%E4%B8%80%E7%A7%8D%E2%80%9C%E5%BC%B1%E7%9B%91%E7%9D%A3%E2%80%9D%EF%BC%9F"><span class="nav-text">6.1.2 为什么离散 token 本身就像一种“弱监督”？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-3-BEiT-%E7%9A%84%E6%94%B6%E8%8E%B7%E4%B8%8E%E4%BB%A3%E4%BB%B7"><span class="nav-text">6.1.3 BEiT 的收获与代价</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-iBOT%EF%BC%9A%E4%B8%8D%E7%94%A8%E7%A6%BB%E6%95%A3%E6%A0%87%E7%AD%BE%EF%BC%8C%E4%B9%9F%E8%83%BD%E5%AD%A6%E5%88%B0%E7%BB%93%E6%9E%84"><span class="nav-text">6.2 iBOT：不用离散标签，也能学到结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-%E6%8A%8A%E2%80%9C%E5%AF%B9%E6%AF%94%E2%80%9D%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%8C%E5%A1%9E%E8%BF%9B%E2%80%9C%E9%81%AE%E8%94%BD%E9%87%8D%E5%BB%BA%E2%80%9D%E9%87%8C"><span class="nav-text">6.2.1 把“对比”的思想，塞进“遮蔽重建”里</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-%E4%B8%BA%E4%BB%80%E4%B9%88-iBOT-%E5%BC%BA%E8%B0%83-patch-%E7%BA%A7%E5%88%AB%EF%BC%9F"><span class="nav-text">6.2.2 为什么 iBOT 强调 patch 级别？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E2%80%9C%E7%9C%9F%E7%9A%84%E8%83%BD%E8%BF%81%E7%A7%BB%E2%80%9D%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B"><span class="nav-text">7. 训练一个“真的能迁移”的自监督视觉模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E4%B8%80%E4%B8%AA%E5%BE%88%E7%8E%B0%E5%AE%9E%E7%9A%84%E8%B5%B7%E7%82%B9%EF%BC%9A%E8%B5%84%E6%BA%90%E6%9C%89%E9%99%90%E3%80%81%E7%9B%AE%E6%A0%87%E5%8A%A1%E5%AE%9E"><span class="nav-text">7.1 一个很现实的起点：资源有限、目标务实</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E9%A2%84%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%80%E5%90%8E%E9%80%89%E4%BA%86-MAE%EF%BC%9F"><span class="nav-text">7.2 预训练阶段：为什么最后选了 MAE？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9%E4%B8%8D%E6%98%AF%E7%9C%8B%E5%88%86%E6%95%B0%EF%BC%8C%E8%80%8C%E6%98%AF%E7%9C%8B%E2%80%9C%E7%9C%81%E4%B8%8D%E7%9C%81%E5%BF%83%E2%80%9D"><span class="nav-text">7.2.1 方法选择不是看分数，而是看“省不省心”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-%E8%AE%AD%E7%BB%83%E6%97%B6%E9%97%B4%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E6%9B%B4%E9%87%8D%E8%A6%81"><span class="nav-text">7.2.2 训练时间比你想象的更重要</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E6%80%8E%E4%B9%88%E8%AF%84%E4%BC%B0%E2%80%9C%E5%AD%A6%E5%BE%97%E5%A5%BD%E4%B8%8D%E5%A5%BD%E2%80%9D%EF%BC%9F%E5%88%AB%E8%A2%AB%E4%B8%80%E4%B8%AA%E6%8C%87%E6%A0%87%E9%AA%97%E4%BA%86"><span class="nav-text">7.3 怎么评估“学得好不好”？别被一个指标骗了</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-%E7%BA%BF%E6%80%A7%E6%8E%A2%E9%92%88%EF%BC%9A%E5%8F%82%E8%80%83%E5%8F%AF%E4%BB%A5%EF%BC%8C%E4%BD%86%E5%88%AB%E8%BF%B7%E4%BF%A1"><span class="nav-text">7.3.1 线性探针：参考可以，但别迷信</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-%E7%9C%9F%E6%AD%A3%E6%8B%89%E5%BC%80%E5%B7%AE%E8%B7%9D%E7%9A%84%EF%BC%8C%E6%98%AF%E4%B8%8B%E6%B8%B8%E5%BE%AE%E8%B0%83"><span class="nav-text">7.3.2 真正拉开差距的，是下游微调</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E4%B8%80%E4%BA%9B%E7%9C%8B%E4%BC%BC%E4%B8%8D%E8%B5%B7%E7%9C%BC%EF%BC%8C%E4%BD%86%E5%BE%88%E5%85%B3%E9%94%AE%E7%9A%84%E5%B7%A5%E7%A8%8B%E7%BB%86%E8%8A%82"><span class="nav-text">7.4 一些看似不起眼，但很关键的工程细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-1-batch-%E4%B8%8D%E5%A4%9F%E5%A4%A7%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F"><span class="nav-text">7.4.1 batch 不够大怎么办？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-2-%E5%92%8C%E8%9E%8D%E5%90%88%E6%96%B9%E6%A1%88%E7%9A%84%E5%AF%B9%E7%85%A7%EF%BC%9A%E4%B8%8D%E6%98%AF%E8%B6%8A%E5%A4%8D%E6%9D%82%E8%B6%8A%E5%A5%BD"><span class="nav-text">7.4.2 和融合方案的对照：不是越复杂越好</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-%E5%88%AB%E5%8F%AA%E7%9C%8B%E6%9C%80%E7%BB%88%E7%B2%BE%E5%BA%A6%EF%BC%8C%E8%A6%81%E7%9C%8B%E2%80%9C%E6%A0%B7%E6%9C%AC%E6%95%88%E7%8E%87%E6%9B%B2%E7%BA%BF%E2%80%9D"><span class="nav-text">7.5 别只看最终精度，要看“样本效率曲线”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%9C%A8%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2%E5%8F%98%E5%8C%96%EF%BC%9A%E4%BB%8E%E2%80%9C%E5%A4%87%E9%80%89%E6%96%B9%E6%A1%88%E2%80%9D%E5%88%B0%E2%80%9C%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E2%80%9D"><span class="nav-text">8. 自监督在视觉大模型中的角色变化：从“备选方案”到“默认配置”</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E7%9B%91%E7%9D%A3%E6%88%90%E4%BA%86%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E2%80%9C%E5%BA%95%E5%BA%A7%E2%80%9D%EF%BC%9F"><span class="nav-text">8.1 为什么自监督成了视觉大模型的“底座”？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-1-%E6%A8%A1%E5%9E%8B%E8%B6%8A%E5%A4%A7%EF%BC%8C%E6%A0%87%E7%AD%BE%E5%8F%8D%E8%80%8C%E8%B6%8A%E4%B8%8D%E5%A4%9F%E7%94%A8"><span class="nav-text">8.1.1 模型越大，标签反而越不够用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-2-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%92%8C%E7%9B%91%E7%9D%A3%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%98%AF%E5%88%86%E5%B7%A5%E5%90%88%E4%BD%9C"><span class="nav-text">8.1.2 自监督和监督，其实是分工合作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%A6%82%E4%BD%95%E6%94%AF%E6%92%91%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-text">8.2 自监督如何支撑多模态模型？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-%E5%85%88%E2%80%9C%E7%9C%8B%E6%87%82%E4%B8%96%E7%95%8C%E2%80%9D%EF%BC%8C%E5%86%8D%E2%80%9C%E5%92%8C%E8%AF%AD%E8%A8%80%E5%AF%B9%E9%BD%90%E2%80%9D"><span class="nav-text">8.2.1 先“看懂世界”，再“和语言对齐”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-%E5%A5%BD%E7%9A%84%E8%A7%86%E8%A7%89%E8%A1%A8%E7%A4%BA%EF%BC%8C%E8%AE%A9%E5%AF%B9%E9%BD%90%E5%8F%98%E5%BE%97%E6%9B%B4%E5%AE%B9%E6%98%93"><span class="nav-text">8.2.2 好的视觉表示，让对齐变得更容易</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E8%87%AA%E7%9B%91%E7%9D%A3%E6%88%90%E5%8A%9F%E4%BA%86%EF%BC%8C%E4%BD%86%E9%97%AE%E9%A2%98%E5%B9%B6%E6%B2%A1%E6%9C%89%E7%BB%93%E6%9D%9F"><span class="nav-text">8.3 自监督成功了，但问题并没有结束</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-1-%E8%A1%A8%E7%A4%BA%E5%A1%8C%E7%BC%A9%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E8%A2%AB%E9%81%BF%E5%85%8D%EF%BC%9F"><span class="nav-text">8.3.1 表示塌缩，为什么还能被避免？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-2-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%9A%E6%98%AF%E5%9C%A8%E5%B8%AE%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%98%E6%98%AF%E5%9C%A8%E2%80%9C%E5%81%B7%E5%81%B7%E6%95%99%E6%A8%A1%E5%9E%8B%E2%80%9D%EF%BC%9F"><span class="nav-text">8.3.2 数据增强：是在帮模型，还是在“偷偷教模型”？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-%E6%80%BB%E7%BB%93%EF%BC%9A%E8%87%AA%E7%9B%91%E7%9D%A3%E7%9C%9F%E6%AD%A3%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F%E5%AE%83%E8%BF%98%E6%B2%A1%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-text">9. 总结：自监督真正改变了什么？它还没解决什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-%E8%87%AA%E7%9B%91%E7%9D%A3%E7%9C%9F%E6%AD%A3%E6%94%B9%E5%8F%98%E7%9A%84%EF%BC%8C%E4%B8%8D%E6%98%AF%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%80%8C%E6%98%AF%E2%80%9C%E6%80%8E%E4%B9%88%E5%AD%A6%E2%80%9D"><span class="nav-text">9.1 自监督真正改变的，不是模型，而是“怎么学”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-1-%E4%BB%8E%E2%80%9C%E7%9C%8B%E6%A0%87%E7%AD%BE%E2%80%9D%E5%88%B0%E2%80%9C%E7%9C%8B%E6%95%B0%E6%8D%AE%E6%9C%AC%E8%BA%AB%E2%80%9D"><span class="nav-text">9.1.1 从“看标签”到“看数据本身”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-2-%E4%BB%8E%E2%80%9C%E6%8A%8A%E4%B8%80%E4%B8%AA%E4%BB%BB%E5%8A%A1%E5%81%9A%E5%A5%BD%E2%80%9D%EF%BC%8C%E5%88%B0%E2%80%9C%E5%AD%A6%E4%B8%80%E4%B8%AA%E8%83%BD%E5%8F%8D%E5%A4%8D%E7%94%A8%E7%9A%84%E8%A1%A8%E7%A4%BA%E2%80%9D"><span class="nav-text">9.1.2 从“把一个任务做好”，到“学一个能反复用的表示”</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-%E8%87%AA%E7%9B%91%E7%9D%A3%E4%B9%9F%E4%B8%8D%E6%98%AF%E4%B8%87%E8%83%BD%E7%9A%84"><span class="nav-text">9.2 自监督也不是万能的</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-1-%E5%AE%83%E4%B8%8D%E6%98%AF%E2%80%9C%E8%A3%85%E4%B8%8A%E5%B0%B1%E8%83%BD%E8%B5%A2%E2%80%9D%E7%9A%84%E9%93%B6%E5%BC%B9"><span class="nav-text">9.2.1 它不是“装上就能赢”的银弹</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-2-%E5%BE%88%E5%A4%9A%E6%88%90%E5%8A%9F%EF%BC%8C%E9%9D%A0%E7%9A%84%E6%98%AF%E7%BB%8F%E9%AA%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E7%90%86%E8%AE%BA"><span class="nav-text">9.2.2 很多成功，靠的是经验而不是理论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-3-%E4%BB%8E%E2%80%9C%E5%AD%A6%E8%A1%A8%E7%A4%BA%E2%80%9D%E8%B5%B0%E5%90%91%E2%80%9C%E5%AD%A6%E4%B8%96%E7%95%8C%E2%80%9D"><span class="nav-text">9.2.3 从“学表示”走向“学世界”</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">145</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="视觉自监督学习：从对比学习到 MAE，再到通用视觉表征 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          视觉自监督学习：从对比学习到 MAE，再到通用视觉表征
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2026-02-03 21:19:12 / 修改时间：21:19:48" itemprop="dateCreated datePublished" datetime="2026-02-03T21:19:12+08:00">2026-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2026/02/03/055-visual-self-supervised-learning/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2026/02/03/055-visual-self-supervised-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>51 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-引言：自监督学习，真的能“看懂”图像吗？"><a href="#1-引言：自监督学习，真的能“看懂”图像吗？" class="headerlink" title="1. 引言：自监督学习，真的能“看懂”图像吗？"></a>1. 引言：自监督学习，真的能“看懂”图像吗？</h2><p>在很长一段时间里，计算机视觉的进步，几乎完全依赖一种方式：<strong>监督学习</strong>。简单来说，就是给模型大量图片，再告诉它“这是什么”，让它不断做题、纠错、记答案。</p>
<p>ImageNet 分类、COCO 检测与分割，这些经典成果，都是在这种模式下取得的。只要数据够多、标签够准，模型的分数就能不断刷新。但随着模型越来越大、应用场景越来越真实，这条路开始遇到问题了：</p>
<ul>
<li>标注图片成本很高</li>
<li>真实世界并不总是“有标准答案”</li>
<li>模型在新场景下，经常表现失常</li>
</ul>
<span id="more"></span>
<p>于是，一个更根本的问题开始被反复追问：<strong>模型是真的“理解”了图像，还是只是学会了在考试里拿高分？</strong> 自监督学习，正是在这种质疑中走到舞台中央的。</p>
<h3 id="1-1-监督学习很成功，但真的没问题吗？"><a href="#1-1-监督学习很成功，但真的没问题吗？" class="headerlink" title="1.1 监督学习很成功，但真的没问题吗？"></a>1.1 监督学习很成功，但真的没问题吗？</h3><h4 id="1-1-1-图片多、标签多，不等于模型懂了"><a href="#1-1-1-图片多、标签多，不等于模型懂了" class="headerlink" title="1.1.1 图片多、标签多，不等于模型懂了"></a>1.1.1 图片多、标签多，不等于模型懂了</h4><p>监督学习背后的直觉其实很简单：<strong>只要我告诉模型足够多的“正确答案”，它自然就能学会看世界。</strong> 在一些干净、封闭的数据集上，这确实成立。但在真实世界里，这个假设开始变得不那么可靠。</p>
<p>首先，<strong>标签本身是昂贵的</strong>。在医学影像、工业检测、遥感等场景中，一张图的标注可能需要专家花很长时间，而且不同人给出的标签还可能不一致。</p>
<p>其次，<strong>标签本身很“粗”</strong>。一张复杂的图片，可能只被简化成一个类别名，而大量有价值的信息——结构、关系、细节差异——在打标签的那一刻就被忽略了。</p>
<p>更重要的是：<strong>模型在“答对题”这件事上表现很好，并不代表它真的形成了稳固的视觉理解。</strong></p>
<p>只要换一个数据分布，或者换一个任务，模型往往就需要重新大量标注、重新训练。这说明它学到的东西，很可能是“对某套标签有用的技巧”，而不是通用的视觉能力。</p>
<h4 id="1-1-2-ImageNet-的成功，为何难以复制？"><a href="#1-1-2-ImageNet-的成功，为何难以复制？" class="headerlink" title="1.1.2 ImageNet 的成功，为何难以复制？"></a>1.1.2 ImageNet 的成功，为何难以复制？</h4><p>ImageNet 常被视为监督学习的巅峰案例，但它的成功，其实有很强的“历史和条件加成”。</p>
<ul>
<li>类别定义清晰</li>
<li>图片质量较高</li>
<li>标注规则相对统一</li>
</ul>
<p>在这种理想环境下，模型只要把“类别区分”这件事做好，性能就能稳步提升。但在现实应用中：</p>
<ul>
<li>类别边界往往是模糊的</li>
<li>语义层级是连续的，而不是离散的</li>
<li>有些场景甚至根本不存在明确的标签体系</li>
</ul>
<p>这时，单纯依赖固定标签训练的模型，往往会学到一些<strong>只在特定数据集上成立的“捷径”</strong>，而不是能迁移到新任务的视觉规律。这迫使研究者重新思考一个问题：<strong>模型真正该学的，是“完成某个任务”，还是“构建对世界更通用的表示”？</strong></p>
<h3 id="1-2-自监督学习想解决什么问题？"><a href="#1-2-自监督学习想解决什么问题？" class="headerlink" title="1.2 自监督学习想解决什么问题？"></a>1.2 自监督学习想解决什么问题？</h3><h4 id="1-2-1-不用人工标签，模型怎么学？"><a href="#1-2-1-不用人工标签，模型怎么学？" class="headerlink" title="1.2.1 不用人工标签，模型怎么学？"></a>1.2.1 不用人工标签，模型怎么学？</h4><p>自监督学习的核心想法其实并不神秘：<strong>既然图片本身就包含大量结构信息，能不能直接从这些信息中学习？</strong> 比如：</p>
<ul>
<li>图像中相邻区域通常是相关的</li>
<li>同一张图在不同裁剪、旋转下，本质内容不变</li>
<li>被遮住的部分，可以从上下文推断出来</li>
</ul>
<p>自监督方法正是通过这些“图像自带的规律”，人为构造训练任务，让模型在<strong>没有人工标签的情况下</strong>学习有用的表示。从这个角度看，自监督并不是“没有监督”，而是把监督来源，从“人给的标签”，变成了“数据自身的结构”。</p>
<p>这也很像人类的学习方式：<strong>我们并不是先被告知“这是猫、这是狗”，而是通过长期观察，逐渐形成对形状、结构和关系的理解。</strong></p>
<h4 id="1-2-2-自监督只是“更花哨的数据增强吗”？"><a href="#1-2-2-自监督只是“更花哨的数据增强吗”？" class="headerlink" title="1.2.2 自监督只是“更花哨的数据增强吗”？"></a>1.2.2 自监督只是“更花哨的数据增强吗”？</h4><p>当然，自监督并不是没有争议。一种常见的质疑是：自监督看起来厉害，是不是因为</p>
<ul>
<li>用了更强的数据增强</li>
<li>训练得更久</li>
<li>模型更大</li>
</ul>
<p>本质上还是监督学习那一套？这个问题并不是毫无根源。很多自监督方法对数据增强非常敏感，增强策略一变，学到的表示也会明显不同。</p>
<p>因此，如果只看下游任务的分数，很难判断提升到底来自：</p>
<ul>
<li>学习机制本身</li>
<li>还是工程细节的叠加效果</li>
</ul>
<p>这也是为什么，理解自监督学习，不能只盯着排行榜，而要看它<strong>学到的表示到底有什么不同</strong>。</p>
<h3 id="1-3-本文打算怎么讨论这个问题？"><a href="#1-3-本文打算怎么讨论这个问题？" class="headerlink" title="1.3 本文打算怎么讨论这个问题？"></a>1.3 本文打算怎么讨论这个问题？</h3><h4 id="1-3-1-两条主流路线：对比学习-vs-遮蔽重建"><a href="#1-3-1-两条主流路线：对比学习-vs-遮蔽重建" class="headerlink" title="1.3.1 两条主流路线：对比学习 vs 遮蔽重建"></a>1.3.1 两条主流路线：对比学习 vs 遮蔽重建</h4><p>当前视觉自监督方法，大致可以分为两类：</p>
<ul>
<li><strong>对比学习</strong><br>  通过“同一张图的不同视角应该相似、不同图片应该区分开”这一原则，学习一个区分性很强的特征空间。</li>
<li><strong>遮蔽重建方法</strong><br>  通过遮住图像的一部分，让模型根据上下文去“补全”，从而逼迫模型理解整体结构和空间关系。</li>
</ul>
<p>它们的差异，并不只是性能高低，而是<strong>对“什么是好表示”的理解不同</strong>：</p>
<ul>
<li>对比学习更强调“可分、好用、判别性强”</li>
<li>遮蔽重建更强调“整体结构、上下文一致性”</li>
</ul>
<h4 id="1-3-2-不追问“谁更强”，而关注“谁适合什么”"><a href="#1-3-2-不追问“谁更强”，而关注“谁适合什么”" class="headerlink" title="1.3.2 不追问“谁更强”，而关注“谁适合什么”"></a>1.3.2 不追问“谁更强”，而关注“谁适合什么”</h4><p>因此，本文并不试图给出一个简单答案，比如：哪种自监督方法最好？相反，我们更关心：</p>
<ul>
<li>不同方法各自学到了什么样的表示</li>
<li>在什么任务和条件下更有优势</li>
<li>它们的局限在哪里</li>
</ul>
<p>接下来的章节，将围绕这些问题展开，帮助建立一个<strong>可解释、可迁移的理解框架</strong>，而不仅是记住几种算法名字。</p>
<p><strong>本章小结</strong><br>监督学习在视觉领域取得了巨大成功，但它对人工标签的高度依赖，也限制了模型在真实、开放环境中的泛化能力。</p>
<p>自监督学习试图绕开这一限制，通过挖掘数据自身的结构，让模型在无标签条件下学习通用的视觉表示。但它究竟带来了怎样的本质变化，仍需要从表示结构和适用边界上加以分析。</p>
<p>本章明确了本文的核心立场：<strong>关注方法能学到什么，而不是只看分数高低</strong>。这为后续对对比学习与遮蔽重建方法的深入讨论，奠定了理解基础。</p>
<h2 id="2-用一个统一的眼光看自监督视觉学习"><a href="#2-用一个统一的眼光看自监督视觉学习" class="headerlink" title="2. 用一个统一的眼光看自监督视觉学习"></a>2. 用一个统一的眼光看自监督视觉学习</h2><p>在前一章里，我们已经讨论了：<strong>为什么单靠监督学习不够，以及自监督学习想解决什么问题。</strong></p>
<p>这一章要回答的是另一个关键问题：<strong>市面上这么多自监督方法，看起来差别很大，它们到底有没有一个共同的逻辑？</strong> 答案是：<strong>有的。</strong> 而且一旦站到这个“统一视角”上，很多方法之间的差异，会变得非常自然、也更容易理解。</p>
<h3 id="2-1-自监督学习，其实并不是“没有人管”"><a href="#2-1-自监督学习，其实并不是“没有人管”" class="headerlink" title="2.1 自监督学习，其实并不是“没有人管”"></a>2.1 自监督学习，其实并不是“没有人管”</h3><h4 id="2-1-1-模型并不是自由发挥，而是在做“指定练习题”"><a href="#2-1-1-模型并不是自由发挥，而是在做“指定练习题”" class="headerlink" title="2.1.1 模型并不是自由发挥，而是在做“指定练习题”"></a>2.1.1 模型并不是自由发挥，而是在做“指定练习题”</h4><p>很多初学者第一次听到“自监督”，会产生一个误解：没有标签，是不是模型就自己随便学？实际上完全不是这样。</p>
<p>自监督学习几乎都会给模型设计一个<strong>明确的训练任务</strong>，只不过这个任务的“答案”不是人标出来的，而是<strong>从数据本身算出来的</strong>。这些任务通常被称为 <strong>预文本任务（pretext task）</strong>，你可以把它理解成：<strong>为了学会看图像，先让模型做的一套“基础练习题”。</strong> 举几个直观的例子：</p>
<ul>
<li>在<strong>对比学习</strong>中：<br>  模型要判断——两张经过不同处理的图片，是不是来自同一张原图。</li>
<li>在<strong>遮蔽重建方法</strong>中：<br>  模型会看到一张被遮住一部分的图片，需要根据周围内容，把缺失的地方“猜回来”。</li>
</ul>
<p>虽然形式不同，但它们有一个共同点：<strong>模型被明确要求完成一件事，而且做得好不好，是可以计算分数的。</strong> 所以，自监督并不是“没有目标”，而是：<strong>不用人工标签，但依然有清晰的训练目标。</strong></p>
<p>可以把它理解成：<strong>显式给任务，隐式学语义。</strong></p>
<h4 id="2-1-2-偏见没有消失，只是换了个地方"><a href="#2-1-2-偏见没有消失，只是换了个地方" class="headerlink" title="2.1.2 偏见没有消失，只是换了个地方"></a>2.1.2 偏见没有消失，只是换了个地方</h4><p>在监督学习中，模型的“学习方向”很大程度上是由<strong>标签决定的</strong>：</p>
<ul>
<li>类别怎么分</li>
<li>一张图只能属于一个类，还是多个类</li>
<li>评价指标是什么</li>
</ul>
<p>这些都会直接影响模型学到什么。而在自监督学习中，标签消失了，但 <strong>“人为假设”并没有消失</strong>，只是换了个位置。这些假设主要体现在两件事上：</p>
<ol>
<li><strong>预文本任务怎么设计</strong></li>
<li><strong>数据增强怎么做</strong></li>
</ol>
<p>举个例子，如果我们告诉模型：“同一张图片的不同裁剪，应该被当成同一个东西”，那其实是在假设：位置、尺度变化不重要；如果让模型去补全被遮住的区域，那是在假设：图像内部是有规律、有上下文结构的。这些选择看起来像“工程细节”，但实际上：<strong>它们决定了模型最终会关注什么、忽略什么。</strong></p>
<p>这也是为什么自监督方法看起来很自由，但结果差异却可能很大——因为我们给模型设定的“练习题”，本身就在悄悄引导它形成不同的视觉理解方式。</p>
<h3 id="2-2-两条主流路线，本质上在“教模型怎么学”"><a href="#2-2-两条主流路线，本质上在“教模型怎么学”" class="headerlink" title="2.2 两条主流路线，本质上在“教模型怎么学”"></a>2.2 两条主流路线，本质上在“教模型怎么学”</h3><p>目前视觉自监督方法，大致可以分成两大类。理解它们最好的方式，不是记公式，而是问一句话：<strong>是通过“区分不同”，还是通过“补全整体”，来教模型看图？</strong></p>
<h4 id="2-2-1-对比学习：靠“分清楚”来学表示"><a href="#2-2-1-对比学习：靠“分清楚”来学表示" class="headerlink" title="2.2.1 对比学习：靠“分清楚”来学表示"></a>2.2.1 对比学习：靠“分清楚”来学表示</h4><p>对比学习的核心思想非常直观：<strong>同一个东西，在不同视角下应该看起来一样；不同的东西，应该尽量区分开。</strong> 于是，模型的主要任务就变成了：</p>
<ul>
<li>把“同一张图的不同版本”拉近</li>
<li>把“不同图片”推远</li>
</ul>
<p>长期训练下来，模型会学到一个<strong>区分性很强的特征空间</strong>。这种表示有一个明显优点：</p>
<ul>
<li>在分类任务中非常好用</li>
<li>用简单的线性分类器就能取得不错效果</li>
</ul>
<p>但它也有一个潜在代价：<strong>为了区分得足够清楚，模型可能会主动忽略一些不利于区分的细节</strong>，比如全局结构或生成性信息。</p>
<h4 id="2-2-2-遮蔽重建：靠“补全缺失”来理解整体"><a href="#2-2-2-遮蔽重建：靠“补全缺失”来理解整体" class="headerlink" title="2.2.2 遮蔽重建：靠“补全缺失”来理解整体"></a>2.2.2 遮蔽重建：靠“补全缺失”来理解整体</h4><p>遮蔽重建方法走的是另一条路。它不要求模型区分“你是谁”，而是不断问模型：<strong>在信息不完整的情况下，你能不能理解整体结构？</strong></p>
<p>通过遮住图片的一部分，让模型根据上下文去预测或重建缺失内容，模型被迫学习：</p>
<ul>
<li>哪些区域彼此相关</li>
<li>图像整体的空间和语义结构</li>
<li>局部与全局之间的关系</li>
</ul>
<p>这类方法在大模型和大数据下，往往能学到<strong>更稳定、更通用的表示</strong>，在任务迁移时表现不错。不过，相比对比学习，它在一些强调“快速区分”的任务上，可能并不是最直接高效的选择。</p>
<h3 id="2-3-不同方法，走的是同一条“训练—迁移”路线"><a href="#2-3-不同方法，走的是同一条“训练—迁移”路线" class="headerlink" title="2.3 不同方法，走的是同一条“训练—迁移”路线"></a>2.3 不同方法，走的是同一条“训练—迁移”路线</h3><p>尽管对比学习和遮蔽重建在“怎么学”上差异很大，但它们在<strong>整体流程</strong>上其实非常一致：</p>
<ol>
<li>先在<strong>大量无标签图片</strong>上进行自监督预训练</li>
<li>用简单任务（比如线性分类）快速检查表示质量</li>
<li>再迁移到具体下游任务（分类、检测、分割等）</li>
</ol>
<p>这说明了一点很重要的事实：<strong>自监督方法的价值，不在于预文本任务本身，而在于最终学到的表示能不能被反复使用。</strong> 不同方法的优劣，最终都要在“迁移表现”中接受检验。</p>
<pre class="mermaid">flowchart LR
    A[大规模无标签图像数据] --> B[共享阶段 自监督预训练]

    %% Contrastive Learning Branch
    B --> C1[对比学习路线]
    C1 --> D1[预文本任务 实例区分与视角一致]
    D1 --> E1[损失形式 对比目标]
    E1 --> F1[表征特性<br/>判别性强<br/>几何结构清晰<br/>线性可分性好]

    %% Masked Reconstruction Branch
    B --> C2[遮蔽重建路线]
    C2 --> D2[预文本任务 遮蔽预测与内容重建]
    D2 --> E2[损失形式 重建或离散预测目标]
    E2 --> F2[表征特性<br/>结构感强<br/>全局语义丰富<br/>迁移稳定性好]

    %% Shared Transfer Stage
    F1 --> G[共享阶段 表征迁移与评估]
    F2 --> G

    G --> H1[线性探针评估]
    G --> H2[下游任务微调<br/>分类 检测 分割]</pre>
<center>图2-1 自监督视觉学习的统一框架示意图</center>

<p>用一张图来总结这一章，可以这样理解，所有方法都从<strong>同一堆无标签数据</strong>出发；中间分成两条主要学习路线；最后又在下游任务中“会合”。区别不在流程，而在于是通过“区分”学会看世界，还是通过“补全”理解世界</p>
<p><strong>本章小结</strong><br>本章从一个整体视角梳理了自监督视觉学习的基本逻辑。自监督并不是“没有监督”，而是通过精心设计的任务，让模型从数据自身的结构中获得学习信号。模型的学习偏好，并不会消失，而是从“标签怎么定义”，转移到了“任务和数据怎么设计”。</p>
<p>通过对比学习与遮蔽重建两条主流路线的对照，我们看到：前者强调区分性和可分性，后者强调整体结构和上下文理解。尽管方法不同，它们最终都要通过迁移能力来证明自身价值。</p>
<p>这一统一视角，有助于我们在后续章节中，不被具体算法细节淹没，而始终抓住“模型到底学到了什么”这一核心问题。</p>
<h2 id="3-对比学习：光靠“分清楚”，模型真的懂语义吗？"><a href="#3-对比学习：光靠“分清楚”，模型真的懂语义吗？" class="headerlink" title="3. 对比学习：光靠“分清楚”，模型真的懂语义吗？"></a>3. 对比学习：光靠“分清楚”，模型真的懂语义吗？</h2><p>在所有视觉自监督方法中，对比学习几乎是<strong>最先被证明“真的有用”</strong> 的一条路线。它的想法非常直接，甚至有点“粗暴”：<br><strong>只要我能把不同图片区分得足够清楚，那我学到的表示，应该就很有用了吧？</strong></p>
<p>事实证明，在很多任务上，这个直觉是对的。对比学习在自监督早期，展现出了非常好的迁移能力，也很容易在工程上扩展。但与此同时，一个关键问题始终没有彻底解决：<strong>模型到底是在“理解图像的语义”，还是只是学会了一套高效的“区分技巧”？</strong></p>
<p>理解这个问题，是判断对比学习<strong>适用边界</strong>的关键。</p>
<h3 id="3-1-对比学习到底在干什么？"><a href="#3-1-对比学习到底在干什么？" class="headerlink" title="3.1 对比学习到底在干什么？"></a>3.1 对比学习到底在干什么？</h3><h4 id="3-1-1-正样本、负样本，其实是在玩“认人游戏”"><a href="#3-1-1-正样本、负样本，其实是在玩“认人游戏”" class="headerlink" title="3.1.1 正样本、负样本，其实是在玩“认人游戏”"></a>3.1.1 正样本、负样本，其实是在玩“认人游戏”</h4><p>大多数对比学习方法，都会把训练过程设计成这样一种游戏：</p>
<ul>
<li>同一张图片，经过不同的数据增强（裁剪、旋转、颜色变化等）→ <strong>应该被当成“同一个东西”</strong></li>
<li>不同图片→ <strong>应该被区分开</strong></li>
</ul>
<p>于是，训练中会出现两种关系：</p>
<ul>
<li><strong>正样本对</strong>：来自同一张原图的不同版本</li>
<li><strong>负样本对</strong>：来自不同图片的版本</li>
</ul>
<p>模型要做的事很简单：</p>
<ul>
<li>把正样本拉近</li>
<li>把负样本推远</li>
</ul>
<p>如果你换一种更生活化的说法，其实就是：<strong>不管怎么拍、怎么变，这还是你；但你和别人，必须分清楚。</strong> 这个过程背后，隐含着一个非常重要的假设：<strong>同一张图片的不同视角，在语义上是等价的；不同图片，在统计意义上是可以区分的。</strong></p>
<p>在数据量足够大、增强策略合理的情况下，这个假设通常成立，于是模型就会学到对光照、裁剪、尺度变化不敏感的特征。但这里已经埋下了一个伏笔：<strong>整个过程，并没有显式告诉模型“什么是猫、什么是狗”。</strong> 模型只是在学习：哪些变化要忽略，哪些差异要放大。</p>
<h4 id="3-1-2-不变性：既是优点，也是代价"><a href="#3-1-2-不变性：既是优点，也是代价" class="headerlink" title="3.1.2 不变性：既是优点，也是代价"></a>3.1.2 不变性：既是优点，也是代价</h4><p>对比学习成功的一个核心原因，是它<strong>强迫模型学会“不变性”</strong>。也就是说：</p>
<ul>
<li>不重要的变化（裁剪、颜色、噪声）要忽略</li>
<li>稳定的因素要保留下来</li>
</ul>
<p>这让模型学到的表示更加稳定、好迁移。但问题在于：<strong>不是所有任务，都希望模型“忽略这么多东西”。</strong> 如果数据增强过强，比如：</p>
<ul>
<li>裁剪掉关键局部</li>
<li>扰乱了颜色或纹理信息</li>
</ul>
<p>那么模型可能会被迫丢掉一些<strong>对细粒度任务非常重要的线索</strong>。于是，不变性开始从“优势”，变成“限制”。</p>
<h3 id="3-2-用信息论解释对比学习：有用，但不万能"><a href="#3-2-用信息论解释对比学习：有用，但不万能" class="headerlink" title="3.2 用信息论解释对比学习：有用，但不万能"></a>3.2 用信息论解释对比学习：有用，但不万能</h3><h4 id="3-2-1-一个常见说法：对比学习在“最大化共同信息”"><a href="#3-2-1-一个常见说法：对比学习在“最大化共同信息”" class="headerlink" title="3.2.1 一个常见说法：对比学习在“最大化共同信息”"></a>3.2.1 一个常见说法：对比学习在“最大化共同信息”</h4><p>为了从理论上理解对比学习，研究者常常借助信息论的语言。你可能会看到一种说法：对比学习通过 InfoNCE 损失，在最大化同一图片不同视角之间的“共同信息”。</p>
<p>直观理解就是：</p>
<ul>
<li>两个视角都能看到的东西，往往是稳定、重要的</li>
<li>学到这些东西，就能得到好表示</li>
</ul>
<p>这个解释在早期非常有帮助，它让人们第一次从理论角度理解：<strong>“为什么这种拉近 &#x2F; 推远的训练方式能工作”。</strong></p>
<h4 id="3-2-2-为什么温度、负样本数量这么重要？"><a href="#3-2-2-为什么温度、负样本数量这么重要？" class="headerlink" title="3.2.2 为什么温度、负样本数量这么重要？"></a>3.2.2 为什么温度、负样本数量这么重要？</h4><p>在实际训练中，人们发现一些现象：</p>
<ul>
<li>负样本越多，效果往往越好</li>
<li>温度参数调小，表示空间更“紧凑”</li>
<li>线性分类性能会随之提升</li>
</ul>
<p>这些经验现象，通常被解读为：<strong>区分压力越大，模型学到的表示越清晰。</strong> 但问题在于：<strong>这些细节，很难用“互信息最大化”一句话完全解释清楚。</strong></p>
<h4 id="3-2-3-信息论解释的局限在哪里？"><a href="#3-2-3-信息论解释的局限在哪里？" class="headerlink" title="3.2.3 信息论解释的局限在哪里？"></a>3.2.3 信息论解释的局限在哪里？</h4><p>随着研究深入，人们逐渐意识到：InfoNCE 只是一个很松的理论下界，实际训练效果，强烈依赖：</p>
<ul>
<li>网络结构</li>
<li>归一化方式</li>
<li>优化动态</li>
</ul>
<p>这说明信息论更像是一个“帮助理解的比喻”，而不是对比学习的完整理论基础。它有用，但不能当成最终答案。</p>
<h3 id="3-3-三个经典方法，三种设计取向"><a href="#3-3-三个经典方法，三种设计取向" class="headerlink" title="3.3 三个经典方法，三种设计取向"></a>3.3 三个经典方法，三种设计取向</h3><h4 id="3-3-1-SimCLR：资源够多，就把事情做简单"><a href="#3-3-1-SimCLR：资源够多，就把事情做简单" class="headerlink" title="3.3.1 SimCLR：资源够多，就把事情做简单"></a>3.3.1 SimCLR：资源够多，就把事情做简单</h4><p>SimCLR 的核心思想非常“直接”：</p>
<ul>
<li>用很强的数据增强</li>
<li>用很大的 batch，保证有足够多负样本</li>
<li>端到端训练，不引入复杂机制</li>
</ul>
<p>它传达的一个重要信息是：<strong>很多时候，不是方法不行，而是资源不够。</strong> SimCLR 让人们意识到：对比学习的效果，往往更多取决于<strong>训练规模和增强策略</strong>，而不是花哨的算法设计。</p>
<h4 id="3-3-2-MoCo：工程设计能不能救方法？"><a href="#3-3-2-MoCo：工程设计能不能救方法？" class="headerlink" title="3.3.2 MoCo：工程设计能不能救方法？"></a>3.3.2 MoCo：工程设计能不能救方法？</h4><p>MoCo 关注的是另一个现实问题：<strong>如果我没有那么大的 batch，对比学习还能不能稳定训练？</strong> 它通过：</p>
<ul>
<li>动量编码器</li>
<li>负样本队列</li>
</ul>
<p>在显存可控的前提下，维持了大量负样本。这说明了一点非常重要的工程事实：<strong>对比学习的成功，不只是目标函数的胜利，也是工程稳定性的胜利。</strong></p>
<h4 id="3-3-3-DINO：没有负样本，也能学到好表示？"><a href="#3-3-3-DINO：没有负样本，也能学到好表示？" class="headerlink" title="3.3.3 DINO：没有负样本，也能学到好表示？"></a>3.3.3 DINO：没有负样本，也能学到好表示？</h4><p>DINO 走了一条更“反直觉”的路：</p>
<ul>
<li>不显式使用负样本</li>
<li>用教师—学生结构</li>
<li>通过自蒸馏维持表示多样性</li>
</ul>
<p>它的成功，直接挑战了一个长期默认的观点：<strong>对比学习 ≠ 必须有负样本。</strong> 这也进一步说明：表示是否塌缩；是否有足够结构，并不完全由“负样本数量”决定，<strong>优化动态和结构约束同样关键。</strong></p>
<h3 id="3-4-对比学习的局限：什么时候开始“不太好用”？"><a href="#3-4-对比学习的局限：什么时候开始“不太好用”？" class="headerlink" title="3.4 对比学习的局限：什么时候开始“不太好用”？"></a>3.4 对比学习的局限：什么时候开始“不太好用”？</h3><h4 id="3-4-1-在细粒度任务中，问题会暴露"><a href="#3-4-1-在细粒度任务中，问题会暴露" class="headerlink" title="3.4.1 在细粒度任务中，问题会暴露"></a>3.4.1 在细粒度任务中，问题会暴露</h4><p>当任务需要关注：</p>
<ul>
<li>微小局部差异</li>
<li>精细结构关系</li>
</ul>
<p>实例级区分目标，反而可能成为负担。模型为了“区分实例”，会主动忽略一些对语义其实很重要的细节。</p>
<h4 id="3-4-2-对增强策略的依赖，让方法不够“省心”"><a href="#3-4-2-对增强策略的依赖，让方法不够“省心”" class="headerlink" title="3.4.2 对增强策略的依赖，让方法不够“省心”"></a>3.4.2 对增强策略的依赖，让方法不够“省心”</h4><p>对比学习对数据增强非常敏感，而增强策略：</p>
<ul>
<li>往往依赖经验</li>
<li>需要针对数据集反复调试</li>
</ul>
<p>这使得它在一些场景下，不够“即插即用”。</p>
<p><strong>本章小结</strong><br>本章围绕对比学习这一经典自监督路线，分析了它<strong>为什么有效、又为什么不够完整</strong>。</p>
<p>通过实例区分和正负样本构造，对比学习成功地塑造了判别性很强的表示空间，使模型在许多下游任务中表现优秀。但这种成功，更多来自于“区分能力”的强化，而非对高层语义的显式建模。</p>
<p>信息论视角为理解对比学习提供了有益直觉，但无法解释所有经验现象。方法的实际效果，来自目标函数、模型结构、数据增强与优化动态的共同作用。</p>
<p>这些分析也自然引出了一个问题：<strong>如果我们不只想“分清楚”，而是想“看懂整体结构”，还能不能有更合适的自监督方式？</strong> 这正是下一章——遮蔽重建方法——要回答的问题。</p>
<h2 id="4-遮蔽重建与-MAE：让模型“补全图片”，能学到更稳的表示吗？"><a href="#4-遮蔽重建与-MAE：让模型“补全图片”，能学到更稳的表示吗？" class="headerlink" title="4. 遮蔽重建与 MAE：让模型“补全图片”，能学到更稳的表示吗？"></a>4. 遮蔽重建与 MAE：让模型“补全图片”，能学到更稳的表示吗？</h2><p>上一章的对比学习，核心是“把不同图片分清楚”。它确实很强，但也有明显特点：<strong>非常依赖数据增强</strong>，而且有时会把一些对任务重要的细节“当成噪声”丢掉。所以研究者开始想另一条路：</p>
<blockquote>
<p>如果不要求模型拼命区分不同图片，而是让模型学会“理解一张图片内部的结构”，会不会得到更稳、更通用的视觉表示？</p>
</blockquote>
<p>这就是<strong>遮蔽重建（masked reconstruction）</strong> 的思路。其中最典型的代表，就是 <strong>MAE（Masked Autoencoder）</strong>。我们可以把 MAE 理解成一句话：<strong>把图片遮住一大半，让模型把缺的部分猜回来。</strong></p>
<h3 id="4-1-为什么“遮蔽建模”在-NLP-很成功，但在视觉里很晚才火？"><a href="#4-1-为什么“遮蔽建模”在-NLP-很成功，但在视觉里很晚才火？" class="headerlink" title="4.1 为什么“遮蔽建模”在 NLP 很成功，但在视觉里很晚才火？"></a>4.1 为什么“遮蔽建模”在 NLP 很成功，但在视觉里很晚才火？</h3><h4 id="4-1-1-语言是“离散词块”，图像是“连续像素”"><a href="#4-1-1-语言是“离散词块”，图像是“连续像素”" class="headerlink" title="4.1.1 语言是“离散词块”，图像是“连续像素”"></a>4.1.1 语言是“离散词块”，图像是“连续像素”</h4><p>在 NLP 里，BERT 这类方法做的事是：遮住一句话里的某些词，让模型预测这些词是什么。这很自然，因为：</p>
<ul>
<li>词是离散单位（一个词一个词地出现）</li>
<li>预测“被遮住的词”本身就有明确意义</li>
</ul>
<p>但图像不一样。图像是连续像素组成的，如果你让模型去预测被遮住区域的<strong>每个像素值</strong>，模型很容易走捷径：</p>
<ul>
<li>用局部纹理复制一下</li>
<li>用颜色插值糊一糊</li>
<li>甚至只学会“画得像”，不学“理解是什么”</li>
</ul>
<p>结果就是：<strong>早期视觉遮蔽方法经常学到的是低级统计规律，而不是可迁移的语义表示。</strong> 这也是为什么遮蔽建模在视觉里很长时间都不如对比学习出彩。</p>
<h4 id="4-1-2-关键转折：Vision-Transformer-让“图像像句子一样”可建模"><a href="#4-1-2-关键转折：Vision-Transformer-让“图像像句子一样”可建模" class="headerlink" title="4.1.2 关键转折：Vision Transformer 让“图像像句子一样”可建模"></a>4.1.2 关键转折：Vision Transformer 让“图像像句子一样”可建模</h4><p>遮蔽重建真正强起来，很大程度上因为 <strong>Vision Transformer（ViT）</strong>。ViT 会先把图像切成一块块小方块（patch），然后把每一块当成一个 token。这样，图像输入在形式上就很像 NLP 的序列：</p>
<ul>
<li>句子：token 序列</li>
<li>图片：patch token 序列</li>
</ul>
<p>再加上 Transformer 的注意力机制很擅长抓全局关系，于是：<strong>即使遮掉很多局部信息，模型也能靠全局上下文推断缺失内容。</strong> 这才让视觉遮蔽建模真正变得“可行且有效”。</p>
<h3 id="4-2-MAE-为什么能成功？三个关键设计"><a href="#4-2-MAE-为什么能成功？三个关键设计" class="headerlink" title="4.2 MAE 为什么能成功？三个关键设计"></a>4.2 MAE 为什么能成功？三个关键设计</h3><h4 id="4-2-1-“遮住-75-”反而更好：逼模型别走捷径"><a href="#4-2-1-“遮住-75-”反而更好：逼模型别走捷径" class="headerlink" title="4.2.1 “遮住 75%”反而更好：逼模型别走捷径"></a>4.2.1 “遮住 75%”反而更好：逼模型别走捷径</h4><p>MAE 最反直觉的一点是：它不是遮一点点，而是<strong>遮掉大部分</strong>，常见是 75% 甚至更多。直觉上我们会觉得：遮这么多，模型怎么猜得出来？但效果反而更好，原因是：</p>
<ul>
<li>遮得少 → 模型容易靠局部纹理、插值复制完成任务</li>
<li>遮得多 → 模型没法只靠局部复制，必须依赖<strong>整体结构和语义线索</strong></li>
</ul>
<p>所以高遮蔽率的真实作用是：<strong>逼迫模型学“整体理解”，而不是学“局部糊弄”。</strong></p>
<h4 id="4-2-2-非对称结构：编码器很强，解码器很轻"><a href="#4-2-2-非对称结构：编码器很强，解码器很轻" class="headerlink" title="4.2.2 非对称结构：编码器很强，解码器很轻"></a>4.2.2 非对称结构：编码器很强，解码器很轻</h4><p>MAE 还有一个非常工程友好的设计：</p>
<ul>
<li><strong>编码器（Encoder）</strong>：只看没被遮住的那一小部分 patch<br>  → 可以做得很深很强（这部分最终会用于下游任务）</li>
<li><strong>解码器（Decoder）</strong>：负责把缺的 patch 重建出来<br>  → 做得比较轻量就够了</li>
</ul>
<p>这样做有两个好处：</p>
<ol>
<li>训练更省算力：编码器只处理少量可见块</li>
<li>把能力集中在“真正要用的编码器”上</li>
</ol>
<p>也体现了 MAE 的核心态度：<strong>重建不是目的，只是手段；真正要的是一个好用的视觉编码器。</strong></p>
<h4 id="4-2-3-为什么-CNN-很难复制-MAE-的效果？"><a href="#4-2-3-为什么-CNN-很难复制-MAE-的效果？" class="headerlink" title="4.2.3 为什么 CNN 很难复制 MAE 的效果？"></a>4.2.3 为什么 CNN 很难复制 MAE 的效果？</h4><p>可能有人会问：既然是遮住再补回来，那 CNN 做自编码器不也可以吗？理论上可以，但实践里 CNN 往往没 MAE 强，原因很直观：</p>
<ul>
<li>CNN 天然偏向局部模式（卷积看的是邻域）</li>
<li>在大量遮蔽时，很多邻域信息根本不存在</li>
<li>这时就需要强大的全局建模能力，而 Transformer 更擅长</li>
</ul>
<p>所以 MAE 的成功不是“换个损失函数”就行，而是：<strong>任务设计（高遮蔽） + 结构优势（ViT 全局关系）一起起作用。</strong></p>
<h3 id="4-3-MAE-学到的表示有什么特点？哪里强、哪里不占优？"><a href="#4-3-MAE-学到的表示有什么特点？哪里强、哪里不占优？" class="headerlink" title="4.3 MAE 学到的表示有什么特点？哪里强、哪里不占优？"></a>4.3 MAE 学到的表示有什么特点？哪里强、哪里不占优？</h3><h4 id="4-3-1-强项：更“懂整体”，迁移更稳"><a href="#4-3-1-强项：更“懂整体”，迁移更稳" class="headerlink" title="4.3.1 强项：更“懂整体”，迁移更稳"></a>4.3.1 强项：更“懂整体”，迁移更稳</h4><p>很多实验观察到，MAE 的表示在这些方面很有优势：</p>
<ul>
<li>语义理解更强（更会用上下文）</li>
<li>迁移更稳（换任务、换分布时更靠谱）</li>
<li>小样本监督时往往更吃香（少量标注也能跑得不错）</li>
</ul>
<p>直觉原因也很好理解：MAE 训练时就一直在做“缺信息情况下靠整体推断”，所以学到的表示更偏向整体结构。</p>
<h4 id="4-3-2-可能的弱点：判别性不如对比学习那么“天生强”"><a href="#4-3-2-可能的弱点：判别性不如对比学习那么“天生强”" class="headerlink" title="4.3.2 可能的弱点：判别性不如对比学习那么“天生强”"></a>4.3.2 可能的弱点：判别性不如对比学习那么“天生强”</h4><p>但 MAE 并不意味着全方位吊打对比学习。在一些纯粹强调“快速区分”的任务里（比如只做分类并追求线性可分性），对比学习有时会更占优，因为它训练目标就是“拉开距离、分得更开”。所以可以把两者的偏好简单理解为：</p>
<ul>
<li><strong>对比学习</strong>：更擅长“分清楚”</li>
<li><strong>MAE</strong>：更擅长“补完整、看结构”</li>
</ul>
<pre class="mermaid">flowchart LR
    A[输入图像] --> B[Patch 划分]

    B --> C[随机高比例遮蔽]
    C --> D[可见 Patch 子集]

    %% Encoder
    D --> E[编码器<br/>高容量 Transformer]

    %% Decoder
    E --> F[轻量解码器]
    F --> G[重建被遮蔽 Patch]

    %% Pretraining Loss
    G --> H[重建损失]

    %% Transfer
    E --> I[预训练完成的视觉编码器]
    I --> J[下游任务迁移]
    J --> K[分类 检测 分割]</pre>
<center>图 4-1 MAE 架构与训练流程示意图</center>

<p>MAE 的流程可以一句话概括：</p>
<ol>
<li>把图片切成很多 patch</li>
<li>随机遮住其中绝大部分</li>
<li>用编码器只处理可见部分</li>
<li>用解码器把缺失部分重建出来</li>
<li>预训练结束后，<strong>只保留编码器</strong>去做分类&#x2F;检测&#x2F;分割等任务</li>
</ol>
<p>重点不在“重建得多逼真”，而在：<strong>通过重建过程，逼编码器学到更通用的表示。</strong></p>
<p><strong>本章小结</strong><br>本章介绍了遮蔽重建路线，并重点解释了 MAE 为什么能在视觉自监督中崛起。</p>
<p>遮蔽建模在视觉领域起步较晚，是因为图像像素是连续的，早期方法容易学到低级纹理捷径；而 ViT 把图像变成 patch token 序列，再加上注意力的全局建模能力，才让遮蔽预测真正有意义。</p>
<p>MAE 的成功来自三个关键点：高遮蔽率逼模型关注整体结构，非对称编码器–解码器降低训练成本并把能力集中在编码器上，以及 Transformer 在全局推断上的结构优势。</p>
<p>总体上，MAE 的表示往往更稳、更偏向全局语义，但在纯判别任务上不一定总胜过对比学习。这也为下一章的对照分析铺垫了结论：两条路线更可能是互补，而不是替代。</p>
<h2 id="5-对比学习与-MAE：不是谁取代谁，而是各干各的活"><a href="#5-对比学习与-MAE：不是谁取代谁，而是各干各的活" class="headerlink" title="5. 对比学习与 MAE：不是谁取代谁，而是各干各的活"></a>5. 对比学习与 MAE：不是谁取代谁，而是各干各的活</h2><p>前两章我们分别看了两种自监督方法：</p>
<ul>
<li><strong>对比学习</strong>：通过把不同图片分清楚，学到好用的表示</li>
<li><strong>MAE（遮蔽重建）</strong>：通过补全被遮住的图片，学到对整体结构的理解</li>
</ul>
<p>如果只看结果，很容易掉进一个误区：到底是对比学习更强，还是 MAE 更先进？但更合理、也更有解释力的理解方式，其实是：<strong>它们不是在解决同一个问题，而是在做“不同分工”的表征学习。</strong></p>
<h3 id="5-1-它们学到的“表示”，本质上哪里不一样？"><a href="#5-1-它们学到的“表示”，本质上哪里不一样？" class="headerlink" title="5.1 它们学到的“表示”，本质上哪里不一样？"></a>5.1 它们学到的“表示”，本质上哪里不一样？</h3><h4 id="5-1-1-一个擅长“分清楚”，一个擅长“看整体”"><a href="#5-1-1-一个擅长“分清楚”，一个擅长“看整体”" class="headerlink" title="5.1.1 一个擅长“分清楚”，一个擅长“看整体”"></a>5.1.1 一个擅长“分清楚”，一个擅长“看整体”</h4><p>我们可以这样直观地理解两者的区别：</p>
<ul>
<li><strong>对比学习</strong><br>  更像是在教模型：“哪些东西一定要区分开？”</li>
<li><strong>MAE</strong><br>  更像是在教模型：“这张图整体上长什么样？各部分怎么关联？”</li>
</ul>
<p>对比学习不断拉开不同图片在特征空间中的距离，于是得到的是：</p>
<ul>
<li>结构清晰</li>
<li>很好分</li>
<li>用简单分类器就能用的表示</li>
</ul>
<p>这也是为什么它在<strong>线性分类测试</strong>中经常表现很好。而 MAE 的训练目标根本不是“把图片分得越开越好”，而是：</p>
<ul>
<li>在信息缺失的情况下</li>
<li>尽可能理解整体结构</li>
<li>把缺的部分补回来</li>
</ul>
<p>所以它学到的表示，往往：</p>
<ul>
<li>更偏向全局结构</li>
<li>不一定在几何上特别“好分”</li>
<li>但在复杂任务和迁移中更稳</li>
</ul>
<p><strong>两者都学到了语义，只是“语义被组织的方式不一样”。</strong></p>
<h4 id="5-1-2-一个在“主动丢信息”，一个在“尽量保信息”"><a href="#5-1-2-一个在“主动丢信息”，一个在“尽量保信息”" class="headerlink" title="5.1.2 一个在“主动丢信息”，一个在“尽量保信息”"></a>5.1.2 一个在“主动丢信息”，一个在“尽量保信息”</h4><p>这是一个非常关键、但经常被忽略的差别。对比学习的核心手段是“不变性”：</p>
<ul>
<li>不重要的变化 → 忽略</li>
<li>重要的变化 → 保留</li>
</ul>
<p>这种做法的好处是表示更紧凑，判别更高效。但代价是：</p>
<ul>
<li>有些细节，可能在预训练阶段就被扔掉了</li>
<li>等下游任务想用时，已经找不回来了</li>
</ul>
<p>MAE 的思路正好相反。因为要重建被遮住的大量内容，模型不得不：</p>
<ul>
<li>尽量保留对结构有帮助的信息</li>
<li>哪怕暂时用不上，也先留着</li>
</ul>
<p>结果是表示更“厚”，下游任务可以再用监督信号来“雕刻”判别边界。所以可以简单总结为：<strong>对比学习更像“提前做筛选”，MAE 更像“先尽量都记下来”。</strong></p>
<h3 id="5-2-实际用的时候，该选哪一个？"><a href="#5-2-实际用的时候，该选哪一个？" class="headerlink" title="5.2 实际用的时候，该选哪一个？"></a>5.2 实际用的时候，该选哪一个？</h3><p>这其实是初学者最关心的问题。</p>
<h4 id="5-2-1-看三件事：数据、任务、模型"><a href="#5-2-1-看三件事：数据、任务、模型" class="headerlink" title="5.2.1 看三件事：数据、任务、模型"></a>5.2.1 看三件事：数据、任务、模型</h4><ul>
<li><strong>数据规模</strong><ul>
<li>数据不算特别大 → 对比学习往往更省事</li>
<li>数据非常大 → MAE 更容易发挥优势</li>
</ul>
</li>
<li><strong>任务类型</strong><ul>
<li>目标很明确、偏分类 → 对比学习通常很好用</li>
<li>任务复杂、类型多、需要迁移 → MAE 更稳</li>
</ul>
</li>
<li><strong>模型结构</strong><ul>
<li>CNN 或轻量模型 → 对比学习更现实</li>
<li>Vision Transformer → MAE 非常适配</li>
</ul>
</li>
</ul>
<h4 id="5-2-2-Few-shot-和密集预测任务的区别"><a href="#5-2-2-Few-shot-和密集预测任务的区别" class="headerlink" title="5.2.2 Few-shot 和密集预测任务的区别"></a>5.2.2 Few-shot 和密集预测任务的区别</h4><p>在 <strong>Few-shot（样本很少）</strong> 的场景下：</p>
<ul>
<li>MAE 往往更稳，因为它学到的表示更完整</li>
<li>对比学习有时会因为“过早分得太死”而不稳定</li>
</ul>
<p>在 <strong>检测、分割这类任务</strong> 中：</p>
<ul>
<li>如果场景简单、类别边界清晰 → 对比学习仍然有优势</li>
<li>如果场景复杂、跨数据集 → MAE 的迁移一致性更好</li>
</ul>
<p>这再次说明：<strong>不是方法好坏的问题，而是是否“对路”。</strong></p>
<pre class="mermaid">flowchart LR
    %% Left: Contrastive Learning
    subgraph CL[对比学习 Contrastive Learning]
        CL1[数据规模<br/>中等~大规模]
        CL2[任务类型<br/>判别型任务优先<br/>分类 线性探针]
        CL3[模型结构<br/>CNN / ViT 均可]
        CL4[学习偏好<br/>不变性<br/>实例区分]
        CL5[表征特性<br/>几何结构清晰<br/>线性可分性强]
        CL6[迁移需求<br/>快速适配<br/>低成本下游训练]
    end

    %% Right: MAE
    subgraph MAE[遮蔽重建 MAE]
        MAE1[数据规模<br/>大规模更优]
        MAE2[任务类型<br/>结构与语义建模<br/>检测 分割 多任务]
        MAE3[模型结构<br/>Vision Transformer 更适配]
        MAE4[学习偏好<br/>可预测性<br/>上下文建模]
        MAE5[表征特性<br/>全局语义强<br/>迁移稳定性高]
        MAE6[迁移需求<br/>通用基础表征<br/>跨任务一致性]
    end

    %% Alignment lines
    CL1 --- MAE1
    CL2 --- MAE2
    CL3 --- MAE3
    CL4 --- MAE4
    CL5 --- MAE5
    CL6 --- MAE6</pre>
<center>图 5-1 对比学习与 MAE 的适用场景与表征偏好对照</center>

<p>如上图可以概括为</p>
<ul>
<li>对比学习：“我想要一个好分、好用、能快速上线的表示。”</li>
<li>MAE：“我想要一个底子扎实、以后干啥都不容易翻车的表示。”</li>
</ul>
<p><strong>本章小结</strong><br>本章的核心结论其实只有一句话：<strong>对比学习和 MAE 不是替代关系，而是功能分工。</strong></p>
<p>对比学习通过强判别目标，构建了几何结构清晰、效率高的表示，非常适合快速迁移到判别型任务；MAE 通过生成式目标，学习了更完整、更稳健的结构表示，在复杂任务和跨场景迁移中更有优势。它们分别回答了两个不同的问题：</p>
<ul>
<li>如何快速、有效地“分清楚”？</li>
<li>如何在信息不完整时“看懂整体”？</li>
</ul>
<p>也正是因为这两个问题都很重要，后续研究才开始尝试把它们<strong>融合起来</strong>。下一章，我们将进一步讨论这些融合式方法，看看它们如何在实践中结合判别性与结构理解的优势。</p>
<h2 id="6-融合路线：当“分清楚”和“看整体”开始一起学"><a href="#6-融合路线：当“分清楚”和“看整体”开始一起学" class="headerlink" title="6. 融合路线：当“分清楚”和“看整体”开始一起学"></a>6. 融合路线：当“分清楚”和“看整体”开始一起学</h2><p>在前面的章节里，我们已经看到两条非常清晰的路线：</p>
<ul>
<li><strong>对比学习</strong>：靠区分不同图片，学到判别性很强的表示</li>
<li><strong>遮蔽重建（MAE）</strong>：靠补全缺失内容，学到对整体结构的理解</li>
</ul>
<p>随着研究的深入，大家逐渐意识到一个事实：这两种方法都很有价值，但<strong>单独使用时，各自都有明显短板</strong>。于是，研究重心开始发生变化：不再纠结“哪条路线更正确”，而是开始思考：<strong>能不能把它们的优点结合起来？</strong> 这就是所谓的<strong>融合路线</strong>。</p>
<h3 id="6-1-BEiT：把“补像素”升级成“猜语义标签”"><a href="#6-1-BEiT：把“补像素”升级成“猜语义标签”" class="headerlink" title="6.1 BEiT：把“补像素”升级成“猜语义标签”"></a>6.1 BEiT：把“补像素”升级成“猜语义标签”</h3><h4 id="6-1-1-一个关键想法：问题可能出在“预测目标太低级”"><a href="#6-1-1-一个关键想法：问题可能出在“预测目标太低级”" class="headerlink" title="6.1.1 一个关键想法：问题可能出在“预测目标太低级”"></a>6.1.1 一个关键想法：问题可能出在“预测目标太低级”</h4><p>研究者回头一看早期遮蔽重建失败的原因，发现一个核心问题：<strong>让模型去预测每个像素值，太容易学成“纹理复制”，而不是语义理解。</strong> 于是 BEiT 提出了一个很巧妙的改动：</p>
<ul>
<li>不直接预测像素</li>
<li>而是先把图像 patch 变成<strong>离散的视觉 token</strong></li>
<li>再让模型去预测这些 token</li>
</ul>
<p>如果用人话说，就是：不再让模型“画出来长什么样”，而是让模型“说出来这块像什么”。</p>
<h4 id="6-1-2-为什么离散-token-本身就像一种“弱监督”？"><a href="#6-1-2-为什么离散-token-本身就像一种“弱监督”？" class="headerlink" title="6.1.2 为什么离散 token 本身就像一种“弱监督”？"></a>6.1.2 为什么离散 token 本身就像一种“弱监督”？</h4><p>这些视觉 token 并不是人工标签，但它们也不是纯随机的：</p>
<ul>
<li>tokenizer 本身是预训练过的</li>
<li>已经包含了一些“视觉原型”的信息</li>
</ul>
<p>所以当模型去预测这些 token 时，实际上是在：<strong>用一种“已经带点语义味道的目标”来做遮蔽预测。</strong> 这就像给模型加了一点“提示”，而不是让它完全靠像素自己悟。</p>
<h4 id="6-1-3-BEiT-的收获与代价"><a href="#6-1-3-BEiT-的收获与代价" class="headerlink" title="6.1.3 BEiT 的收获与代价"></a>6.1.3 BEiT 的收获与代价</h4><p>BEiT 在很多任务上确实效果不错，尤其是：</p>
<ul>
<li>模型大</li>
<li>数据多</li>
</ul>
<p>但它也引入了新的问题：</p>
<ul>
<li>tokenizer 好不好，直接决定效果</li>
<li>tokenizer 本身训练也很复杂</li>
<li>整个系统不再“自洽”，而是依赖外部模块</li>
</ul>
<p>所以 BEiT 的经验是：离散化很有用，但<strong>依赖外部先验，本身也是一种风险</strong>。</p>
<h3 id="6-2-iBOT：不用离散标签，也能学到结构"><a href="#6-2-iBOT：不用离散标签，也能学到结构" class="headerlink" title="6.2 iBOT：不用离散标签，也能学到结构"></a>6.2 iBOT：不用离散标签，也能学到结构</h3><h4 id="6-2-1-把“对比”的思想，塞进“遮蔽重建”里"><a href="#6-2-1-把“对比”的思想，塞进“遮蔽重建”里" class="headerlink" title="6.2.1 把“对比”的思想，塞进“遮蔽重建”里"></a>6.2.1 把“对比”的思想，塞进“遮蔽重建”里</h4><p>iBOT 的出发点是另一个问题：我能不能在遮蔽建模的框架里，加一点“判别约束”，但又不回到传统对比学习那套负样本机制？它的答案是：<strong>自蒸馏</strong>。具体来说：</p>
<ul>
<li>用教师–学生模型</li>
<li>看同一张图的不同视角、不同遮蔽方式</li>
<li>要求它们的表示“保持一致”</li>
</ul>
<p>这样做的效果是不用显式负样本，但表示也不会塌缩，同时还能保留生成式建模的好处。</p>
<h4 id="6-2-2-为什么-iBOT-强调-patch-级别？"><a href="#6-2-2-为什么-iBOT-强调-patch-级别？" class="headerlink" title="6.2.2 为什么 iBOT 强调 patch 级别？"></a>6.2.2 为什么 iBOT 强调 patch 级别？</h4><p>和很多只看整张图的方法不同，iBOT 把约束下放到了 <strong>patch 层面</strong>。结果是：</p>
<ul>
<li>全局表示：学整体语义</li>
<li>局部表示：保留区域级结构</li>
</ul>
<p>这在很多实际任务中非常有用，尤其是：</p>
<ul>
<li>检测</li>
<li>分割</li>
<li>跨任务迁移</li>
</ul>
<p>iBOT 传达的一个重要信息是：融合不是只能在“整张图”这个层面发生，在更细粒度的表示上，空间反而更大。</p>
<pre class="mermaid">flowchart LR
    %% Input
    A[输入图像] --> B[Patch 划分]
    B --> C[随机遮蔽]

    %% BEiT Branch
    subgraph BEiT[BEiT：离散化重建路线]
        C --> D1[可见 Patch]
        D1 --> E1[视觉编码器]
        E1 --> F1[预测被遮蔽 Patch 的离散 Token]
        F1 --> G1[监督信号<br/>来自外部 Tokenizer<br/>（如 dVAE）]
        G1 --> H1[表征粒度<br/>偏语义原型级]
    end

    %% iBOT Branch
    subgraph iBOT[iBOT：判别一致性路线]
        C --> D2[可见 Patch + 遮蔽 Patch]
        D2 --> E2[学生编码器]
        E2 --> F2[教师编码器<br/>EMA 更新]
        E2 --> G2[Patch / CLS 表征预测]
        F2 --> G2
        G2 --> H2[监督信号<br/>自蒸馏一致性<br/>无显式离散 Token]
        H2 --> I2[表征粒度<br/>实例级 + Patch 级]
    end

    %% Alignment notes
    H1 --- I2</pre>
<center>图6-1 融合式自监督方法结构示意图（BEiT vs iBOT）</center>

<p>这张图想表达的核心对比，其实可以浓缩成一句话：</p>
<ul>
<li><strong>BEiT</strong>：先想好“答案长什么样”，再照着这个答案去猜。</li>
<li><strong>iBOT</strong>：不给标准答案，但要求你在不同条件下“自己说话要前后一致”。</li>
</ul>
<p>两条路，目标相同：<strong>同时要语义，又要结构。</strong></p>
<p><strong>本章小结</strong><br>本章介绍了自监督学习中的融合路线，重点讨论了 BEiT 与 iBOT 两种代表性方法。</p>
<p>BEiT 通过引入离散视觉 token，把遮蔽建模从像素层面提升到更接近语义的层面，在一定程度上弥补了纯像素重建的不足；但这种做法也引入了对外部 tokenizer 的依赖。</p>
<p>iBOT 则在遮蔽建模框架中融入自蒸馏和一致性约束，在不显式使用负样本的情况下，引入了判别式结构，同时保留了生成式建模的优势。</p>
<p>这些方法共同说明了一点：<strong>对比与重建并不是非此即彼，融合设计正在成为新的常态。</strong> 但融合并不意味着问题已经解决，它往往带来更复杂的训练流程和更多设计选择。</p>
<p>下一章将把视角拉回工程实践，讨论这些自监督方法在真实训练和落地过程中，会遇到哪些现实挑战。</p>
<h2 id="7-训练一个“真的能迁移”的自监督视觉模型"><a href="#7-训练一个“真的能迁移”的自监督视觉模型" class="headerlink" title="7. 训练一个“真的能迁移”的自监督视觉模型"></a>7. 训练一个“真的能迁移”的自监督视觉模型</h2><p>前面的章节，我们已经讨论了很多“方法该怎么理解”的问题。但对大多数初学者来说，真正关心的往往是：</p>
<blockquote>
<p><strong>我照着做，能不能真的训练出一个有用的模型？</strong></p>
</blockquote>
<p>这一章我们就不再抽象讨论方法，而是假设一个<strong>非常真实的工程场景</strong>，完整走一遍：<strong>预训练 → 评估 → 迁移</strong>，看看哪些选择真的重要，哪些指标反而容易误导。</p>
<h3 id="7-1-一个很现实的起点：资源有限、目标务实"><a href="#7-1-一个很现实的起点：资源有限、目标务实" class="headerlink" title="7.1 一个很现实的起点：资源有限、目标务实"></a>7.1 一个很现实的起点：资源有限、目标务实</h3><p>先说清楚背景。我们想做的事是：<strong>训练一个通用视觉编码器</strong>，以后可以拿去做：</p>
<ul>
<li>图像分类</li>
<li>目标检测</li>
<li>语义分割</li>
</ul>
<p>但现实条件是：</p>
<ul>
<li>有很多<strong>无标签图片</strong>（大约 5000 万张，来源杂）</li>
<li>有一点<strong>标注数据</strong>（ImageNet + 一个中等规模检测数据集）</li>
<li>算力不算差，但也<strong>不是无限大</strong></li>
</ul>
<p>目标也很清醒：<strong>不追 SOTA，只求在多任务迁移时稳定、不翻车。</strong> 在这个前提下，对比学习和 MAE 都是选项，但工程体验差异会非常明显。</p>
<h3 id="7-2-预训练阶段：为什么最后选了-MAE？"><a href="#7-2-预训练阶段：为什么最后选了-MAE？" class="headerlink" title="7.2 预训练阶段：为什么最后选了 MAE？"></a>7.2 预训练阶段：为什么最后选了 MAE？</h3><h4 id="7-2-1-方法选择不是看分数，而是看“省不省心”"><a href="#7-2-1-方法选择不是看分数，而是看“省不省心”" class="headerlink" title="7.2.1 方法选择不是看分数，而是看“省不省心”"></a>7.2.1 方法选择不是看分数，而是看“省不省心”</h4><p>最终我们选择的是：<strong>MAE + ViT-B</strong>。原因并不复杂：</p>
<ul>
<li>对数据增强不那么敏感</li>
<li>不强依赖超大 batch</li>
<li>在不同下游任务中表现更稳定</li>
</ul>
<p>一句话总结就是：<strong>在资源和调参精力有限的情况下，MAE 更“省心”。</strong> 一个很重要的经验是遮蔽比例：</p>
<ul>
<li>遮蔽 75%：效果最好</li>
<li>遮蔽降到 50% 以下：<br>  → 模型更容易学到纹理细节<br>  → 迁移性能明显变差</li>
</ul>
<h4 id="7-2-2-训练时间比你想象的更重要"><a href="#7-2-2-训练时间比你想象的更重要" class="headerlink" title="7.2.2 训练时间比你想象的更重要"></a>7.2.2 训练时间比你想象的更重要</h4><p>很多初学者会纠结：</p>
<ul>
<li>模型要不要再大一点？</li>
<li>架构要不要再复杂一点？</li>
</ul>
<p>但实践中，一个反复出现的结论是：<strong>训练不够久，比模型不够大更致命。</strong> 在多次实验中发现：</p>
<ul>
<li>训练轮数太少 → 学到的表示很浅</li>
<li>后面再怎么微调，都补不回来</li>
</ul>
<p>所以我们宁愿用 ViT-B，但训练得足够久（800+ epoch）</p>
<h3 id="7-3-怎么评估“学得好不好”？别被一个指标骗了"><a href="#7-3-怎么评估“学得好不好”？别被一个指标骗了" class="headerlink" title="7.3 怎么评估“学得好不好”？别被一个指标骗了"></a>7.3 怎么评估“学得好不好”？别被一个指标骗了</h3><h4 id="7-3-1-线性探针：参考可以，但别迷信"><a href="#7-3-1-线性探针：参考可以，但别迷信" class="headerlink" title="7.3.1 线性探针：参考可以，但别迷信"></a>7.3.1 线性探针：参考可以，但别迷信</h4><p>预训练完成后，第一件事通常是跑 <strong>ImageNet 线性分类</strong>。结果是：MAE 的线性精度，明显不如对比学习。如果你只看这个数字，很容易得出结论：MAE 不行？</p>
<p>但在这个实践里，我们<strong>并没有因此否定 MAE</strong>。原因是：线性探针主要反映的是——<strong>“不用怎么调，直接拿来分，能分多清楚？”</strong> 而不是：<strong>“这个表示以后还能被塑造成什么样？”</strong></p>
<h4 id="7-3-2-真正拉开差距的，是下游微调"><a href="#7-3-2-真正拉开差距的，是下游微调" class="headerlink" title="7.3.2 真正拉开差距的，是下游微调"></a>7.3.2 真正拉开差距的，是下游微调</h4><p>当我们把模型迁移到目标检测、语义分割。并且开始正常微调时，情况反而反过来了：</p>
<ul>
<li>MAE 收敛更稳定</li>
<li>最终性能更可靠</li>
<li>在标注样本少的时候优势更明显</li>
</ul>
<p>这说明了一点：<strong>线性探针看的是“起步速度”，下游微调看的是“成长潜力”。</strong></p>
<h3 id="7-4-一些看似不起眼，但很关键的工程细节"><a href="#7-4-一些看似不起眼，但很关键的工程细节" class="headerlink" title="7.4 一些看似不起眼，但很关键的工程细节"></a>7.4 一些看似不起眼，但很关键的工程细节</h3><h4 id="7-4-1-batch-不够大怎么办？"><a href="#7-4-1-batch-不够大怎么办？" class="headerlink" title="7.4.1 batch 不够大怎么办？"></a>7.4.1 batch 不够大怎么办？</h4><p>现实中 batch size 往往受显存限制。我们的经验是：</p>
<ul>
<li>不要硬拉 batch</li>
<li>学习率缩放保守一点</li>
<li>反而更稳定</li>
</ul>
<p>稳定收敛，比“理论最优配置”更重要。</p>
<h4 id="7-4-2-和融合方案的对照：不是越复杂越好"><a href="#7-4-2-和融合方案的对照：不是越复杂越好" class="headerlink" title="7.4.2 和融合方案的对照：不是越复杂越好"></a>7.4.2 和融合方案的对照：不是越复杂越好</h4><p>我们也试过一版 <strong>iBOT 风格</strong> 的模型作为对照：</p>
<ul>
<li>引入 EMA</li>
<li>教师–学生结构</li>
<li>训练确实更稳定</li>
</ul>
<p>但代价是：</p>
<ul>
<li>工程复杂度明显上升</li>
<li>调参成本更高</li>
</ul>
<p>在“稳健迁移优先”的目标下，<strong>MAE 的性价比更高</strong>。</p>
<h3 id="7-5-别只看最终精度，要看“样本效率曲线”"><a href="#7-5-别只看最终精度，要看“样本效率曲线”" class="headerlink" title="7.5 别只看最终精度，要看“样本效率曲线”"></a>7.5 别只看最终精度，要看“样本效率曲线”</h3><p>最后，我们做了 Few-shot 实验：用 10%、20%、30% 的标注数据，观察性能如何随样本数变化。结果非常有启发性：MAE 的性能提升很平滑，对比学习模型波动更大，对初始化和增强策略更敏感。这类曲线往往比“最高精度是多少”，更能说明一个表示<strong>是不是好迁移</strong>。</p>
<p><strong>本章小结</strong><br>通过这一完整实践，我们得到一个非常朴素、但很重要的结论：<strong>一个好的自监督模型，不一定在某个榜单上最亮眼，但应该在不同任务、不同数据量下都不容易翻车。</strong></p>
<p>在真实工程中，方法选择永远是折中的结果：算力、数据分布、调参成本、目标任务共同决定了“最合适”的方案，而不是“理论上最强”的方案。</p>
<p>这一章想传达的核心思想是：<strong>关注长期可迁移性，比追单点指标更重要。</strong> 这也自然引出了下一章：在视觉大模型和多模态体系中，自监督学习正在扮演怎样的新角色？</p>
<h2 id="8-自监督在视觉大模型中的角色变化：从“备选方案”到“默认配置”"><a href="#8-自监督在视觉大模型中的角色变化：从“备选方案”到“默认配置”" class="headerlink" title="8. 自监督在视觉大模型中的角色变化：从“备选方案”到“默认配置”"></a>8. 自监督在视觉大模型中的角色变化：从“备选方案”到“默认配置”</h2><p>在计算机视觉的早期阶段，模型通常是为<strong>某一个具体任务</strong>量身定做的：</p>
<ul>
<li>一个模型做分类</li>
<li>另一个模型做检测</li>
<li>再一个模型做分割</li>
</ul>
<p>但随着模型越来越大、任务越来越多，这种做法开始变得不现实。今天的视觉模型，更像是一个<strong>通用能力模块</strong>，要能被反复复用、不断扩展。在这个过程中，自监督学习的角色发生了一个非常重要的变化：<strong>它不再只是“没标签时的替代方案”，而是视觉大模型预训练阶段的默认选择。</strong></p>
<h3 id="8-1-为什么自监督成了视觉大模型的“底座”？"><a href="#8-1-为什么自监督成了视觉大模型的“底座”？" class="headerlink" title="8.1 为什么自监督成了视觉大模型的“底座”？"></a>8.1 为什么自监督成了视觉大模型的“底座”？</h3><h4 id="8-1-1-模型越大，标签反而越不够用"><a href="#8-1-1-模型越大，标签反而越不够用" class="headerlink" title="8.1.1 模型越大，标签反而越不够用"></a>8.1.1 模型越大，标签反而越不够用</h4><p>在模型规模比较小的时候，监督学习其实很好用：</p>
<ul>
<li>给定明确任务</li>
<li>用足够多标签</li>
<li>直接把性能拉上去</li>
</ul>
<p>但当模型规模和数据规模一起膨胀时，问题就来了：</p>
<ul>
<li>标签永远追不上数据增长</li>
<li>标签定义本身很有限（只能标你事先想到的东西）</li>
<li>过于具体的监督目标，会把模型“训练死在某种任务思路里”</li>
</ul>
<p>这时，自监督的优势开始显现。通过在<strong>海量无标签数据</strong>上训练，模型学到的是：</p>
<ul>
<li>形状</li>
<li>结构</li>
<li>空间关系</li>
<li>稳定的视觉规律</li>
</ul>
<p>而不是某一个具体任务的答案。于是，自监督的定位悄悄发生了变化：<strong>从“提升某个任务的技巧”，变成了“构建通用视觉能力的基础设施”。</strong></p>
<h4 id="8-1-2-自监督和监督，其实是分工合作"><a href="#8-1-2-自监督和监督，其实是分工合作" class="headerlink" title="8.1.2 自监督和监督，其实是分工合作"></a>8.1.2 自监督和监督，其实是分工合作</h4><p>在现在的视觉大模型中，很少再是“全靠监督”或“完全不用监督”，更常见的流程是：</p>
<ol>
<li><strong>先用自监督学一个通用视觉编码器</strong></li>
<li><strong>再用少量标注数据微调或对齐到具体任务</strong></li>
</ol>
<p>可以把它理解成：</p>
<ul>
<li>自监督：打地基</li>
<li>监督学习：装修房子</li>
</ul>
<p>地基越稳，后面改成什么样的房子，都更容易。</p>
<h3 id="8-2-自监督如何支撑多模态模型？"><a href="#8-2-自监督如何支撑多模态模型？" class="headerlink" title="8.2 自监督如何支撑多模态模型？"></a>8.2 自监督如何支撑多模态模型？</h3><h4 id="8-2-1-先“看懂世界”，再“和语言对齐”"><a href="#8-2-1-先“看懂世界”，再“和语言对齐”" class="headerlink" title="8.2.1 先“看懂世界”，再“和语言对齐”"></a>8.2.1 先“看懂世界”，再“和语言对齐”</h4><p>视觉大模型的发展，已经不再局限于“只看图像”。现在很多模型都要：</p>
<ul>
<li>看图</li>
<li>读文本</li>
<li>理解两者之间的关系</li>
</ul>
<p>在这种多模态模型中，一个非常现实的问题是：<strong>如果视觉模型自己都没“看懂图像”，那让它和语言对齐会非常困难。</strong> 自监督视觉预训练，正好解决了这个问题。</p>
<p>在进入“图文对齐”之前，视觉编码器已经对视觉结构有稳定理解，不容易因为数据波动而崩，这就像一个人：</p>
<ul>
<li>先学会观察世界</li>
<li>再学会用语言描述世界</li>
</ul>
<h4 id="8-2-2-好的视觉表示，让对齐变得更容易"><a href="#8-2-2-好的视觉表示，让对齐变得更容易" class="headerlink" title="8.2.2 好的视觉表示，让对齐变得更容易"></a>8.2.2 好的视觉表示，让对齐变得更容易</h4><p>多模态模型里的“对齐学习”，通常靠对比或匹配来完成。但如果视觉特征本身很混乱，对齐就会变成一件非常不稳定的事。自监督学习的作用在这里非常关键：<strong>它先在视觉内部，把表示空间整理好，再让不同模态去对齐这个空间。</strong></p>
<p>这也是为什么，现在很多视觉—语言模型，都先做大规模视觉自监督预训练，而不是从零开始端到端对齐。</p>
<h3 id="8-3-自监督成功了，但问题并没有结束"><a href="#8-3-自监督成功了，但问题并没有结束" class="headerlink" title="8.3 自监督成功了，但问题并没有结束"></a>8.3 自监督成功了，但问题并没有结束</h3><h4 id="8-3-1-表示塌缩，为什么还能被避免？"><a href="#8-3-1-表示塌缩，为什么还能被避免？" class="headerlink" title="8.3.1 表示塌缩，为什么还能被避免？"></a>8.3.1 表示塌缩，为什么还能被避免？</h4><p>我们可能已经注意到一个事实：</p>
<ul>
<li>有的方法靠负样本</li>
<li>有的方法靠自蒸馏</li>
<li>有的方法靠优化技巧</li>
</ul>
<p>都能在实践中防止模型“学成一团一样的东西”。但从理论角度看：<strong>为什么这些方法能奏效？有没有一个统一的解释？</strong> 这个问题在模型越来越大的今天，变得越来越重要。如果搞不清楚原理，我们就很难判断方法还能不能继续扩展。</p>
<h4 id="8-3-2-数据增强：是在帮模型，还是在“偷偷教模型”？"><a href="#8-3-2-数据增强：是在帮模型，还是在“偷偷教模型”？" class="headerlink" title="8.3.2 数据增强：是在帮模型，还是在“偷偷教模型”？"></a>8.3.2 数据增强：是在帮模型，还是在“偷偷教模型”？</h4><p>另一个经常被忽略的问题是数据增强。很多自监督方法，非常依赖裁剪、翻转、颜色扰动，这些增强操作，本质上在告诉模型：</p>
<ul>
<li>什么变化该忽略</li>
<li>什么变化很重要</li>
</ul>
<p>这就引出了一个更深层的问题：自监督学习，到底是在“自己发现语义”，还是在“被我们用增强规则悄悄教语义”？这个问题没有简单答案，但它直接关系到：</p>
<ul>
<li>方法的泛化能力</li>
<li>是否能减少人工经验依赖</li>
</ul>
<p><strong>本章小结</strong><br>本章从视觉大模型的发展背景出发，讨论了自监督学习角色的变化。</p>
<p>随着模型和数据规模的增长，自监督已经从“没有标签时的备选方案”，转变为构建通用视觉能力的默认步骤。它为视觉大模型提供了稳定、可扩展的表示基础，并在多模态模型中承担着关键支撑作用。</p>
<p>与此同时，自监督学习仍然存在一些尚未彻底解决的问题，例如表示塌缩的理论解释，以及数据增强在多大程度上等同于人为注入先验。这些问题说明：<strong>自监督已经很强，但远没有“被完全理解”。</strong></p>
<h2 id="9-总结：自监督真正改变了什么？它还没解决什么？"><a href="#9-总结：自监督真正改变了什么？它还没解决什么？" class="headerlink" title="9. 总结：自监督真正改变了什么？它还没解决什么？"></a>9. 总结：自监督真正改变了什么？它还没解决什么？</h2><p>在前面的章节中，我们从多个角度看了自监督学习：</p>
<ul>
<li>为什么它会出现</li>
<li>它有哪些代表性方法</li>
<li>在工程中怎么用</li>
<li>在大模型时代扮演什么角色</li>
</ul>
<p>现在可以站远一点，回答两个最重要的问题：<strong>自监督学习，真正改变了什么？它又还有哪些问题没有解决？</strong></p>
<h3 id="9-1-自监督真正改变的，不是模型，而是“怎么学”"><a href="#9-1-自监督真正改变的，不是模型，而是“怎么学”" class="headerlink" title="9.1 自监督真正改变的，不是模型，而是“怎么学”"></a>9.1 自监督真正改变的，不是模型，而是“怎么学”</h3><h4 id="9-1-1-从“看标签”到“看数据本身”"><a href="#9-1-1-从“看标签”到“看数据本身”" class="headerlink" title="9.1.1 从“看标签”到“看数据本身”"></a>9.1.1 从“看标签”到“看数据本身”</h4><p>在传统监督学习里，训练模型的核心逻辑是：人先想好标签体系 → 模型学习去复现这个体系。模型的注意力，被牢牢绑定在“答题”上。而自监督学习，做了一件非常关键的转变：<strong>不再把标签当成学习的起点，而是把数据本身当成信息来源。</strong> 模型开始关心：</p>
<ul>
<li>哪些结构是稳定的</li>
<li>哪些变化是可以忽略的</li>
<li>哪些关系在不同视角下都成立</li>
</ul>
<p>不管是对比学习，还是遮蔽重建，本质上都在做同一件事：<strong>在没有人告诉“答案”的情况下，找出值得长期记住的视觉规律。</strong> 这是自监督最根本、也是最深远的改变。</p>
<h4 id="9-1-2-从“把一个任务做好”，到“学一个能反复用的表示”"><a href="#9-1-2-从“把一个任务做好”，到“学一个能反复用的表示”" class="headerlink" title="9.1.2 从“把一个任务做好”，到“学一个能反复用的表示”"></a>9.1.2 从“把一个任务做好”，到“学一个能反复用的表示”</h4><p>监督学习通常是<strong>任务优先</strong>的：为分类而分类、为检测而检测。模型在某个任务上表现很好，但一换任务就要重来。自监督学习把顺序反了过来：</p>
<ol>
<li>先学一个相对通用、稳定的视觉表示</li>
<li>再根据具体任务做少量调整</li>
</ol>
<p>结果是：</p>
<ul>
<li>用更少标注数据</li>
<li>适应更多任务</li>
<li>对分布变化更不容易翻车</li>
</ul>
<p>这也是为什么现在越来越多工作，把“表征本身”当成核心资产，而不是某一个指标。</p>
<h3 id="9-2-自监督也不是万能的"><a href="#9-2-自监督也不是万能的" class="headerlink" title="9.2 自监督也不是万能的"></a>9.2 自监督也不是万能的</h3><h4 id="9-2-1-它不是“装上就能赢”的银弹"><a href="#9-2-1-它不是“装上就能赢”的银弹" class="headerlink" title="9.2.1 它不是“装上就能赢”的银弹"></a>9.2.1 它不是“装上就能赢”的银弹</h4><p>虽然自监督很强，但它<strong>并不适合所有情况</strong>：</p>
<ul>
<li>数据不够多时，优势会被削弱</li>
<li>模型结构、训练细节选错，效果差异很大</li>
<li>在某些明确、简单的任务中，传统监督反而更省成本</li>
</ul>
<p>所以，更现实的定位是：<strong>自监督是一个强力工具，但需要和监督、弱监督配合使用。</strong></p>
<h4 id="9-2-2-很多成功，靠的是经验而不是理论"><a href="#9-2-2-很多成功，靠的是经验而不是理论" class="headerlink" title="9.2.2 很多成功，靠的是经验而不是理论"></a>9.2.2 很多成功，靠的是经验而不是理论</h4><p>一个必须承认的事实是：</p>
<ul>
<li>为什么某些方法不会塌缩？</li>
<li>为什么某些增强策略特别有效？</li>
<li>为什么换个设置效果就差很多？</li>
</ul>
<p>很多时候，我们<strong>知道怎么做有效，但并不完全知道为什么</strong>。这让方法设计：</p>
<ul>
<li>很依赖经验</li>
<li>不够可预测</li>
<li>调参成本高</li>
</ul>
<p>未来一个重要方向，就是把这些“经验规律”提炼成更清晰的理论和设计原则。</p>
<h4 id="9-2-3-从“学表示”走向“学世界”"><a href="#9-2-3-从“学表示”走向“学世界”" class="headerlink" title="9.2.3 从“学表示”走向“学世界”"></a>9.2.3 从“学表示”走向“学世界”</h4><p>越来越多研究开始意识到：仅仅学一个“好特征”，可能还不够。下一步的目标，正在变成：</p>
<ul>
<li>学时间上的一致性</li>
<li>学因果关系</li>
<li>学交互和动态</li>
</ul>
<p>也就是所谓的 <strong>“世界模型”</strong>。在这个过程中，自监督学习很可能继续扮演关键角色——它天然适合在没有明确标签的情况下，从连续世界中提炼结构。</p>
<p><strong>小结</strong></p>
<ul>
<li><strong>自监督真正改变的，是“学习的重心”</strong>，从标签驱动 → 数据结构驱动</li>
<li><strong>它让表征成为核心资产</strong>，而不是只服务于某一个任务</li>
<li><strong>它很强，但不是万能</strong>，仍然需要和监督方法协作</li>
<li><strong>很多成功经验，还没被完全理解</strong>，理论和设计原则仍在发展中</li>
</ul>
<p>自监督学习不是终点，而是一个新的起点。随着模型规模、任务复杂度和研究目标的不断变化，它很可能会继续重塑我们理解“学习”和“智能”的方式。</p>
<script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';	mermaid.initialize({startOnLoad: true, flowchart: {curve: 'linear'}}); </script>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/" title="视觉自监督学习：从对比学习到 MAE，再到通用视觉表征">https://www.keychan.xyz/2026/02/03/055-visual-self-supervised-learning/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
              <a href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" rel="tag"># 迁移学习</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag"># 自监督学习</a>
              <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag"># 对比学习</a>
              <a href="/tags/MAE/" rel="tag"># MAE</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/01/27/054-from-code-to-product/" rel="prev" title="「产品实践①」从代码到产品：工程思维的切换">
                  <i class="fa fa-angle-left"></i> 「产品实践①」从代码到产品：工程思维的切换
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2026/02/10/056-visual-multimodal/" rel="next" title="视觉多模态：CLIP、ALIGN 与视觉语言对齐">
                  视觉多模态：CLIP、ALIGN 与视觉语言对齐 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">533k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2026/02/03/055-visual-self-supervised-learning/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
