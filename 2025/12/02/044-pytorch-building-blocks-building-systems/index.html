<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. 从零搭建一个 PyTorch 模型在正式讨论 PyTorch 之前，我们先从一个最小可用训练流程入手：用最少的代码搭建并训练一个小型神经网络。通过这个过程来快速建立对 PyTorch 的整体认知：数据从哪里来？如何流入模型？损失如何计算？梯度如何反向传播？参数又是如何被更新的？ 后面章节会对这些环节再做细致拆解，我们先做一个“总览式体验”。 1.1 整体思路：从数据到参数更新我们开始搭建的最">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch：从搭积木到构建系统">
<meta property="og:url" content="https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. 从零搭建一个 PyTorch 模型在正式讨论 PyTorch 之前，我们先从一个最小可用训练流程入手：用最少的代码搭建并训练一个小型神经网络。通过这个过程来快速建立对 PyTorch 的整体认知：数据从哪里来？如何流入模型？损失如何计算？梯度如何反向传播？参数又是如何被更新的？ 后面章节会对这些环节再做细致拆解，我们先做一个“总览式体验”。 1.1 整体思路：从数据到参数更新我们开始搭建的最">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/JKr9SK.png">
<meta property="article:published_time" content="2025-12-02T05:09:12.000Z">
<meta property="article:modified_time" content="2025-12-09T12:45:26.863Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="模块化">
<meta property="article:tag" content="Tensor">
<meta property="article:tag" content="梯度调试">
<meta property="article:tag" content="训练工程化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/JKr9SK.png">


<link rel="canonical" href="https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/","path":"2025/12/02/044-pytorch-building-blocks-building-systems/","title":"PyTorch：从搭积木到构建系统"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PyTorch：从搭积木到构建系统 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-PyTorch-%E6%A8%A1%E5%9E%8B"><span class="nav-text">1. 从零搭建一个 PyTorch 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-text">1.1 整体思路：从数据到参数更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E6%9E%84%E5%BB%BA-Tensor-%E4%B8%8E%E5%8F%82%E6%95%B0%EF%BC%9A%E4%BB%8E%E5%BC%A0%E9%87%8F%E5%BC%80%E5%A7%8B"><span class="nav-text">1.2 构建 Tensor 与参数：从张量开始</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E4%B8%A4%E5%B1%82-MLP%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%8D%B3%E6%A8%A1%E5%9D%97"><span class="nav-text">1.3 定义一个两层 MLP：模型即模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Dataset-%E4%B8%8E-DataLoader%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%A6%82%E4%BD%95%E6%B5%81%E5%85%A5%E6%A8%A1%E5%9E%8B"><span class="nav-text">1.4 Dataset 与 DataLoader：数据如何流入模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-Dataset%EF%BC%9A%E6%95%B0%E6%8D%AE%E7%9A%84%E9%80%BB%E8%BE%91%E8%A7%86%E5%9B%BE"><span class="nav-text">1.4.1 Dataset：数据的逻辑视图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-DataLoader%EF%BC%9A%E6%89%B9%E9%87%8F%E5%8A%A0%E8%BD%BD"><span class="nav-text">1.4.2 DataLoader：批量加载</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%E4%B8%8E-MNIST-%E6%A1%88%E4%BE%8B"><span class="nav-text">1.5 完整训练循环与 MNIST 案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-1-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%E7%9A%84%E4%B8%89%E6%AD%A5%E6%A0%B8%E5%BF%83"><span class="nav-text">1.5.1 训练循环的三步核心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-2-%E5%9C%A8-MNIST-%E4%B8%8A%E8%B7%91%E9%80%9A%E4%B8%80%E4%B8%AA%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-text">1.5.2 在 MNIST 上跑通一个分类器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%BC%A0%E9%87%8F%EF%BC%9APyTorch-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6"><span class="nav-text">2. 张量：PyTorch 的核心机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Tensor-vs-NumPy%EF%BC%9A%E7%9B%B8%E4%BC%BC%E5%A4%96%E8%A1%A8%E4%B8%8B%EF%BC%8C%E4%B8%8D%E5%90%8C%E7%9A%84%E2%80%9C%E5%86%85%E6%A0%B8%E2%80%9D"><span class="nav-text">2.1 Tensor vs NumPy：相似外表下，不同的“内核”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-NumPy%EF%BC%9A%E8%AE%A1%E7%AE%97%E5%AE%8C%E5%B0%B1%E7%BB%93%E6%9D%9F"><span class="nav-text">2.1.1 NumPy：计算完就结束</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-PyTorch%EF%BC%9A%E8%AE%A1%E7%AE%97%E7%9A%84%E5%90%8C%E6%97%B6%E5%9C%A8%E2%80%9C%E7%94%BB%E5%9B%BE%E2%80%9D"><span class="nav-text">2.1.2 PyTorch：计算的同时在“画图”</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%B9%BF%E6%92%AD%E4%B8%8E%E8%A7%86%E5%9B%BE%EF%BC%9A%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="nav-text">2.2 广播与视图：高效计算的机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-%E5%B9%BF%E6%92%AD%EF%BC%9A%E4%BB%A5%E6%9C%80%E5%B0%91%E5%AD%98%E5%82%A8%E5%AE%8C%E6%88%90%E6%9C%80%E5%A4%A7%E8%AE%A1%E7%AE%97%E8%A1%A8%E8%BE%BE"><span class="nav-text">2.2.1 广播：以最少存储完成最大计算表达</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C%EF%BC%88in-place%EF%BC%89%EF%BC%9A%E6%98%AF%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98%EF%BC%8C%E4%BD%86%E5%8F%AF%E8%83%BD%E5%B8%A6%E6%9D%A5%E9%97%AE%E9%A2%98"><span class="nav-text">2.3 原地操作（in-place）：是节省内存，但可能带来问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-%E4%B8%BA%E4%BB%80%E4%B9%88-in-place-%E6%93%8D%E4%BD%9C%E9%A3%8E%E9%99%A9%E5%BE%88%E5%A4%A7%EF%BC%9F"><span class="nav-text">2.3.1 为什么 in-place 操作风险很大？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-%E5%AE%9E%E8%B7%B5%E7%BB%8F%E9%AA%8C%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%97%B6%E5%B0%BD%E9%87%8F%E9%81%BF%E5%85%8D-in-place"><span class="nav-text">2.3.2 实践经验：训练时尽量避免 in-place</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E8%AE%A1%E7%AE%97%E8%AE%BE%E5%A4%87%EF%BC%9ACPU-GPU-MPS-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BF%81%E7%A7%BB"><span class="nav-text">2.4 计算设备：CPU &#x2F; GPU &#x2F; MPS 之间的迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-%E6%8A%8A%E5%BC%A0%E9%87%8F%E7%A7%BB%E5%8A%A8%E5%88%B0%E6%8C%87%E5%AE%9A%E8%AE%BE%E5%A4%87"><span class="nav-text">2.4.1 把张量移动到指定设备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-2-%E8%B7%A8%E8%AE%BE%E5%A4%87%E5%BC%A0%E9%87%8F%E6%97%A0%E6%B3%95%E7%9B%B4%E6%8E%A5%E8%BF%90%E7%AE%97"><span class="nav-text">2.4.2 跨设备张量无法直接运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-3-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-GPU-%E5%8A%A0%E9%80%9F%E7%A4%BA%E4%BE%8B"><span class="nav-text">2.4.3 一个简单的 GPU 加速示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1%EF%BC%9A%E6%9E%84%E5%BB%BA%E5%8F%AF%E5%A4%8D%E7%94%A8%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6"><span class="nav-text">3. 模块化设计：构建可复用的神经网络组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%9C%80%E8%A6%81%E6%A8%A1%E5%9D%97%E5%8C%96%EF%BC%9F"><span class="nav-text">3.1 为什么深度学习模型需要模块化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%83%8F%E6%98%AF%E4%B8%80%E6%A3%B5%E6%A8%A1%E5%9D%97%E6%A0%91%EF%BC%88module-tree%EF%BC%89"><span class="nav-text">3.2 神经网络像是一棵模块树（module tree）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-nn-Sequential%EF%BC%9A%E6%9C%80%E8%BD%BB%E9%87%8F%E7%9A%84%E7%A7%AF%E6%9C%A8%E6%8B%BC%E6%8E%A5%E6%96%B9%E5%BC%8F"><span class="nav-text">3.3 nn.Sequential：最轻量的积木拼接方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97%EF%BC%9Aforward-%E5%86%99%E9%80%BB%E8%BE%91%EF%BC%8Cinit-%E5%AE%9A%E4%B9%89%E7%A7%AF%E6%9C%A8"><span class="nav-text">3.4 自定义模块：forward 写逻辑，init 定义积木</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9D%97%EF%BC%88Residual-Block%EF%BC%89%EF%BC%9A%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B"><span class="nav-text">3.5 残差模块（Residual Block）：模块化设计的典型案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-%E5%AD%90%E6%A8%A1%E5%9D%97%E4%B8%8E%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BD%A0%E6%89%8B%E5%8A%A8%E5%88%97%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="nav-text">3.6 子模块与参数管理：为什么优化器不需要你手动列参数？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-%E4%BD%BF%E7%94%A8-register-buffer%EF%BC%9A%E5%AD%98%E5%82%A8%E9%9D%9E%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%BC%A0%E9%87%8F"><span class="nav-text">3.7 使用 register_buffer：存储非可训练张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-8-%E6%A8%A1%E5%9D%97%E5%8C%96%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%8F%AF%E7%BB%84%E5%90%88%E6%80%A7%EF%BC%9A%E6%90%AD%E5%BB%BA%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="nav-text">3.8 模块化带来的可组合性：搭建大型模型的方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%BA%BF%EF%BC%9A%E8%AE%A9-GPU-%E4%B8%8D%E5%86%8D%E7%A9%BA%E7%AD%89%E6%95%B0%E6%8D%AE"><span class="nav-text">4. 高性能数据管线：让 GPU 不再空等数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-PyTorch-%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%BA%BF%EF%BC%9ADataset-%E2%89%A0-DataLoader"><span class="nav-text">4.1 PyTorch 的数据管线：Dataset ≠ DataLoader</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Dataset%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E7%9A%84%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84"><span class="nav-text">4.2 Dataset：定义数据的逻辑结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-DataLoader%EF%BC%9A%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%A9%E7%90%86%E5%8A%A0%E8%BD%BD%E6%96%B9%E5%BC%8F"><span class="nav-text">4.3 DataLoader：数据的物理加载方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-DataLoader-%E7%9A%84%E5%86%85%E9%83%A8%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-text">4.4 DataLoader 的内部工作机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-num-workers-%E5%B7%AE%E5%BC%82%E7%9A%84%E5%AE%9E%E9%AA%8C"><span class="nav-text">4.5 num_workers 差异的实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-Checklist"><span class="nav-text">4.6 高性能数据加载的最佳实践 Checklist</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%9APyTorch-%E7%9A%84%E7%81%B5%E9%AD%82%E6%9C%BA%E5%88%B6"><span class="nav-text">5. 动态计算图与自动微分：PyTorch 的灵魂机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Define-by-Run%EF%BC%9APyTorch-%E7%9A%84%E5%8A%A8%E6%80%81%E5%9B%BE%E6%80%9D%E6%83%B3"><span class="nav-text">5.1 Define-by-Run：PyTorch 的动态图思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Tensor-grad-fn%EF%BC%9A%E6%AF%8F%E4%B8%AA%E5%BC%A0%E9%87%8F%E9%83%BD%E7%9F%A5%E9%81%93%E2%80%9C%E8%87%AA%E5%B7%B1%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C%E2%80%9D"><span class="nav-text">5.2 Tensor.grad_fn：每个张量都知道“自己来自哪里”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-backward%EF%BC%9A%E6%B2%BF%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%8D%E5%90%91%E6%B1%82%E5%AF%BC"><span class="nav-text">5.3 backward：沿计算图反向求导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9A%E6%94%AF%E6%8C%81-Python-%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E5%85%B3%E9%94%AE"><span class="nav-text">5.4 动态计算图：支持 Python 控制流的关键</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-detach-%EF%BC%9A%E5%88%87%E6%96%AD%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD"><span class="nav-text">5.5 detach()：切断梯度传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-requires-grad-%EF%BC%9A%E5%8A%A8%E6%80%81%E5%BC%80%E5%90%AF-%E5%85%B3%E9%97%AD%E6%A2%AF%E5%BA%A6"><span class="nav-text">5.6 requires_grad_()：动态开启&#x2F;关闭梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-7-autograd-set-detect-anomaly-%EF%BC%9A%E8%B0%83%E8%AF%95%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%88%A9%E5%99%A8"><span class="nav-text">5.7 autograd.set_detect_anomaly()：调试梯度的利器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-8-%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BB%8E%E6%A2%AF%E5%BA%A6%E6%B5%81%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E2%80%9C%E6%AD%A3%E5%B8%B8%E5%AD%A6%E4%B9%A0%E2%80%9D"><span class="nav-text">5.8 实战：从梯度流判断模型是否“正常学习”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-PyTorch-%E8%B0%83%E8%AF%95%EF%BC%9A%E4%BB%8E%E6%A2%AF%E5%BA%A6%E5%88%B0%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A"><span class="nav-text">6. PyTorch 调试：从梯度到数值稳定</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E8%B0%83%E8%AF%95%E6%A0%B8%E5%BF%83%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BF%A1%E5%8F%B7%E6%B5%81%E5%8A%A8%E8%BF%87%E7%A8%8B"><span class="nav-text">6.1 调试核心：训练是一个信号流动过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E7%AC%AC%E4%B8%80%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9A%E6%A2%AF%E5%BA%A6%E5%BC%82%E5%B8%B8%EF%BC%880%E3%80%81%E5%B7%A8%E5%A4%A7%E3%80%81NaN%EF%BC%89"><span class="nav-text">6.2 第一类问题：梯度异常（0、巨大、NaN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E7%AC%AC%E4%BA%8C%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9Aloss-NaN-Inf%EF%BC%88%E6%95%B0%E5%80%BC%E4%B8%8D%E7%A8%B3%E5%AE%9A%EF%BC%89"><span class="nav-text">6.3 第二类问题：loss &#x3D; NaN &#x2F; Inf（数值不稳定）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-%E7%AC%AC%E4%B8%89%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E2%80%9C%E6%B2%A1%E6%9C%89%E6%9B%B4%E6%96%B0%E2%80%9D"><span class="nav-text">6.4 第三类问题：模型参数“没有更新”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-%E7%AC%AC%E5%9B%9B%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9A%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%96%AD%E8%A3%82%EF%BC%88Graph-Break%EF%BC%89"><span class="nav-text">6.5 第四类问题：计算图断裂（Graph Break）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-6-%E7%AC%AC%E4%BA%94%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%9Abackward-%E6%8A%A5%E9%94%99%EF%BC%88%E5%AE%9A%E4%BD%8D%E7%AE%97%E5%AD%90%EF%BC%89"><span class="nav-text">6.6 第五类问题：backward 报错（定位算子）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-%E4%BD%BF%E7%94%A8-Hook-%E8%B0%83%E8%AF%95%EF%BC%9A%E6%8D%95%E8%8E%B7%E4%BB%BB%E6%84%8F%E5%B1%82%E7%9A%84%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA-%E6%A2%AF%E5%BA%A6"><span class="nav-text">6.7 使用 Hook 调试：捕获任意层的输入&#x2F;输出&#x2F;梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-8-%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%EF%BC%9A%E6%9C%80%E5%AE%B9%E6%98%93%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E8%AE%AD%E7%BB%83%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0"><span class="nav-text">6.8 数据问题：最容易被忽略的训练失败原因</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E6%80%A7%E8%83%BD%EF%BC%9A%E4%BB%8E-Eager-%E6%89%A7%E8%A1%8C%E5%88%B0%E7%BC%96%E8%AF%91%E5%8A%A0%E9%80%9F"><span class="nav-text">7. 性能：从 Eager 执行到编译加速</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E8%AE%AD%E7%BB%83%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9FEager-%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-text">7.1 训练是如何执行的？Eager 模式的本质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-PyTorch-2-0%EF%BC%9Atorch-compile-%E7%9A%84%E5%87%BA%E7%8E%B0"><span class="nav-text">7.3 PyTorch 2.0：torch.compile 的出现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-torch-compile-%E7%9A%84%E4%B8%89%E6%AE%B5%E5%BC%8F%E6%9E%B6%E6%9E%84"><span class="nav-text">7.4 torch.compile 的三段式架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%88AMP%EF%BC%89%EF%BC%9ATensor-Core-%E6%9B%B4%E5%B0%91%E6%98%BE%E5%AD%98-%E6%9B%B4%E9%AB%98%E9%80%9F"><span class="nav-text">7.5 混合精度训练（AMP）：Tensor Core + 更少显存 + 更高速</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-Gradient-Accumulation%EF%BC%9A%E6%98%BE%E5%AD%98%E4%B8%8D%E8%B6%B3%E6%97%B6%E7%9A%84%E2%80%9C%E5%A4%A7-batch-%E6%8A%80%E5%B7%A7%E2%80%9D"><span class="nav-text">7.6 Gradient Accumulation：显存不足时的“大 batch 技巧”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-DataLoader-%E6%80%A7%E8%83%BD%EF%BC%9A%E5%87%8F%E5%B0%91-Python-overhead%EF%BC%8C%E6%8F%90%E5%8D%87%E5%90%9E%E5%90%90"><span class="nav-text">7.7 DataLoader 性能：减少 Python overhead，提升吞吐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-8-Profiling%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8D%E5%86%8D%E9%9D%A0%E7%8C%9C%E6%B5%8B"><span class="nav-text">7.8 Profiling：性能优化不再靠猜测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-9-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-Checklist"><span class="nav-text">7.9 性能优化 Checklist</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E5%B7%A5%E7%A8%8B%E7%94%9F%E6%80%81%EF%BC%9A%E4%BB%8E%E5%AE%9E%E9%AA%8C%E5%88%B0%E9%83%A8%E7%BD%B2%E7%9A%84%E4%BC%98%E9%9B%85%E9%97%AD%E7%8E%AF"><span class="nav-text">8. 工程生态：从实验到部署的优雅闭环</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%EF%BC%9A%E4%BB%8E-state-dict-%E5%BC%80%E5%A7%8B%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%9F%B3"><span class="nav-text">8.1 模型保存与加载：从 state_dict 开始的工程基石</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-1-state-dict%EF%BC%9A%E5%AE%98%E6%96%B9%E6%8E%A8%E8%8D%90%E3%80%81%E6%9C%80%E5%AE%89%E5%85%A8%E3%80%81%E6%9C%80%E7%81%B5%E6%B4%BB%E7%9A%84%E4%BF%9D%E5%AD%98%E6%96%B9%E5%BC%8F"><span class="nav-text">8.1.1 state_dict：官方推荐、最安全、最灵活的保存方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-2-%E8%AE%AD%E7%BB%83%E6%96%AD%E7%82%B9%EF%BC%9A%E4%BF%9D%E5%AD%98%E4%BC%98%E5%8C%96%E5%99%A8%E7%8A%B6%E6%80%81%EF%BC%88%E5%BF%85%E8%A6%81%E6%97%B6%EF%BC%89"><span class="nav-text">8.1.2 训练断点：保存优化器状态（必要时）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-PyTorch-Lightning%EF%BC%9A%E8%AE%A9%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%E4%BB%8E%E2%80%9C%E8%84%9A%E6%9C%AC%E2%80%9D%E5%8F%98%E6%88%90%E2%80%9C%E7%B3%BB%E7%BB%9F%E2%80%9D"><span class="nav-text">8.2 PyTorch Lightning：让训练循环从“脚本”变成“系统”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-LightningModule%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E6%9B%B4%E6%B8%85%E6%99%B0"><span class="nav-text">8.2.1 LightningModule：模型定义更清晰</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-Lightning-%E8%87%AA%E5%8A%A8%E5%A4%84%E7%90%86%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%BA%8B%E5%8A%A1"><span class="nav-text">8.2.2 Lightning 自动处理的工程事务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Hydra%EF%BC%9A%E7%9C%9F%E6%AD%A3%E5%8F%AF%E5%A4%8D%E7%8E%B0%E7%9A%84%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F"><span class="nav-text">8.3 Hydra：真正可复现的配置管理系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-1-%E5%85%B8%E5%9E%8B-Hydra-%E9%85%8D%E7%BD%AE%E7%BB%93%E6%9E%84"><span class="nav-text">8.3.1 典型 Hydra 配置结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-%E5%AE%9E%E9%AA%8C%E8%BF%BD%E8%B8%AA%EF%BC%9A%E8%AE%A9%E7%A7%91%E7%A0%94%E4%BB%8E%E2%80%9C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%9D%E8%BF%9B%E5%85%A5%E2%80%9C%E7%B3%BB%E7%BB%9F%E5%B9%B3%E5%8F%B0%E2%80%9D"><span class="nav-text">8.4 实验追踪：让科研从“文件系统”进入“系统平台”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-1-W-B-%E6%9C%80%E5%B0%8F%E7%A4%BA%E4%BE%8B"><span class="nav-text">8.4.1 W&amp;B 最小示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA%EF%BC%9A%E4%BB%8E-PyTorch-%E5%88%B0%E8%B7%A8%E5%B9%B3%E5%8F%B0%E6%8E%A8%E7%90%86"><span class="nav-text">8.5 模型导出：从 PyTorch 到跨平台推理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-1-ONNX%EF%BC%9A%E5%B7%A5%E4%B8%9A%E7%95%8C%E6%9C%80%E9%80%9A%E7%94%A8%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BA%A4%E6%8D%A2%E6%A0%BC%E5%BC%8F"><span class="nav-text">8.5.1 ONNX：工业界最通用的深度学习交换格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-2-TorchScript%EF%BC%88%E9%83%A8%E7%BD%B2%E4%BE%A7%EF%BC%89"><span class="nav-text">8.5.2 TorchScript（部署侧）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-3-TensorRT%EF%BC%9ANVIDIA-GPU-%E7%9A%84%E7%BB%88%E6%9E%81%E5%8A%A0%E9%80%9F%E6%96%B9%E5%BC%8F"><span class="nav-text">8.5.3 TensorRT：NVIDIA GPU 的终极加速方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-6-TorchServe%EF%BC%9A%E7%94%9F%E4%BA%A7%E7%BA%A7-PyTorch-%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1"><span class="nav-text">8.6 TorchServe：生产级 PyTorch 推理服务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-%E5%AE%9E%E6%88%98%E6%95%B4%E5%90%88%EF%BC%9A%E4%BB%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%88%B0%E5%AE%9E%E9%AA%8C%E8%BF%BD%E8%B8%AA%E7%9A%84%E5%AE%8C%E6%95%B4%E9%A1%B9%E7%9B%AE"><span class="nav-text">9. 实战整合：从模型训练到实验追踪的完整项目</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E5%AE%8C%E6%95%B4%E9%A1%B9%E7%9B%AE%E2%80%9D%E6%AF%94%E2%80%9C%E8%83%BD%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E2%80%9D%E6%9B%B4%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="nav-text">9.1 为什么“完整项目”比“能训练模型”更重要？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-text">9.2 项目结构设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%BA%BF%EF%BC%9A%E5%B9%B6%E8%A1%8C%E5%8A%A0%E8%BD%BD-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-%E9%AB%98%E5%90%9E%E5%90%90"><span class="nav-text">9.3 数据管线：并行加载 + 数据增强 + 高吞吐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-%E6%A8%A1%E5%9E%8B%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1%EF%BC%9ASimpleCNN"><span class="nav-text">9.4 模型模块化设计：SimpleCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-Lightning-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9D%97%EF%BC%9A%E8%AE%AD%E7%BB%83%E9%80%BB%E8%BE%91%E4%BB%8E%E8%84%9A%E6%9C%AC%E4%B8%AD%E8%A7%A3%E6%94%BE"><span class="nav-text">9.5 Lightning 训练模块：训练逻辑从脚本中解放</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-Hydra-%E9%85%8D%E7%BD%AE%EF%BC%9A%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%8F%AF%E5%A4%8D%E7%8E%B0%E8%AE%AD%E7%BB%83"><span class="nav-text">9.6 Hydra 配置：真正的可复现训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-7-%E5%B7%A5%E7%A8%8B%E5%8C%96%E8%AE%AD%E7%BB%83%EF%BC%9A%E8%87%AA%E5%8A%A8%E4%BF%9D%E5%AD%98-%E8%87%AA%E5%8A%A8%E5%AF%BC%E5%87%BA%E6%A8%A1%E5%9E%8B"><span class="nav-text">9.7 工程化训练：自动保存 + 自动导出模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-8-%E8%AE%AD%E7%BB%83%E5%87%BA%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%EF%BC%9F"><span class="nav-text">9.8 训练出的模型有什么用？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-9-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%EF%BC%9A%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%88%90%E4%B8%BA%E7%9C%9F%E5%AE%9E%E6%9C%8D%E5%8A%A1"><span class="nav-text">9.9 模型部署：让模型成为真实服务</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-9-1-%E6%9C%AC%E5%9C%B0%E6%8E%A8%E7%90%86%EF%BC%88inference-py%EF%BC%89"><span class="nav-text">9.9.1 本地推理（inference.py）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-9-2-%E5%9C%A8%E7%BA%BF-API-%E6%8E%A8%E7%90%86%EF%BC%88FastAPI%EF%BC%89"><span class="nav-text">9.9.2 在线 API 推理（FastAPI）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-10-%E5%AF%BC%E5%87%BA-ONNX%EF%BC%9A%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%B7%A8%E5%B9%B3%E5%8F%B0"><span class="nav-text">9.10 导出 ONNX：让模型跨平台</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-11-%E5%85%A8%E6%B5%81%E7%A8%8B%E5%B7%A5%E7%A8%8B%E5%9B%BE"><span class="nav-text">9.11 全流程工程图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-%E6%80%BB%E7%BB%93%EF%BC%9A%E4%BB%8E%E2%80%9C%E5%86%99%E4%BB%A3%E7%A0%81%E2%80%9D%E5%88%B0%E2%80%9C%E6%9E%84%E5%BB%BA%E7%B3%BB%E7%BB%9F%E2%80%9D"><span class="nav-text">10. 总结：从“写代码”到“构建系统”</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-%E5%A4%87%E6%B3%A8"><span class="nav-text">11. 备注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">130</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="PyTorch：从搭积木到构建系统 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch：从搭积木到构建系统
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-12-02 13:09:12" itemprop="dateCreated datePublished" datetime="2025-12-02T13:09:12+08:00">2025-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-12-09 20:45:26" itemprop="dateModified" datetime="2025-12-09T20:45:26+08:00">2025-12-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/12/02/044-pytorch-building-blocks-building-systems/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/12/02/044-pytorch-building-blocks-building-systems/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:07</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-从零搭建一个-PyTorch-模型"><a href="#1-从零搭建一个-PyTorch-模型" class="headerlink" title="1. 从零搭建一个 PyTorch 模型"></a>1. 从零搭建一个 PyTorch 模型</h2><p>在正式讨论 PyTorch 之前，我们先从一个<strong>最小可用训练流程</strong>入手：用最少的代码搭建并训练一个小型神经网络。通过这个过程来快速建立对 PyTorch 的整体认知：<strong>数据从哪里来？如何流入模型？损失如何计算？梯度如何反向传播？参数又是如何被更新的？</strong> 后面章节会对这些环节再做细致拆解，我们先做一个“总览式体验”。</p>
<h3 id="1-1-整体思路：从数据到参数更新"><a href="#1-1-整体思路：从数据到参数更新" class="headerlink" title="1.1 整体思路：从数据到参数更新"></a>1.1 整体思路：从数据到参数更新</h3><p>我们开始搭建的最小神经网络包含四个核心阶段：</p>
<ol>
<li><strong>数据准备</strong>：把原始数据转换为张量，并按批次（batch）组织；</li>
<li><strong>模型构建</strong>：用 nn.Module 定义前向计算逻辑；</li>
<li><strong>损失与优化器</strong>：定义“好坏标准”和“如何更新参数”的规则；</li>
<li><strong>训练循环</strong>：反复执行前向、反向和参数更新。</li>
</ol>
<span id="more"></span>
<p>一个典型的 PyTorch 训练循环大致如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = model(x)                 <span class="comment"># 前向传播：x → 预测 y</span></span><br><span class="line">loss = criterion(y, target)  <span class="comment"># 损失计算：预测y 与 真实y 的误差</span></span><br><span class="line">loss.backward()              <span class="comment"># 反向传播：沿计算图求梯度</span></span><br><span class="line">optimizer.step()             <span class="comment"># 参数更新：根据梯度调整权重</span></span><br><span class="line">optimizer.zero_grad()        <span class="comment"># 清除旧梯度，防止累加</span></span><br></pre></td></tr></table></figure>

<pre class="mermaid">flowchart LR
    A[Dataset<br/>样本定义] --> B[DataLoader<br/>批量加载/打乱]
    B --> C[Model Forward<br/>前向传播]
    C --> D[Loss Function<br/>损失计算]
    D --> E[Backward<br/>反向传播<br/>梯度求导]
    E --> F[Optimizer Step<br/>参数更新]

    %% 反向路径标注（虚线）
    E -.-> C</pre>
<center>图1-1 PyTorch 训练数据流动路径图</center>

<h3 id="1-2-构建-Tensor-与参数：从张量开始"><a href="#1-2-构建-Tensor-与参数：从张量开始" class="headerlink" title="1.2 构建 Tensor 与参数：从张量开始"></a>1.2 构建 Tensor 与参数：从张量开始</h3><p>在 PyTorch 中，一切计算都围绕 <strong>Tensor（张量）</strong> 展开。输入数据、模型参数、中间激活值，本质上都是张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量：4 个样本，每个样本 10 个维度特征</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重矩阵：把 10 维输入投影到 5 维输出</span></span><br><span class="line">W1 = torch.randn(<span class="number">10</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 偏置向量：对应 5 个输出神经元</span></span><br><span class="line">b1 = torch.zeros(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向计算：线性变换 + ReLU</span></span><br><span class="line">h = torch.relu(x @ W1 + b1)   <span class="comment"># 形状 [4, 5]</span></span><br></pre></td></tr></table></figure>
<p>这里有个关键点，<code>requires_grad=True</code> 意味着 <strong>需要PyTorch追踪这个张量的梯度</strong>，它会被视为可训练参数，一旦参与运算，这些张量背后会动态构建一张<strong>计算图</strong>，为之后的 backward() 做准备。这一小节先感受“<strong>张量 + 运算 &#x3D; 可求导的计算</strong>”，详细的张量机制会在第 2 章展开。</p>
<h3 id="1-3-定义一个两层-MLP：模型即模块"><a href="#1-3-定义一个两层-MLP：模型即模块" class="headerlink" title="1.3 定义一个两层 MLP：模型即模块"></a>1.3 定义一个两层 MLP：模型即模块</h3><p>在 PyTorch 中，模型通常通过继承 nn.Module 定义。这样做的好处是参数会被自动注册，便于优化器管理，而且前向逻辑集中在 forward() 中，结构清晰，还可以通过模块组合构建复杂网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 第 1 层：线性变换（输入 → 隐藏层）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_dim, hidden_dim)</span><br><span class="line">        <span class="comment"># 第 2 层：线性变换（隐藏层 → 输出层）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_dim, out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播：说明输入 x 如何一步步变成输出 y</span></span><br><span class="line"><span class="string">        每次调用 model(x) 时，PyTorch 会自动执行这里的逻辑</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># 全连接 + ReLU 激活</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)          <span class="comment"># 输出层线性变换</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型：输入 784 维（28x28 像素）→ 隐层 256 → 输出 10 类</span></span><br><span class="line">model = SimpleMLP(<span class="number">784</span>, <span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<p>第 3 章会专门讨论 nn.Module 的模块化设计理念，这里先把它当作“定义模型结构的标准方式”。</p>
<h3 id="1-4-Dataset-与-DataLoader：数据如何流入模型"><a href="#1-4-Dataset-与-DataLoader：数据如何流入模型" class="headerlink" title="1.4 Dataset 与 DataLoader：数据如何流入模型"></a>1.4 Dataset 与 DataLoader：数据如何流入模型</h3><p>PyTorch 把“<strong>数据的逻辑组织</strong>”和“<strong>数据的物理加载</strong>”明确拆开：</p>
<ul>
<li>Dataset 负责：给定一个索引 idx，返回“第 idx 个样本是什么”；</li>
<li>DataLoader 负责：如何<strong>按批次</strong>、<strong>多进程</strong>地取样本。</li>
</ul>
<h4 id="1-4-1-Dataset：数据的逻辑视图"><a href="#1-4-1-Dataset：数据的逻辑视图" class="headerlink" title="1.4.1 Dataset：数据的逻辑视图"></a>1.4.1 Dataset：数据的逻辑视图</h4><p>以下示例使用 torchvision.datasets.MNIST，它已经实现了标准的 Dataset 接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理：转为 Tensor 并归一化到 [-1, 1]</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_data = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">test_data = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<h4 id="1-4-2-DataLoader：批量加载"><a href="#1-4-2-DataLoader：批量加载" class="headerlink" title="1.4.2 DataLoader：批量加载"></a>1.4.2 DataLoader：批量加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle=True:每个 epoch 打乱数据顺序，提升泛化</span></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">128</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>这样，我们在训练循环中就可以直接迭代 train_loader，而不用关心底层是如何一条条读文件的。</p>
<h3 id="1-5-完整训练循环与-MNIST-案例"><a href="#1-5-完整训练循环与-MNIST-案例" class="headerlink" title="1.5 完整训练循环与 MNIST 案例"></a>1.5 完整训练循环与 MNIST 案例</h3><p>前面我们新建了 SimpleMLP 模型和加载了一个训练集 train_loader 与测试集 test_loader。接下来把它们拼成一个完整的训练流程。</p>
<h4 id="1-5-1-训练循环的三步核心"><a href="#1-5-1-训练循环的三步核心" class="headerlink" title="1.5.1 训练循环的三步核心"></a>1.5.1 训练循环的三步核心</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()              <span class="comment"># 多分类交叉熵</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># ① 前向传播：构建计算图并得到预测</span></span><br><span class="line">        batch_x = batch_x.view(batch_x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># [B, 1, 28, 28] → [B, 784]</span></span><br><span class="line">        pred = model(batch_x)</span><br><span class="line">        loss = criterion(pred, batch_y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ② 反向传播：沿计算图求梯度</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空旧梯度</span></span><br><span class="line">        loss.backward()         <span class="comment"># 自动计算 ∂loss/∂参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ③ 参数更新：根据梯度调整权重</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/3], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-5-2-在-MNIST-上跑通一个分类器"><a href="#1-5-2-在-MNIST-上跑通一个分类器" class="headerlink" title="1.5.2 在 MNIST 上跑通一个分类器"></a>1.5.2 在 MNIST 上跑通一个分类器</h4><p>下方代码是把前面的所有组件整合在一起，训练一个 MNIST MLP 分类器，并在测试集上做简单评估：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省略 import，同前面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据准备（Dataset + DataLoader）</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_data = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_data = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">128</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 模型定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_dim, hidden_dim)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_dim, out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = SimpleMLP(<span class="number">784</span>, <span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 损失 &amp; 优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 训练</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch_x = batch_x.view(batch_x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        pred = model(batch_x)</span><br><span class="line">        loss = criterion(pred, batch_y)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] - Loss: <span class="subst">&#123;running_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">correct, total = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.view(images.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n测试集准确率: <span class="subst">&#123;<span class="number">100</span> * correct / total:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>在普通 CPU 或单卡 GPU 上，训练 5 个 epoch 通常可以达到约 <strong>96%–97% 的测试准确率</strong>。这表示一个“看似朴素”的最小工作流，已经完整覆盖了 PyTorch 的核心计算过程：<strong>张量 → 计算图 → 自动微分 → 参数更新</strong>。</p>
<h2 id="2-张量：PyTorch-的核心机制"><a href="#2-张量：PyTorch-的核心机制" class="headerlink" title="2. 张量：PyTorch 的核心机制"></a>2. 张量：PyTorch 的核心机制</h2><p>在 PyTorch 中，有一个几乎贯穿所有机制的核心概念——<strong>Tensor（张量）</strong>。无论是输入数据、模型参数、激活值，还是计算图中的中间结果，在 PyTorch 的世界中都以张量的形式存在。更重要的是，在 PyTorch 里，张量不只是“存数据的数组”，它还携带了：</p>
<ul>
<li>是否需要梯度（requires_grad）；</li>
<li>属于哪个设备（CPU &#x2F; GPU &#x2F; MPS &#x2F; …）；</li>
<li>与其它张量的计算关系（计算图上的节点）。</li>
</ul>
<p>这使得 <strong>“张量 + 运算”</strong> 不仅完成数值计算，同时也为自动微分系统提供了基础信息。下面我们将在“最小训练循环”的基础上，系统理解 PyTorch 张量和 NumPy 的区别、广播与视图的机制、原地操作的风险，以及设备切换的规范用法。</p>
<h3 id="2-1-Tensor-vs-NumPy：相似外表下，不同的“内核”"><a href="#2-1-Tensor-vs-NumPy：相似外表下，不同的“内核”" class="headerlink" title="2.1 Tensor vs NumPy：相似外表下，不同的“内核”"></a>2.1 Tensor vs NumPy：相似外表下，不同的“内核”</h3><p>很多人第一次见到 torch.Tensor 的 API 时会有种感觉：<strong>这不就是 NumPy 吗？</strong> 确实，在“接口层面”两者很像：都支持加减乘除、索引、切片、广播等。但它们的设计目标却是截然不同：</p>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>NumPy 数组</strong></th>
<th><strong>PyTorch Tensor</strong></th>
</tr>
</thead>
<tbody><tr>
<td>核心用途</td>
<td>通用数值计算</td>
<td><strong>可微分计算 + 深度学习</strong></td>
</tr>
<tr>
<td>执行模式</td>
<td>执行即结束</td>
<td>执行即构图（Define-by-Run）</td>
</tr>
<tr>
<td>自动求导</td>
<td>不支持</td>
<td>内置 Autograd</td>
</tr>
<tr>
<td>设备支持</td>
<td>CPU</td>
<td>CPU &#x2F; GPU &#x2F; MPS &#x2F; XLA &#x2F; …</td>
</tr>
</tbody></table>
<h4 id="2-1-1-NumPy：计算完就结束"><a href="#2-1-1-NumPy：计算完就结束" class="headerlink" title="2.1.1 NumPy：计算完就结束"></a>2.1.1 NumPy：计算完就结束</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">y = (x + <span class="number">2</span>) ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># [[9. 9.]</span></span><br><span class="line"><span class="comment">#  [9. 9.]]</span></span><br></pre></td></tr></table></figure>
<p>运算执行完，结果就“落地”为一个新的数组；NumPy 不会记录中间计算步骤，也不会保留计算图，更不会去算梯度。</p>
<h4 id="2-1-2-PyTorch：计算的同时在“画图”"><a href="#2-1-2-PyTorch：计算的同时在“画图”" class="headerlink" title="2.1.2 PyTorch：计算的同时在“画图”"></a>2.1.2 PyTorch：计算的同时在“画图”</h4><p>对比一下 PyTorch：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = (x + <span class="number">2</span>) ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor([[9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9.]], grad_fn=&lt;PowBackward0&gt;)</span></span><br></pre></td></tr></table></figure>
<p>注意输出里的 <code>grad_fn=&lt;PowBackward0&gt;</code> 意味着PyTorch 不只是算出了结果 9，还记录了“<strong>这个结果是由一个幂运算得到的</strong>”，并且这个幂运算的输入又来自“加法”等操作，这些信息会组成一张<strong>计算图</strong>，在之后调用 y.backward() 时被 Autograd 引擎用来自动求梯度。</p>
<p>换句话说：在 PyTorch 中，Tensor &#x3D; 数据 + 梯度信息 + 设备信息 + 计算图中的拓扑位置。</p>
<h3 id="2-2-广播与视图：高效计算的机制"><a href="#2-2-广播与视图：高效计算的机制" class="headerlink" title="2.2 广播与视图：高效计算的机制"></a>2.2 广播与视图：高效计算的机制</h3><p>PyTorch 的高效不仅来自 GPU，更来自对 <strong>广播（broadcasting）</strong> 和 <strong>视图（view）</strong> 的精细设计。这两者是理解“为什么某些操作几乎不占内存但能完成复杂计算”的关键。</p>
<h4 id="2-2-1-广播：以最少存储完成最大计算表达"><a href="#2-2-1-广播：以最少存储完成最大计算表达" class="headerlink" title="2.2.1 广播：以最少存储完成最大计算表达"></a>2.2.1 广播：以最少存储完成最大计算表达</h4><p>广播允许形状兼容的张量在运算过程中“逻辑扩展”，而不真的复制数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">b = torch.randn(<span class="number">5</span>)     <span class="comment"># 形状 [5]</span></span><br><span class="line"></span><br><span class="line">y = x + b              <span class="comment"># 自动广播 b → [3, 5]</span></span><br></pre></td></tr></table></figure>
<p>这里 b 并不会复制三份，PyTorch 会在内部通过“广播规则”让它在运算时表现得像 [3, 5]，但底层依然只存了一份 [5]。这可以显著节省内存、提升效率。我们可以用一个示意图来理解广播过程：</p>
<pre class="mermaid">flowchart LR
    classDef tensor fill:#E8F1FD,stroke:#4A90E2,stroke-width:2px,rx:8,ry:8,color:#111
    classDef op fill:#fff,stroke:#AAB2BD,stroke-width:1.5px,rx:8,ry:8,color:#111,font-size:12px

    A[张量 x<br/>形状：3×5]:::tensor
    B[向量 b<br/>形状：5]:::tensor
    C[广播视图：<br/>b 逻辑扩展为 3×5]:::op
    D[结果 y = x + b<br/>形状：3×5]:::tensor

    A --> C
    B --> C
    C --> D</pre>
<center>图 2-1 广播机制示意图</center>
#### 2.2.2 视图（view）：共享内存的张量变形
view() 等操作可以在 **不复制数据** 的前提下，改变张量的形状，这类操作返回的是**同一块底层存储的不同“视图”**。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">2</span>, <span class="number">8</span>)   <span class="comment"># y 和 x 共用同一段内存</span></span><br></pre></td></tr></table></figure>
特点是修改 y 的数据，会反映到 x 上（只要没打断计算图等）；view() 要求底层内存是连续的，否则会报错，这时可以用 reshape() 兜底（必要时会复制数据）。

<blockquote>
<p>小结：广播“逻辑扩展维度”，view“逻辑改变形状”，两者共同实现 <strong>“表达复杂运算，但尽量不复制数据”</strong> 的目标。</p>
</blockquote>
<h3 id="2-3-原地操作（in-place）：是节省内存，但可能带来问题"><a href="#2-3-原地操作（in-place）：是节省内存，但可能带来问题" class="headerlink" title="2.3 原地操作（in-place）：是节省内存，但可能带来问题"></a>2.3 原地操作（in-place）：是节省内存，但可能带来问题</h3><p>PyTorch 中所有带下划线的方法，如 <code>x.add_()</code>、<code>x.mul_()</code>、<code>x.relu_()</code>、<code>x.copy_()</code>都是<strong>原地操作（in-place）</strong>，会直接在原内存上修改数据，不再创建新的张量。从数值计算的角度看，这似乎是“更省内存”的做法，但在 <strong>自动微分体系（Autograd）</strong> 中，它们经常是出问题的点。</p>
<h4 id="2-3-1-为什么-in-place-操作风险很大？"><a href="#2-3-1-为什么-in-place-操作风险很大？" class="headerlink" title="2.3.1 为什么 in-place 操作风险很大？"></a>2.3.1 为什么 in-place 操作风险很大？</h4><p>自动微分的本质是：<strong>反向传播需要依赖前向传播时的中间结果</strong>。如果某个中间结果在 forward 之后被原地改掉，而这个中间结果又是反向传播中需要用到的，那 Autograd 在 backward 时就会发现“我需要的值没了&#x2F;被覆盖了”，从而报错。看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">y.relu_()  <span class="comment"># 对 y 原地 ReLU</span></span><br><span class="line">z = y.mean()</span><br><span class="line">z.backward()</span><br></pre></td></tr></table></figure>
<p>在某些场景下，我们可能会看到类似错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: one of the variables needed for gradient computation</span><br><span class="line">has been modified by an inplace operation</span><br></pre></td></tr></table></figure>
<p>这里的关键点并不是“所有 in-place 操作必定会报错”，而是<strong>如果这个张量在反向传播中是“必需的中间值”，而我们对它做了原地修改，就可能破坏计算图的一致性</strong>。</p>
<p>PyTorch 内部会给参与计算图的 Tensor 维护一个“版本号”，原地修改时版本号会变化。如果 Autograd 发现“反向传播时需要的版本”和“现在的版本”对不上，就会抛出上述错误。</p>
<h4 id="2-3-2-实践经验：训练时尽量避免-in-place"><a href="#2-3-2-实践经验：训练时尽量避免-in-place" class="headerlink" title="2.3.2 实践经验：训练时尽量避免 in-place"></a>2.3.2 实践经验：训练时尽量避免 in-place</h4><p>在训练阶段，尤其是对 requires_grad&#x3D;True 的张量，<strong>尽量不要使用 in-place 操作</strong>（带下划线的方法）。除非我们非常清楚 Autograd 的细节，并确定这个张量不会再被反向用到，否则宁愿多占一点内存，用非 in-place 版本（如 y &#x3D; y.relu()）更安全。</p>
<h3 id="2-4-计算设备：CPU-GPU-MPS-之间的迁移"><a href="#2-4-计算设备：CPU-GPU-MPS-之间的迁移" class="headerlink" title="2.4 计算设备：CPU &#x2F; GPU &#x2F; MPS 之间的迁移"></a>2.4 计算设备：CPU &#x2F; GPU &#x2F; MPS 之间的迁移</h3><p>PyTorch 的一大优势是：<strong>同一套 Tensor API 可以在不同设备上无缝运行</strong>。你可以很自然地把一个模型从 CPU 挪到 GPU，而无需大改代码。</p>
<h4 id="2-4-1-把张量移动到指定设备"><a href="#2-4-1-把张量移动到指定设备" class="headerlink" title="2.4.1 把张量移动到指定设备"></a>2.4.1 把张量移动到指定设备</h4><p>常见写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x = x.to(device)              <span class="comment"># 显式迁移</span></span><br><span class="line"><span class="comment"># 或者在创建时就指定设备</span></span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">3</span>, device=device)</span><br></pre></td></tr></table></figure>
<p>模型同理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = SimpleMLP(<span class="number">784</span>, <span class="number">256</span>, <span class="number">10</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>训练时只要确保输入张量 .to(device)和模型 .to(device)，就能在 GPU 上完成前向和反向。</p>
<h4 id="2-4-2-跨设备张量无法直接运算"><a href="#2-4-2-跨设备张量无法直接运算" class="headerlink" title="2.4.2 跨设备张量无法直接运算"></a>2.4.2 跨设备张量无法直接运算</h4><p>下面的代码会报错：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>).to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">2</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">a + b   <span class="comment"># ❌ 报错：不同设备的 Tensor 无法直接运算</span></span><br></pre></td></tr></table></figure>
<p>规则可以简单记为<strong>任何参与同一运算的 Tensor 必须在同一个设备上</strong>。所以我们需要：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b_cpu = b.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">c = a + b_cpu   <span class="comment"># ✅ 都在 CPU 上</span></span><br></pre></td></tr></table></figure>
<h4 id="2-4-3-一个简单的-GPU-加速示例"><a href="#2-4-3-一个简单的-GPU-加速示例" class="headerlink" title="2.4.3 一个简单的 GPU 加速示例"></a>2.4.3 一个简单的 GPU 加速示例</h4><p>下面的例子我们可以感受大矩阵乘法在 GPU 上的速度差异（在支持 CUDA 的环境中）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">10000</span>, <span class="number">10000</span>, device=device)</span><br><span class="line">y = x @ x      <span class="comment"># 大矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(y.device)</span><br></pre></td></tr></table></figure>
<p>在 CPU 上，这个运算可能要好几秒甚至更久，而在 GPU 上，通常会快一个数量级。实际工程中，大部分深度学习训练都会把模型和大张量迁移到 GPU&#x2F;MPS 上。</p>
<p><strong>小结：张量是“PyTorch”的最小单元</strong><br>本章从多个角度回答了“为什么 Tensor 是 PyTorch 的核心机制”这一关键问题。首先，与 NumPy 相比，Tensor 并不仅仅是一个多维数组，而是专为自动微分和多设备计算而设计的计算单元。它内置了 requires_grad 与计算图机制，使得任何运算都能被自动记录并参与反向传播；同时通过 device 属性，可以在 CPU、GPU、MPS 等不同后端之间无缝切换。</p>
<p>Tensor 的广播和视图机制使复杂运算无需复制数据即可完成。广播允许在形状兼容的情况下以逻辑方式扩展张量；视图则在共享底层存储的前提下改变其形状，使得许多高维操作既高效又节省内存。</p>
<p>在自动微分体系中，原地操作则需要格外谨慎。它可能覆写反向传播所依赖的中间结果，从而导致梯度计算错误。因此在训练阶段，应尽量避免使用 in-place 操作，除非明确确认它不会破坏计算图。</p>
<p>最后，设备迁移是性能优化中的基础能力。使用 to(device) 可以以统一方式将张量移动到指定硬件，而不同设备上的张量无法直接进行算术运算，这种约束确保了计算的确定性与安全性。</p>
<h2 id="3-模块化设计：构建可复用的神经网络组件"><a href="#3-模块化设计：构建可复用的神经网络组件" class="headerlink" title="3. 模块化设计：构建可复用的神经网络组件"></a>3. 模块化设计：构建可复用的神经网络组件</h2><p>在前两章中，我们已经看到 nn.Module 是 PyTorch 中“定义一个模型”的基本方式。但在真实项目中，一个模型往往并不是一个单一的大类，而是由许多小模块拼接、嵌套、组合而成的。</p>
<p>本章的目标是系统回答这些问题：</p>
<ul>
<li>为什么深度学习框架需要“模块化”？</li>
<li>为什么 PyTorch 的 nn.Module 不是简单的“把几层写在一起”？</li>
<li>怎样让模型像乐高积木一样可组合、易扩展？</li>
<li>如何用 Sequential、自定义模块、子模块树构建复杂网络？</li>
<li>为什么 forward() 中只能写前向逻辑，而不能写参数初始化？</li>
</ul>
<h3 id="3-1-为什么深度学习模型需要模块化？"><a href="#3-1-为什么深度学习模型需要模块化？" class="headerlink" title="3.1 为什么深度学习模型需要模块化？"></a>3.1 为什么深度学习模型需要模块化？</h3><p>一个现代神经网络本质上就是由许多运算层堆叠起来的系统：线性层（Linear）、卷积层（Conv2d）、激活函数（ReLU）、正则化层（Dropout）、残差模块（Residual Block）、Transformer Block（Self-Attention + FFN + Norm）、……</p>
<p>这些“组件”本质上都是<strong>有输入、有输出、有内部参数（或无参数），可以组合嵌套的模块</strong>。模块化带来的 3 个核心优势：</p>
<ol>
<li><strong>参数自动注册，优化器可见</strong><br> 任何在 nn.Module 中定义的 nn.Parameter 会自动加入参数表中，优化器能一并管理。</li>
<li><strong>可嵌套的层级结构（module tree）</strong><br> 一个模型可以由子模块组成，子模块又可以继续包含模块，递归嵌套。</li>
<li><strong>便于保存 &#x2F; 加载 &#x2F; 调试 &#x2F; 部署</strong><br> 所有模块都会自动记录自身的结构、参数、缓冲区等，整个网络结构天然具备可序列化性。</li>
</ol>
<p>这些能力让深度学习模型像搭积木一样灵活，而不需要写成“一个超级长的 forward 函数”。</p>
<h3 id="3-2-神经网络像是一棵模块树（module-tree）"><a href="#3-2-神经网络像是一棵模块树（module-tree）" class="headerlink" title="3.2 神经网络像是一棵模块树（module tree）"></a>3.2 神经网络像是一棵模块树（module tree）</h3><p>理解 PyTorch 模型最核心的概念是：<strong>一个模型 &#x3D; 一棵由 nn.Module 组成的树（Module Tree）</strong>。例如下面的模型结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.backbone = SimpleMLP(<span class="number">784</span>, <span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.head = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.backbone(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这里Classifier 是根节点，backbone 和 head 是子节点，SimpleMLP 内部又有 fc1、fc2 这些叶子模块。</p>
<h3 id="3-3-nn-Sequential：最轻量的积木拼接方式"><a href="#3-3-nn-Sequential：最轻量的积木拼接方式" class="headerlink" title="3.3 nn.Sequential：最轻量的积木拼接方式"></a>3.3 nn.Sequential：最轻量的积木拼接方式</h3><p>当模型结构是按顺序执行的（链式结构），nn.Sequential 是最简洁的写法。例如一个 3 层前馈网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>调用 model(x) 会依次执行每个层的 forward。那么什么情况下会使用Sequential？</p>
<ul>
<li>模型是纯顺序结构；</li>
<li>不需要分支、残差、跳连等复杂逻辑；</li>
<li>没有共享参数的需求；</li>
<li>不需要在 forward 中写复杂逻辑。</li>
</ul>
<p>Sequential 概括下来就是“快、干净、易懂”。</p>
<h3 id="3-4-自定义模块：forward-写逻辑，init-定义积木"><a href="#3-4-自定义模块：forward-写逻辑，init-定义积木" class="headerlink" title="3.4 自定义模块：forward 写逻辑，init 定义积木"></a>3.4 自定义模块：forward 写逻辑，init 定义积木</h3><p>只要模型逻辑稍微复杂，就应该写成自定义 Module。例如：线性层 → ReLU → Dropout → 线性层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, p=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_dim, hidden_dim)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_dim, out_dim)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>模块内部的分工： <code>__init__</code>声明积木（哪个层、哪些参数）；forward()说明积木如何拼接使用。一个常见错误示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    fc = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)  <span class="comment"># ❌ forward 里创建层</span></span><br><span class="line">    <span class="keyword">return</span> fc(x)</span><br></pre></td></tr></table></figure>
<p>这样做的问题，每次 forward 都重新创建参数，无法被优化器管理，计算图混乱，无法保存模型，会导致模型完全不可训练。</p>
<blockquote>
<p><strong>规则：所有可训练的层必须放在 <strong>init</strong> 中定义，不要在 forward 中创建新层。</strong></p>
</blockquote>
<h3 id="3-5-残差模块（Residual-Block）：模块化设计的典型案例"><a href="#3-5-残差模块（Residual-Block）：模块化设计的典型案例" class="headerlink" title="3.5 残差模块（Residual Block）：模块化设计的典型案例"></a>3.5 残差模块（Residual Block）：模块化设计的典型案例</h3><p>残差模块（ResNet block）是一种有分支、有跳连的结构，极其适合用 nn.Module 表达。下面是经典 BasicBlock 的一个简化实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x                <span class="comment"># 残差连接</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)</span><br><span class="line">        out = out + identity        <span class="comment"># 残差相加</span></span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>结构示意图用 mermaid 表达如下：</p>
<pre class="mermaid">graph LR
    A[input] --> B[Conv2d]
    B --> C[ReLU]
    C --> D[Conv2d]
    A -- 跳连 --> D
    D --> E[ReLU]</pre>
<center>图 3-2 残差结构（Residual Block）示意图</center>

<p>左侧输入 x一路向右进入 Conv → ReLU → Conv，输入 x 同时走一条上方“直连”路线到第二个 Conv 的输出。两条路径在 “Add” 节点相加，最后再过 ReLU。这个例子体现了模块化带来的最大价值：forward 里可以写任意计算图，而不会被 Sequential 限制。</p>
<h3 id="3-6-子模块与参数管理：为什么优化器不需要你手动列参数？"><a href="#3-6-子模块与参数管理：为什么优化器不需要你手动列参数？" class="headerlink" title="3.6 子模块与参数管理：为什么优化器不需要你手动列参数？"></a>3.6 子模块与参数管理：为什么优化器不需要你手动列参数？</h3><p>任何写成 nn.Module 的模型，所有 nn.Parameter 会被自动注册，且所有子模块的参数会自动被包含。model.parameters() 会<strong>递归遍历整个 Module Tree</strong>。我们不需要手动把参数列出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<p>哪怕模型结构是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Model</span><br><span class="line"> ├── Backbone</span><br><span class="line"> │    ├── Block1</span><br><span class="line"> │    └── Block2</span><br><span class="line"> │          ├── conv1</span><br><span class="line"> │          └── conv2</span><br><span class="line"> └── Head</span><br><span class="line">      └── fc</span><br></pre></td></tr></table></figure>
<p>优化器都会自动找到所有 conv 层的权重以及所有线性层的权重，还有所有可训练参数。<strong>其实我们不需要也不应该去手动整理参数列表。</strong></p>
<h3 id="3-7-使用-register-buffer：存储非可训练张量"><a href="#3-7-使用-register-buffer：存储非可训练张量" class="headerlink" title="3.7 使用 register_buffer：存储非可训练张量"></a>3.7 使用 register_buffer：存储非可训练张量</h3><p>有些张量需要随模型保存，但不需要梯度，比如BatchNorm 的 running mean &#x2F; var、位置编码（固定）、掩码矩阵（mask）、统计量，或一些常量表。这类张量应该使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.register_buffer(<span class="string">&quot;pos_encoding&quot;</span>, pe_tensor)</span><br></pre></td></tr></table></figure>
<p>它们会随着模型保存、加载，不会出现在 model.parameters() 中，也不会被优化器更新，会随着 .to(device) 自动迁移至目标设备。这是很多人容易忽略，但非常重要的模块化工具。</p>
<h3 id="3-8-模块化带来的可组合性：搭建大型模型的方式"><a href="#3-8-模块化带来的可组合性：搭建大型模型的方式" class="headerlink" title="3.8 模块化带来的可组合性：搭建大型模型的方式"></a>3.8 模块化带来的可组合性：搭建大型模型的方式</h3><p>我们可以通过 Module + Module + Module 的方式构建复杂结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.attn = SelfAttention(dim)</span><br><span class="line">        <span class="variable language_">self</span>.ffn = FeedForward(dim, <span class="number">4</span>*dim, dim)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(dim)</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.attn(<span class="variable language_">self</span>.norm1(x))</span><br><span class="line">        x = x + <span class="variable language_">self</span>.ffn(<span class="variable language_">self</span>.norm2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>再用多个 encoder layer 堆叠：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layers = nn.ModuleList([</span><br><span class="line">    TransformerEncoderLayer(dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>这展示了模块化核心价值：<strong>复杂网络 &#x3D; 小模块的组合，而不是一个巨大函数。</strong></p>
<p><strong>小结：nn.Module 是 PyTorch 的“积木体系”</strong><br>本章从多个角度说明了为什么 PyTorch 的模型必须建立在 nn.Module 之上。首先，一个模型在本质上是一棵由子模块组成的树状结构，模块之间可以层层嵌套、分层组织，使参数管理、结构表达以及模型组合都变得清晰而有序。在这种体系中，<strong>init</strong> 用于声明模型所需的“积木”，而 forward 则负责定义计算的实际流程；也正因为职责清晰，任何可训练参数都不应在 forward 中被临时创建。</p>
<p>对于结构简单、按顺序堆叠的网络，nn.Sequential 提供了最直观的表达方式；而更复杂的模型则依赖自定义 Module 来实现，如具有分支结构、跳跃连接的残差模块，它们展示了模块化设计在表达复杂拓扑时的巨大威力。得益于这一体系，模块能够自动记录并管理所有参数，使保存、加载、分布式同步以及优化器的构建全部变得自动而可靠。此外，在某些情况下需要保存状态但又不希望它参与训练，这时 register_buffer 提供了一种专门存放“非可训练张量”的方式，保持模型结构与参数管理的统一性。</p>
<h2 id="4-高性能数据管线：让-GPU-不再空等数据"><a href="#4-高性能数据管线：让-GPU-不再空等数据" class="headerlink" title="4. 高性能数据管线：让 GPU 不再空等数据"></a>4. 高性能数据管线：让 GPU 不再空等数据</h2><p>在前面章节中，我们一直专注于“模型如何计算”，但深度学习训练中另一个关键问题是：<strong>GPU 是否真的在“全速计算”？还是大部分时间都在“等待数据”？</strong> 现代 GPU 的算力远超 CPU，当数据预处理、加载、传输不够快时，GPU 会出现大量空闲。下面是一些常见的问题：</p>
<ul>
<li>GPU 利用率忽高忽低；</li>
<li>每个 batch 之间训练卡顿；</li>
<li>DataLoader 速度远远弱于模型速度；</li>
<li>num_workers 设成默认 0，导致 CPU 串行加载数据。</li>
</ul>
<p>本章目标是回答以下几个问题</p>
<ul>
<li>Dataset 和 DataLoader 在 PyTorch 中扮演什么角色？</li>
<li>如何正确利用多进程、预取、Pinned Memory？</li>
<li>如何构建可扩展、高性能的数据加载体系？</li>
<li>如何验证数据管线的吞吐是否足够快？</li>
</ul>
<h3 id="4-1-PyTorch-的数据管线：Dataset-≠-DataLoader"><a href="#4-1-PyTorch-的数据管线：Dataset-≠-DataLoader" class="headerlink" title="4.1 PyTorch 的数据管线：Dataset ≠ DataLoader"></a>4.1 PyTorch 的数据管线：Dataset ≠ DataLoader</h3><p>PyTorch 将“数据的逻辑组织”和“数据的高效加载”<strong>彻底解耦</strong>：</p>
<ul>
<li>Dataset：定义“给定索引 idx → 返回样本”</li>
<li>DataLoader：控制“如何按批量、多进程、异步预取地取数据”</li>
</ul>
<p>这两者的解耦是提升性能的关键。</p>
<h3 id="4-2-Dataset：定义数据的逻辑结构"><a href="#4-2-Dataset：定义数据的逻辑结构" class="headerlink" title="4.2 Dataset：定义数据的逻辑结构"></a>4.2 Dataset：定义数据的逻辑结构</h3><p>一个 Dataset 最重要的是三个信息：</p>
<ol>
<li><strong>数据有多少条？</strong> → <strong>len</strong></li>
<li><strong>如何通过索引访问一条样本？</strong> → <strong>getitem</strong></li>
<li><strong>是否需要 transform包括数据要如何预处理？</strong></li>
</ol>
<p>例如加载 CIFAR-10 数据集的 Dataset：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<h3 id="4-3-DataLoader：数据的物理加载方式"><a href="#4-3-DataLoader：数据的物理加载方式" class="headerlink" title="4.3 DataLoader：数据的物理加载方式"></a>4.3 DataLoader：数据的物理加载方式</h3><p>DataLoader 决定了：</p>
<ul>
<li>怎样把 Dataset 打包成一个个 batch；</li>
<li>是否多进程并发读取；</li>
<li>是否预先加载下一批数据（prefetch）；</li>
<li>是否使用 Pinned Memory 加速 CPU→GPU 拷贝；</li>
<li>是否 shuffle &#x2F; drop_last。</li>
</ul>
<p>其最基本用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-4-DataLoader-的内部工作机制"><a href="#4-4-DataLoader-的内部工作机制" class="headerlink" title="4.4 DataLoader 的内部工作机制"></a>4.4 DataLoader 的内部工作机制</h3><p>DataLoader 的性能主要取决于三个因素：</p>
<ul>
<li><strong>num_workers：多进程并发取样本</strong><br>  默认 num_workers&#x3D;0，意味着所有数据加载 &#x3D; 主线程串行执行。如果 transform 很复杂或者图片很多，这会极其慢。在设置为多进程后，每个 worker 进程都会调用 <code>Dataset.getitem()</code>。比如 num_workers&#x3D;4，这时4 个进程会同时读取数据，然后由主进程负责拼 batch、调度。</li>
<li><strong>Prefetch（异步预取）：隐藏加载延迟</strong><br>  当 GPU 正在训练当前 batch 时，DataLoader 的 worker 会预先加载<strong>下一批数据</strong>。如果没有预取：训练结束 → 等待加载 batch → 再训练 → 再等待……。有了预取以后，训练 batch N 的同时，worker 正在准备 batch N+1。这样可以极大减少训练间隙。</li>
<li><strong>pin_memory：加速 CPU→GPU 传输</strong><br>  如果模型在 GPU 上训练：<code>pin_memory=True</code>。可以显著减少以下过程的开销：<code>batch.cpu_tensor.to(&quot;cuda&quot;)</code>。Pinned Memory 是一种“页锁定内存”，GPU 能用 DMA 进行零拷贝传输，比普通内存快很多。</li>
</ul>
<h3 id="4-5-num-workers-差异的实验"><a href="#4-5-num-workers-差异的实验" class="headerlink" title="4.5 num_workers 差异的实验"></a>4.5 num_workers 差异的实验</h3><p>下面我们用 <strong>CIFAR-10 + 复杂数据增强 + 大 batch</strong>，让 CPU 负载足够高，从而体现并行加载加速效果。步骤：</p>
<ol>
<li>使用 RandomCrop + RandomHorizontalFlip（CIFAR 常用增强）</li>
<li>batch_size 设置为较大值（如 512）</li>
<li>迭代完整数据集（一个 epoch）</li>
<li>测量耗时</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark</span>(<span class="params">num_workers</span>):</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    dataset = datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 大 batch，让 CPU 压力更大</span></span><br><span class="line">    loader = DataLoader(dataset, batch_size=<span class="number">512</span>, shuffle=<span class="literal">True</span>, num_workers=num_workers, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="keyword">pass</span>  <span class="comment"># 只衡量数据加载时间</span></span><br><span class="line">    <span class="keyword">return</span> time.time() - start</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>]:</span><br><span class="line">    t = benchmark(w)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;num_workers=<span class="subst">&#123;w&#125;</span>: <span class="subst">&#123;t:<span class="number">.2</span>f&#125;</span>s&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_workers=0: 12.5s</span><br><span class="line">num_workers=2: 6.9s</span><br><span class="line">num_workers=4: 4.8s</span><br><span class="line">num_workers=8: 4.5s</span><br></pre></td></tr></table></figure>
<p>可以看到num_workers&#x3D;0 明显最慢，增加到 4 有明显提升，8 稍有收益，但增加过多会引起调度开销。<strong>当 transform 复杂、batch 大时，多进程还可以成倍提升数据吞吐。</strong> 下面是关于选择 num_workers 的一些经验：</p>
<table>
<thead>
<tr>
<th><strong>数据规模</strong></th>
<th><strong>推荐设置</strong></th>
</tr>
</thead>
<tbody><tr>
<td>小数据（MNIST, Fashion-MNIST）</td>
<td>0 或 2 即可</td>
</tr>
<tr>
<td>中等数据（CIFAR, CelebA）</td>
<td>4–8</td>
</tr>
<tr>
<td>大数据（ImageNet）</td>
<td>8–16</td>
</tr>
<tr>
<td>自定义大图 + 重数据增强</td>
<td>num_workers ≈ CPU 核心数</td>
</tr>
</tbody></table>
<p>当然最终还是以<strong>实际 benchmark 为准</strong>。</p>
<h3 id="4-6-高性能数据加载的最佳实践-Checklist"><a href="#4-6-高性能数据加载的最佳实践-Checklist" class="headerlink" title="4.6 高性能数据加载的最佳实践 Checklist"></a>4.6 高性能数据加载的最佳实践 Checklist</h3><p>以下设置适用于大部分 GPU 训练场景：</p>
<ul>
<li>设置 num_workers &gt; 0</li>
<li>开启 pin_memory&#x3D;True</li>
<li>大 batch 情况下开启预取</li>
<li>在 Windows&#x2F;Mac 使用 if <strong>name</strong> &#x3D;&#x3D; “<strong>main</strong>“: 包裹入口</li>
<li>在 GPU 上训练时，不要让 transform 过度复杂（可用 GPU 加速增强库）</li>
<li>多 GPU 训练时可以使用 DistributedSampler</li>
<li>少用 Python 原生操作（如 for 循环），尽量用 torchvision transforms</li>
</ul>
<p><strong>小结：数据管线是训练性能的关键瓶颈</strong><br>本章通过 Dataset、DataLoader、多进程、预取以及 Pinned Memory，说明了<strong>深度学习训练的性能瓶颈常常不在 GPU，而在数据加载。</strong> 现在我们应该清楚：</p>
<ul>
<li>Dataset 只是描述数据是什么；</li>
<li>DataLoader 决定数据如何被高效地“喂”给 GPU；</li>
<li>num_workers 与 pin_memory 是性能优化的核心；</li>
<li>CIFAR-10 实验展示了并行加载的实际提升；</li>
<li>DataLoader 本质上是一个“异步生产者-消费者系统”。</li>
</ul>
<h2 id="5-动态计算图与自动微分：PyTorch-的灵魂机制"><a href="#5-动态计算图与自动微分：PyTorch-的灵魂机制" class="headerlink" title="5. 动态计算图与自动微分：PyTorch 的灵魂机制"></a>5. 动态计算图与自动微分：PyTorch 的灵魂机制</h2><p>在第 1 章我们跑通了训练流程，在第 2 章理解了 Tensor 与梯度，在第 3–4 章学习了模型和数据管线。现在我们继续PyTorch： <strong>为什么 loss.backward() 就能让整个模型自动求梯度？</strong> 以及：</p>
<ul>
<li>计算图是如何构建的？</li>
<li>grad_fn 是什么？</li>
<li>backward 是沿着什么路径传播？</li>
<li>detach() 为什么能“切断”梯度？</li>
<li>in-place 操作为什么危险？</li>
<li>为什么 PyTorch 能支持动态控制流（if&#x2F;for）？</li>
</ul>
<p>这些构成了 PyTorch 相比静态框架最大优势：<strong>Define-by-Run —— 执行即构图（动态图）。</strong> 本章将把自动微分的运行机制彻底讲清楚。</p>
<h3 id="5-1-Define-by-Run：PyTorch-的动态图思想"><a href="#5-1-Define-by-Run：PyTorch-的动态图思想" class="headerlink" title="5.1 Define-by-Run：PyTorch 的动态图思想"></a>5.1 Define-by-Run：PyTorch 的动态图思想</h3><p>我们在 Python 中写什么代码，PyTorch 就记录什么计算路径。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span> + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这两行代码做了两件事：在执行数学计算的同时，构建一张“记录这次计算过程”的计算图（Graph）。后续调用<code>y.backward()</code>，这时PyTorch 就会从 y 的 grad_fn 出发，沿着计算路径反向遍历自动应用链式法则最终把 ∂y&#x2F;∂x 放到 x.grad 中。</p>
<h3 id="5-2-Tensor-grad-fn：每个张量都知道“自己来自哪里”"><a href="#5-2-Tensor-grad-fn：每个张量都知道“自己来自哪里”" class="headerlink" title="5.2 Tensor.grad_fn：每个张量都知道“自己来自哪里”"></a>5.2 Tensor.grad_fn：每个张量都知道“自己来自哪里”</h3><p>在 PyTorch 中，只要一个 Tensor 是 <strong>通过运算产生的</strong>（不是手动创建的），并且它参与了自动求导，那么它都会带上一个属性：grad_fn。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = (x + <span class="number">3</span>) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)  <span class="comment"># &lt;PowBackward0&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn.next_functions) <span class="comment"># ((AddBackward0, 0),)</span></span><br></pre></td></tr></table></figure>
<p>y 是由平方操作（Pow）产生的<code>PowBackward0</code>，平方的输入来自加法操作（Add） → <code>AddBackward0</code>，加法的输入是叶子张量 x，x就没有 <code>grad_fn</code>。可以把 grad_fn 理解成：<strong>“这个张量是由哪个运算算出来的？backward 时应该沿着哪条路径回传？”</strong> 它是 <strong>反向传播的路线图</strong>。</p>
<p>在自动微分中，PyTorch 会把每一步前向运算都登记成一个“节点”，并将它们按计算顺序串在一起。执行 y.backward() 时，系统就会从 y 的 grad_fn 开始，一层层顺着 next_functions 倒推回去，自动完成链式求导。如果一个张量是直接创建的，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = torch.tensor(<span class="number">5.0</span>)</span><br><span class="line"><span class="built_in">print</span>(z.grad_fn)  <span class="comment"># None</span></span><br></pre></td></tr></table></figure>
<p>它就没有 grad_fn，因为它不是由计算生成的，是计算图的“叶子”，是反向传播最终停下来的节点。<strong>grad_fn 让 PyTorch 在 backward 时知道“这个张量从哪里来”，并据此完成自动求导。</strong> 这正是动态计算图能够“边执行边记录”的关键所在。</p>
<h3 id="5-3-backward：沿计算图反向求导"><a href="#5-3-backward：沿计算图反向求导" class="headerlink" title="5.3 backward：沿计算图反向求导"></a>5.3 backward：沿计算图反向求导</h3><p>理解 backward 的关键是：它并不是“凭空”算梯度，而是<strong>顺着 grad_fn 记录的计算路径，一步步把梯度从输出传回输入</strong>。继续使用同一个简单例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = (x + <span class="number">3</span>) ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># tensor(10.)</span></span><br></pre></td></tr></table></figure>
<p>数学上：<br>$$y &#x3D; (x + 3)^2,\quad \frac{dy}{dx} &#x3D; 2(x+3)&#x3D;10$$<br>但 PyTorch 的求导方式不是先推公式再代入数值，而是沿着计算图“倒着走”：</p>
<ol>
<li>y 的 grad_fn 是 PowBackward0，说明 y 来自一次“平方”</li>
<li>backward 会调用平方的反向规则，把梯度传给它的输入 (x+3)</li>
<li>这个输入由 AddBackward0 生成，所以继续向前一步</li>
<li>backward 再调用加法的反向规则，把梯度传给最终的叶子张量 x</li>
<li>得到的 dy&#x2F;dx 就被写入 x.grad</li>
</ol>
<p>换句话说，backward 做的工作就是：根据 grad_fn，把梯度一层一层沿原路传回去。为了更直观，可以画成这样一条路径：</p>
<pre class="mermaid">flowchart LR
    %% --- 样式定义 ---
    classDef forward stroke:#4A90E2,stroke-width:2px,color:#111
    classDef backward stroke:#F5A623,stroke-width:2px,stroke-dasharray:5 5,color:#111
    classDef tensor fill:#fff,stroke:#666,stroke-width:1.5px,rx:6,ry:6

    %% --- Forward Path ---
    X[x]:::tensor -->|Add| A[(x + 3)]:::tensor
    A -->|Pow| Y[y]:::tensor

    %% --- Backward Path (虚线) ---
    Y-.->|PowBackward0| A
    A-.->|AddBackward0| X</pre>
<center>图 5-1 forward/backward 路径</center>

<p>forward 是从 x 走到 y，backward 则沿着 grad_fn 反方向从 y 走回 x。每经过一个节点，就调用对应的“反向算子”（如 PowBackward0、AddBackward0）来计算局部梯度。所以最后 x.grad 得到的 10，既不是手算的，也不是 PyTorch 硬编码的，而是自动沿图反传的结果。</p>
<p>一句话总结：<strong>backward &#x3D; 顺着 grad_fn 标记的计算图，把梯度按链式法则自动传回输入张量。</strong> 这样自动微分才得以实现，不需要你为每个模型手写梯度公式。</p>
<h3 id="5-4-动态计算图：支持-Python-控制流的关键"><a href="#5-4-动态计算图：支持-Python-控制流的关键" class="headerlink" title="5.4 动态计算图：支持 Python 控制流的关键"></a>5.4 动态计算图：支持 Python 控制流的关键</h3><p>在静态图框架（早期 TensorFlow）中，if&#x2F;for&#x2F;while 都必须用特殊算子表达。而 PyTorch 动态图的核心优势是：<strong>控制流写在 Python 中即可自然成为计算图结构。</strong> 例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dynamic_net</span>(<span class="params">x</span>):</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(x.item())):</span><br><span class="line">        y = y + i</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = dynamic_net(x)</span><br><span class="line">y.backward()</span><br></pre></td></tr></table></figure>
<p>这里计算图结构取决于x 的值，以及for 循环次数和分支路径。每一次 forward 都会生成一张<strong>全新的计算图</strong>。图示如下：</p>
<pre class="mermaid">flowchart LR
    X[x] --> A[*2]
    A --> B[+0]
    B --> C[+1]
    C --> D[+2]</pre>
<center>图 5-3 动态控制流下实时生成的计算图</center>

<p>x&#x3D;3 → for 循环执行 3 次，图结构动态展开为 “+0 → +1 → +2”，若 x&#x3D;1，就会只有一次 “+0”，forward 决定图结构，图是“实时搭建”的。</p>
<h3 id="5-5-detach-：切断梯度传播"><a href="#5-5-detach-：切断梯度传播" class="headerlink" title="5.5 detach()：切断梯度传播"></a>5.5 detach()：切断梯度传播</h3><p>detach() 的作用是：<strong>保留张量的数值，但把它从当前计算图中“摘”出来，让它不再参与梯度回传。</strong> 例如，有些操作需要用到参数的“数值”，但并不希望更新它的梯度，最典型的场景是 EMA（指数滑动平均）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shadow = decay * shadow + (<span class="number">1</span> - decay) * param.detach()</span><br></pre></td></tr></table></figure>
<p>又或者我们只是想把中间特征拿出来可视化，而不是让它影响反向传播：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = model(x)</span><br></pre></td></tr></table></figure>
<p>这些操作背后都是同一件事：<strong>使用它的值，但不要它的梯度。</strong> 我们来看一个更直观的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line">z = y.detach()  <span class="comment"># z 与计算图脱离</span></span><br></pre></td></tr></table></figure>
<p>这里y 仍然属于从 x 出发的计算图，因此 backward 会从 y 回到 x，z 只是“拿到了 y 的值”，但梯度链路被切断，z.grad_fn &#x3D; None。可以用下面的图来理解（虚线表示“没有梯度关系”）：</p>
<pre class="mermaid">flowchart LR
    X[x] -->|*2| Y[y = x*2]
    Y -.->|detach| Z[z 无梯度]</pre>
<center>图 5-4 detach() 生成不带梯度的新张量</center>

<p>y 与 x 的连接是正常计算图，它们会一起参与 backward，z 虽然数值等于 y，但已完全脱离计算图，在反向传播中不会影响任何梯度。</p>
<h3 id="5-6-requires-grad-：动态开启-关闭梯度"><a href="#5-6-requires-grad-：动态开启-关闭梯度" class="headerlink" title="5.6 requires_grad_()：动态开启&#x2F;关闭梯度"></a>5.6 requires_grad_()：动态开启&#x2F;关闭梯度</h3><p>冻结模型部分参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.backbone.parameters():</span><br><span class="line">    p.requires_grad_(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>再开启：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.requires_grad_(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>适用于冻结预训练 backbone，分阶段训练，或者手动控制梯度流动</p>
<h3 id="5-7-autograd-set-detect-anomaly-：调试梯度的利器"><a href="#5-7-autograd-set-detect-anomaly-：调试梯度的利器" class="headerlink" title="5.7 autograd.set_detect_anomaly()：调试梯度的利器"></a>5.7 autograd.set_detect_anomaly()：调试梯度的利器</h3><p>当反向传播中出现NaN、Inf，或者某个算子 backward 报错。使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.set_detect_anomaly(<span class="literal">True</span>)</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p>PyTorch 会提示我们：哪个算子出现了问题，该算子的 forward&#x2F;backward 在哪一行，可能的原因是什么。这是训练大模型时必备的调试工具。</p>
<h3 id="5-8-实战：从梯度流判断模型是否“正常学习”"><a href="#5-8-实战：从梯度流判断模型是否“正常学习”" class="headerlink" title="5.8 实战：从梯度流判断模型是否“正常学习”"></a>5.8 实战：从梯度流判断模型是否“正常学习”</h3><p>一个模型是否在学习，不看 loss，也不看 acc，而看：<strong>梯度是否健康地沿网络流动。</strong> 简单监控梯度范数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">total_norm = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        total_norm += p.grad.data.norm().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Grad Norm:&quot;</span>, total_norm)</span><br></pre></td></tr></table></figure>
<p>异常信号：</p>
<ul>
<li><strong>梯度范数接近 0</strong> → 梯度消失</li>
<li><strong>梯度范数巨大（&gt;1000）</strong> → 梯度爆炸</li>
<li><strong>某层梯度一直为 0</strong> → 计算图断裂 &#x2F; in-place 破坏</li>
<li><strong>突然出现 NaN</strong> → 数值稳定性问题（学习率太大等）</li>
</ul>
<p><strong>小结：PyTorch 的灵魂是动态图</strong><br>本章系统阐述了 PyTorch 自动微分机制的运行方式，核心思想在于它采用了“Define-by-Run”的动态图范式，即计算图不是预先定义好的，而是在每一次 forward 中即时生成。前向传播结束后，会得到一张完整且只属于当前计算的一次性计算图；在 backward 执行完毕后，这张图便会被立即释放，从而保持 PyTorch 的灵活性和高效性。</p>
<p>在这张图中，每个 Tensor 都带有自己的来源信息，grad_fn 记录了它是由哪一步运算生成的。反向传播会根据这条链条，从输出位置沿着计算图自动向后追踪，依次计算梯度并传递至所有参与运算的张量。由于图是“运行时构建”的，因此 PyTorch 天然支持 Python 语言的全部控制流，包括循环、条件分支、动态结构等，这是静态图框架难以做到的。</p>
<p>本章也介绍了控制计算图行为的关键机制，例如 detach 可以切断梯度路径，让某个张量不再参与反向传播；requires_grad 决定了哪些张量需要追踪梯度；而 autograd 的异常检测工具则帮助我们在求导出错时快速定位问题。</p>
<p>在调试层面，监控梯度流是理解模型是否真正“在学习”的重要手段。无论是梯度消失、梯度爆炸，还是某些层完全没有梯度更新，都可以通过仔细检查梯度流动情况来定位和解决。</p>
<h2 id="6-PyTorch-调试：从梯度到数值稳定"><a href="#6-PyTorch-调试：从梯度到数值稳定" class="headerlink" title="6. PyTorch 调试：从梯度到数值稳定"></a>6. PyTorch 调试：从梯度到数值稳定</h2><p>深度学习代码能跑并不意味着能“正常训练”。真正棘手的问题往往出现在：</p>
<ul>
<li>模型不收敛</li>
<li>梯度莫名其妙为 0 或爆炸</li>
<li>loss 出现 NaN&#x2F;Inf</li>
<li>某层完全不更新参数</li>
<li>backward 报异常，看不懂 traceback</li>
<li>修改一点代码后训练结果完全不同</li>
<li>DataLoader 或模型结构导致图断裂</li>
</ul>
<p>训练真实模型时，这些问题比单纯写模型碰到的问题还常见。下面讲讲 PyTorch 的调试方法，来帮助定位“训练不稳定”的真正原因。</p>
<h3 id="6-1-调试核心：训练是一个信号流动过程"><a href="#6-1-调试核心：训练是一个信号流动过程" class="headerlink" title="6.1 调试核心：训练是一个信号流动过程"></a>6.1 调试核心：训练是一个信号流动过程</h3><p>训练的本质是：<strong>前向是数据的流动，反向是梯度的流动。</strong> 模型能否学习，核心是要看梯度是否能顺畅地流通。因此，调试的核心不是“loss 多少”，而是：</p>
<ul>
<li>梯度是否在每一层都有合理的值？</li>
<li>数据是否有异常（极值 &#x2F; 常数 &#x2F; 错位）？</li>
<li>哪一层截断了梯度？</li>
<li>是否存在不稳定数值（Inf &#x2F; NaN）？</li>
<li>模型参数是否被冻结？</li>
</ul>
<p>我们可以用一个简化示意图来理解训练信号流：</p>
<pre class="mermaid">flowchart LR
    Input --> F1[Layer 1]
    F1 --> F2[Layer 2]
    F2 --> F3[Layer 3]
    F3 --> Loss

    Loss -.-> B3[Grad 3] -.-> F3
    B3 -.-> B2[Grad 2] -.-> F2
    B2 -.-> B1[Grad 1] -.-> F1
    B1 -.-> Input</pre>
<center>图 6-1 训练的“信号流”模型：前向数据流（实线）与反向梯度流（虚线）</center>

<p>从左到右是前向数据流 (forward)，从右到左是反向梯度流 (backward)。如果某一处发生断裂，就会影响整条链路。整个训练是前向 + 反向 的循环</p>
<h3 id="6-2-第一类问题：梯度异常（0、巨大、NaN）"><a href="#6-2-第一类问题：梯度异常（0、巨大、NaN）" class="headerlink" title="6.2 第一类问题：梯度异常（0、巨大、NaN）"></a>6.2 第一类问题：梯度异常（0、巨大、NaN）</h3><p>最常见、也最致命的问题来自“梯度异常”。我们可以通过一个通用监控工具来检测：监控梯度范数（Gradient Norm）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_norm</span>(<span class="params">model</span>):</span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            total += p.grad.data.norm().item()</span><br><span class="line">    <span class="keyword">return</span> total</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环中调用：</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Grad Norm:&quot;</span>, grad_norm(model))</span><br></pre></td></tr></table></figure>
<p><strong>如何判断梯度异常？</strong></p>
<table>
<thead>
<tr>
<th><strong>异常类型</strong></th>
<th><strong>症状</strong></th>
<th><strong>原因</strong></th>
</tr>
</thead>
<tbody><tr>
<td>梯度为 <strong>0</strong></td>
<td>模型不更新、loss 不下降</td>
<td>激活函数饱和、图断裂、忘记 requires_grad</td>
</tr>
<tr>
<td>梯度 <strong>爆炸</strong></td>
<td>loss 变成 Inf &#x2F; NaN</td>
<td>学习率太大、指数增长模块、RNN 不稳定</td>
</tr>
<tr>
<td>梯度中有 <strong>NaN</strong></td>
<td>loss&#x3D;NaN &#x2F; backward 崩溃</td>
<td>0 除、log(0)、无效操作、数值上溢</td>
</tr>
</tbody></table>
<p>特别是如果梯度突然爆炸，基本一定是数值不稳定，梯度一直是 0 → 90% 是计算图断了。</p>
<h3 id="6-3-第二类问题：loss-NaN-Inf（数值不稳定）"><a href="#6-3-第二类问题：loss-NaN-Inf（数值不稳定）" class="headerlink" title="6.3 第二类问题：loss &#x3D; NaN &#x2F; Inf（数值不稳定）"></a>6.3 第二类问题：loss &#x3D; NaN &#x2F; Inf（数值不稳定）</h3><p>AI 工程中最常见的问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Epoch 12: loss = nan</span><br></pre></td></tr></table></figure>
<p>数值不稳定通常来自触发无效数学运算</p>
<table>
<thead>
<tr>
<th><strong>操作</strong></th>
<th><strong>问题点</strong></th>
<th><strong>示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>log(0)</td>
<td>结果为 -inf</td>
<td>F.log_softmax 一般会做保护</td>
</tr>
<tr>
<td>除以 0</td>
<td>inf</td>
<td>x &#x2F; eps 要写成 +1e-8</td>
</tr>
<tr>
<td>指数 e^x</td>
<td>很容易溢出</td>
<td>attention score</td>
</tr>
<tr>
<td>平方和累加</td>
<td>大数堆叠</td>
<td><code>((x**2).sum())**2</code></td>
</tr>
</tbody></table>
<ul>
<li>Softmax&#x2F;Logits 数值溢出</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误写法</span></span><br><span class="line">prob = torch.exp(x) / torch.exp(x).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 正确写法</span></span><br><span class="line">prob = F.softmax(x, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>没有使用 CrossEntropyLoss</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不这么写</span></span><br><span class="line">prob = F.softmax(logits)</span><br><span class="line">loss = -(y * prob.log()).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 一般用CrossEntropyLoss做数值稳定处理</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(logits, target)</span><br></pre></td></tr></table></figure>
<h3 id="6-4-第三类问题：模型参数“没有更新”"><a href="#6-4-第三类问题：模型参数“没有更新”" class="headerlink" title="6.4 第三类问题：模型参数“没有更新”"></a>6.4 第三类问题：模型参数“没有更新”</h3><p>常见原因如下：</p>
<ul>
<li>忘记设置 requires_grad</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认不追踪梯度</span></span><br><span class="line">x = torch.randn(<span class="number">10</span>) </span><br><span class="line"><span class="comment"># 应该是这样</span></span><br><span class="line">x = torch.randn(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)     </span><br></pre></td></tr></table></figure>
<ul>
<li>在 forward 中创建层<br>  这样会导致Optimizer 无法管理以及参数不更新。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    fc = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)  <span class="comment"># 每次 forward 又创建新参数</span></span><br><span class="line">    <span class="keyword">return</span> fc(x)</span><br></pre></td></tr></table></figure>
<ul>
<li>detach() 用错位置</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="variable language_">self</span>.encoder(x).detach()  <span class="comment"># 移除了梯度</span></span><br></pre></td></tr></table></figure>
<ul>
<li>in-place 破坏计算图</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.relu_()  <span class="comment"># 原地覆盖</span></span><br></pre></td></tr></table></figure>
<h3 id="6-5-第四类问题：计算图断裂（Graph-Break）"><a href="#6-5-第四类问题：计算图断裂（Graph-Break）" class="headerlink" title="6.5 第四类问题：计算图断裂（Graph Break）"></a>6.5 第四类问题：计算图断裂（Graph Break）</h3><p>图断裂会导致梯度不再回传，症状是某几层梯度一直为0，或者loss 不下降。常见原因有以下几点：</p>
<ul>
<li>Tensor 转 numpy</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 彻底断裂，梯度丢失</span></span><br><span class="line">x = x.numpy()   </span><br><span class="line"><span class="comment"># 明确表示“不需要梯度”</span></span><br><span class="line">x = x.detach().cpu().numpy()   </span><br></pre></td></tr></table></figure>
<ul>
<li>Tensor.item() 参与运算<br>  float 不在计算图中，梯度链路断裂。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x * x.item()   <span class="comment"># item() 返回 Python float</span></span><br></pre></td></tr></table></figure>
<ul>
<li>detach()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x.detach()</span><br><span class="line">z = y * <span class="number">2</span>        <span class="comment"># 不会回传给 x</span></span><br></pre></td></tr></table></figure>
<h3 id="6-6-第五类问题：backward-报错（定位算子）"><a href="#6-6-第五类问题：backward-报错（定位算子）" class="headerlink" title="6.6 第五类问题：backward 报错（定位算子）"></a>6.6 第五类问题：backward 报错（定位算子）</h3><p>PyTorch 提供的调试工具：</p>
<ul>
<li>autograd.set_detect_anomaly(True)<br>  当 backward 出错时它会打印出 <strong>forward</strong> 中的问题算子，显示具体行号，说明哪个节点梯度计算失败。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.set_detect_anomaly(<span class="literal">True</span>)</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<h3 id="6-7-使用-Hook-调试：捕获任意层的输入-输出-梯度"><a href="#6-7-使用-Hook-调试：捕获任意层的输入-输出-梯度" class="headerlink" title="6.7 使用 Hook 调试：捕获任意层的输入&#x2F;输出&#x2F;梯度"></a>6.7 使用 Hook 调试：捕获任意层的输入&#x2F;输出&#x2F;梯度</h3><p>Hook 是 PyTorch 中一个低调但极其强大的功能。</p>
<ul>
<li>注册梯度 hook<br>  可用于查看，某层梯度是否消失，数值是否溢出，或者是否为 NaN</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_grad</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Grad:&quot;</span>, grad.norm())</span><br><span class="line"></span><br><span class="line">x.register_hook(print_grad)</span><br></pre></td></tr></table></figure>
<ul>
<li>注册 forward hook<br>  一般用于检测：是否某层输出全部是 0、是否某层输出巨大（引发爆炸）、是否某层被误触发 in-place 改写</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fwd_hook</span>(<span class="params">module, inp, out</span>):</span><br><span class="line">    <span class="built_in">print</span>(module.__class__.__name__, out.mean().item())</span><br><span class="line"></span><br><span class="line">layer.register_forward_hook(fwd_hook)</span><br></pre></td></tr></table></figure>
<h3 id="6-8-数据问题：最容易被忽略的训练失败原因"><a href="#6-8-数据问题：最容易被忽略的训练失败原因" class="headerlink" title="6.8 数据问题：最容易被忽略的训练失败原因"></a>6.8 数据问题：最容易被忽略的训练失败原因</h3><p>常见数据异常：</p>
<ul>
<li>数据全是 0&#x2F;255<br> 图像输入未归一化 → 模型无法训练：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transforms.Normalize(mean, std)</span><br></pre></td></tr></table></figure>
<ul>
<li>标签错位<br>  train_loader 输出 (data, label)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> label, data <span class="keyword">in</span> train_loader:</span><br><span class="line">   …</span><br></pre></td></tr></table></figure>
<ul>
<li>Batch Size 太小<br>  BN（BatchNorm）在 batch_size 太小时会不稳定，导致发散。</li>
<li>shuffle&#x3D;False<br>  模型学习不到统计多样性，可能出现奇怪收敛轨迹。</li>
</ul>
<p><strong>小结：PyTorch 调试的系统方法</strong><br>通过本章内容，我们建立了一套系统化的 PyTorch 调试思维。首先，从梯度出发，可以通过观察 grad_norm 来判断整体梯度是否健康，也可以借助 hook 精确查看某一层的梯度流动情况。在此基础上，需要特别警惕 requires_grad、detach 的误用，以及任何可能导致计算图断裂的操作，这是调试梯度问题的核心。</p>
<p>在数值稳定性方面，需要避免出现 log(0)、除以零、指数运算溢出等典型错误，并确保 softmax 与 crossentropy 的使用方式正确，同时对 exp、平方、范数累积等操作保持谨慎，以防止出现数值爆炸。</p>
<p>当模型出现“参数不更新”的情况时，应从结构与逻辑两方面排查：例如是否在 forward 中意外创建了新的层，是否错误冻结了参数，是否因为 in-place 操作覆盖了中间值，或是否因 detach 阶段性切断了梯度。这些问题往往直接影响模型能否成功学习。</p>
<p>如果 backward 阶段出现异常，则可以借助 anomaly 工具捕获具体的算子错误，也可以通过逐层 hook 逐步缩小排查范围，最终找到导致求导失败的节点。</p>
<p>调试过程中还必须关注数据本身是否正常，包括输入的归一化是否正确、标签是否错位、batch 是否过小导致统计不稳定，以及 shuffle 是否在训练阶段被正确启用。数据问题往往是训练异常的根源之一。</p>
<h2 id="7-性能：从-Eager-执行到编译加速"><a href="#7-性能：从-Eager-执行到编译加速" class="headerlink" title="7. 性能：从 Eager 执行到编译加速"></a>7. 性能：从 Eager 执行到编译加速</h2><p>PyTorch 之所以深受研究者和工程师喜爱，来源于其“动态计算图 + Pythonic API”带来的自由度。虽然Eager（即时执行）模式容易调试，却缺乏静态结构，难以进行深度优化。</p>
<p>PyTorch 2.0 之后，性能有了一个提升：<strong>在保持 Eager 的灵活性，同时获得接近静态图框架的性能。</strong> 下面我们将从 Eager → TorchScript → torch.compile → AMP → Profiling为主线，来理解PyTorch究竟是如何提升性能的。</p>
<h3 id="7-1-训练是如何执行的？Eager-模式的本质"><a href="#7-1-训练是如何执行的？Eager-模式的本质" class="headerlink" title="7.1 训练是如何执行的？Eager 模式的本质"></a>7.1 训练是如何执行的？Eager 模式的本质</h3><p>Eager Execution（即时执行）是 PyTorch 的核心，在 Eager 中，这段代码每一步都立即执行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># @：matmul 调用</span></span><br><span class="line"><span class="comment"># +：add 调用</span></span><br><span class="line"><span class="comment"># relu：activation 调用</span></span><br><span class="line">y = torch.relu(x @ w + b)</span><br><span class="line">```  </span><br><span class="line">每个算子都需要 Python 调度 -&gt; C++ -&gt; CUDA kernel 上下文切换。这样虽然灵活，但有两个天然瓶颈：</span><br><span class="line"><span class="number">1.</span> 无法提前优化计算图，算子已经在执行，没有静态结构可以整体优化。</span><br><span class="line"><span class="number">2.</span> Python 调度开销大，每一个算子都是一次独立 Python 调用。</span><br><span class="line"></span><br><span class="line">所以 Eager 的优点是：极易调试、控制流友好、与 Python 一致。缺点是速度不如静态图、算子融合有限、Kernel 数量多还会带来串行 overhead，**这就是“灵活 vs 性能”的根本矛盾。**</span><br><span class="line"><span class="comment">### 7.2 TorchScript：第一次尝试让 PyTorch 拥有静态图</span></span><br><span class="line">在 PyTorch <span class="number">1.</span>x 时代，官方曾尝试通过 TorchScript 提供静态图式加速。TorchScript 有两种方式：</span><br><span class="line">- Tracing（跟踪法），优点是简单但缺点是无法处理控制流（<span class="keyword">if</span>/<span class="keyword">for</span>）。</span><br><span class="line">```python</span><br><span class="line">	traced = torch.jit.trace(model, example_input)</span><br></pre></td></tr></table></figure>
<ul>
<li>Scripting（脚本化），优点是支持控制流，但缺点语法限制多、理解难度大、难调试。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scripted = torch.jit.script(model)</span><br></pre></td></tr></table></figure>
<p>TorchScript 最大的问题是： <strong>用户必须迁就框架，而不是框架适应用户。</strong> 因此 TorchScript 更多用于“部署”，而非“提升训练性能”。</p>
<h3 id="7-3-PyTorch-2-0：torch-compile-的出现"><a href="#7-3-PyTorch-2-0：torch-compile-的出现" class="headerlink" title="7.3 PyTorch 2.0：torch.compile 的出现"></a>7.3 PyTorch 2.0：torch.compile 的出现</h3><p>从 PyTorch 2.0 开始，性能有了一个很大的提升：<strong>不改模型代码，通过编译自动获得加速。</strong> 启用方式很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.<span class="built_in">compile</span>(model)</span><br></pre></td></tr></table></figure>
<p>这样就可以直接训练，不需要改变任何 forward 或训练脚本。这是 PyTorch 性能策略的终极目标：</p>
<ul>
<li>让用户写完全“Eager 风格”的代码</li>
<li>PyTorch 在背后捕获计算图并优化</li>
<li>用户获得 <strong>2×–3× 实测加速</strong></li>
</ul>
<h3 id="7-4-torch-compile-的三段式架构"><a href="#7-4-torch-compile-的三段式架构" class="headerlink" title="7.4 torch.compile 的三段式架构"></a>7.4 torch.compile 的三段式架构</h3><p>PyTorch 2.0 的编译体系由三个关键模块组成：</p>
<ol>
<li><p>TorchDynamo：捕获 Python 层下面的计算图<br> Dynamo 会拦截 Python 执行，将每次 tensor 运算捕获为 FX Graph（中间表示）。它能处理：</p>
<ul>
<li>if &#x2F; for &#x2F; while 等控制流</li>
<li>动态形状（dynamic shapes）</li>
<li>Python side effect</li>
</ul>
<p> 这是 TorchScript 无法做到的。</p>
</li>
<li><p>AOTAutograd：提前构建反向图<br> 它会生成前向和反向的静态图，便于做算子融合与优化。相当于：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward_graph, backward_graph = AOTAutograd(fx_graph)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><p>Inductor：生成高性能 CUDA&#x2F;CPU Kernel。Inductor 会：</p>
<ul>
<li>fuse（融合）多个算子</li>
<li>减少 kernel launch 数量</li>
<li>用 Triton 自动生成 CUDA Kernel</li>
<li>减少 memory access</li>
<li>提升吞吐率</li>
</ul>
<p> 这部分性能提升非常可观。</p>
</li>
</ol>
<h3 id="7-5-混合精度训练（AMP）：Tensor-Core-更少显存-更高速"><a href="#7-5-混合精度训练（AMP）：Tensor-Core-更少显存-更高速" class="headerlink" title="7.5 混合精度训练（AMP）：Tensor Core + 更少显存 + 更高速"></a>7.5 混合精度训练（AMP）：Tensor Core + 更少显存 + 更高速</h3><p>混合精度训练（Automatic Mixed Precision）已经成为现代训练的“默认配置”。PyTorch 提供两类 AMP：</p>
<ul>
<li><strong>FP16 AMP</strong></li>
<li><strong>BF16 AMP</strong>（更稳定、推荐使用 A100&#x2F;H100）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line"></span><br><span class="line">    scaler.scale(loss).backward()</span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br></pre></td></tr></table></figure>
<p>AMP 的优势：</p>
<ul>
<li>使用 Tensor Core → 更高吞吐</li>
<li>FP16 权重减少显存占用</li>
<li>速度提升 <strong>1.3×–2×</strong></li>
<li>batch size 增大</li>
<li>大模型训练更稳定</li>
</ul>
<p>结合 torch.compile → 速度会进一步提升。</p>
<h3 id="7-6-Gradient-Accumulation：显存不足时的“大-batch-技巧”"><a href="#7-6-Gradient-Accumulation：显存不足时的“大-batch-技巧”" class="headerlink" title="7.6 Gradient Accumulation：显存不足时的“大 batch 技巧”"></a>7.6 Gradient Accumulation：显存不足时的“大 batch 技巧”</h3><p>如果显存不够训练大 batch，可以用梯度累积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">accum_steps = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        loss = model(x, y) / accum_steps</span><br><span class="line"></span><br><span class="line">    scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % accum_steps == <span class="number">0</span>:</span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line">        scaler.update()</span><br><span class="line">        optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>你用 4 次 batch size&#x3D;32 的 forward，就能模拟：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">effective batch = 32 × 4 = 128</span><br></pre></td></tr></table></figure>
<p>这在小显存设备（如 8GB GPU）上非常实用。</p>
<h3 id="7-7-DataLoader-性能：减少-Python-overhead，提升吞吐"><a href="#7-7-DataLoader-性能：减少-Python-overhead，提升吞吐" class="headerlink" title="7.7 DataLoader 性能：减少 Python overhead，提升吞吐"></a>7.7 DataLoader 性能：减少 Python overhead，提升吞吐</h3><p>DataLoader 性能影响整个训练速度的 20%–50%。最关键参数：</p>
<ul>
<li><strong>num_workers</strong>：数据加载并行度</li>
<li><strong>pin_memory&#x3D;True</strong>：加速 H2D 传输</li>
<li><strong>prefetch_factor</strong>：更高的预加载吞吐</li>
<li><strong>persistent_workers&#x3D;True</strong>：减少 worker 重建开销</li>
</ul>
<p>推荐配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">64</span>, num_workers=<span class="number">4</span>, pin_memory=<span class="literal">True</span>, persistent_workers=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="7-8-Profiling：性能优化不再靠猜测"><a href="#7-8-Profiling：性能优化不再靠猜测" class="headerlink" title="7.8 Profiling：性能优化不再靠猜测"></a>7.8 Profiling：性能优化不再靠猜测</h3><p>PyTorch Profiler 能详细显示：</p>
<ul>
<li>哪些算子最慢</li>
<li>DataLoader 是否成为瓶颈</li>
<li>CUDA kernel 启动频率</li>
<li>CPU&#x2F;GPU 并行度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.profiler <span class="keyword">as</span> profiler</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> profiler.profile(</span><br><span class="line">    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],</span><br><span class="line">    record_shapes=<span class="literal">True</span></span><br><span class="line">) <span class="keyword">as</span> prof:</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(prof.key_averages().table(sort_by=<span class="string">&quot;cuda_time_total&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>所有性能优化都应该先 profile，再决策。</p>
<h3 id="7-9-性能优化-Checklist"><a href="#7-9-性能优化-Checklist" class="headerlink" title="7.9 性能优化 Checklist"></a>7.9 性能优化 Checklist</h3><p>在模型加速方面，可以优先启用 torch.compile 来减少 Python 调度开销，并使用自动混合精度（AMP）获得显著的吞吐提升。同时，应尽量减少 Python 层面的条件分支，避免反复创建新的 Tensor，以减少构图与调度的额外成本。此外，除非必要，不应依赖 inplace 操作，以免破坏 autograd 或导致不必要的重新构图。</p>
<p>在数据吞吐方面，DataLoader 的并行加载至关重要。合理设置 num_workers 可以明显提升数据准备速度，pin_memory&#x3D;True 则有助于加快 CPU 到 GPU 的数据传输。如果显存充足，可以采用更大的 batch；显存有限时，则可以通过梯度累积来模拟大批量训练。同时，分布式训练（DDP）能让吞吐进一步扩大，适用于多卡甚至多机环境。</p>
<p>在显存优化上，AMP 能减少 30%–50% 的显存压力，是最直接有效的手段。对于更大模型，可以使用 Gradient Checkpointing 以计算换显存；必要时还可以从 Adam 切换为占用更低的 SGD。此外，清理不必要的缓存、减少多余的中间激活，也有助于提升整体可训练规模。</p>
<p>在数值稳定性方面，GradScaler 是混合精度训练中保证稳定性的关键工具，应默认开启。对于 softmax 或 logits 等关键算子，尽量避免直接使用 FP16，以降低数值溢出风险。如果硬件支持（如 A100&#x2F;H100），BF16 通常是兼具性能和稳定性的更佳选择。</p>
<p><strong>小结：PyTorch 性能进入“编译时代”</strong><br>本章构建了一套系统化的性能优化思路。我们首先理解了 Eager 模式的优势与瓶颈：它极其灵活，但在大规模训练中难以避免 Python 调度带来的开销。随后回顾了 TorchScript 作为早期静态图方案的局限性，也正因为这些限制，PyTorch 2.0 才通过 torch.compile 真正实现了“动态图与高性能”兼得的目标，让模型在保持原有写法的前提下获得编译器级别的加速。</p>
<p>在此基础上，我们进一步引入 AMP，使训练在吞吐、显存和数值稳定性之间获得最佳平衡；同时讨论了梯度累积与 Gradient Checkpointing，在大模型训练中，它们是突破显存瓶颈的关键技术。数据加载侧的优化同样重要，通过合理的 DataLoader 设置，我们能够解决输入阶段的性能瓶颈，让 GPU 不再等待数据。</p>
<p>在高性能训练中，Profiler 是定位瓶颈不可或缺的工具，它让优化不再依靠猜测，而能基于可视化证据进行决策。最后，通过一份性能 Checklist，我们将所有策略系统化、可执行化，使训练性能能够被清晰管理。</p>
<p>总结一句：<strong>PyTorch 的性能优化，正在从手工技巧转向编译器驱动的系统智能。</strong></p>
<h2 id="8-工程生态：从实验到部署的优雅闭环"><a href="#8-工程生态：从实验到部署的优雅闭环" class="headerlink" title="8. 工程生态：从实验到部署的优雅闭环"></a>8. 工程生态：从实验到部署的优雅闭环</h2><p>深度学习项目远不止“训练一个模型”那么简单。在真实工程中，一个模型往往需要经历：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">训练 → 评估 → 记录 → 管理 → 导出 → 部署 → 监控</span><br></pre></td></tr></table></figure>
<p>这是一条“完整生命周期（ML Lifecycle）”，PyTorch 的生态正是围绕它构建的。</p>
<p>本章目标是<strong>构建训练端与工程端的桥梁，让 PyTorch 代码真正进入“可复现、可共享、可部署、可维护”的工程状态。</strong></p>
<h3 id="8-1-模型保存与加载：从-state-dict-开始的工程基石"><a href="#8-1-模型保存与加载：从-state-dict-开始的工程基石" class="headerlink" title="8.1 模型保存与加载：从 state_dict 开始的工程基石"></a>8.1 模型保存与加载：从 state_dict 开始的工程基石</h3><p>PyTorch 的核心设计思想之一：<strong>模型 &#x3D; 代码结构（类） + 权重（state_dict）</strong> 。这比 TensorFlow 的“冻结图”更加灵活。</p>
<h4 id="8-1-1-state-dict：官方推荐、最安全、最灵活的保存方式"><a href="#8-1-1-state-dict：官方推荐、最安全、最灵活的保存方式" class="headerlink" title="8.1.1 state_dict：官方推荐、最安全、最灵活的保存方式"></a>8.1.1 state_dict：官方推荐、最安全、最灵活的保存方式</h4><p>保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;model.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = MyModel()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;model.pth&quot;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>这种方式的好处：</p>
<ul>
<li>只保存权重，不保存代码逻辑</li>
<li>不依赖具体 Python 环境</li>
<li>不受 pickle 安全问题影响</li>
<li>易于迁移学习、fine-tune</li>
<li>文件体积更小</li>
</ul>
<p>很多初学者犯的错误是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, <span class="string">&quot;model.pth&quot;</span>)  <span class="comment"># 强依赖当前代码环境</span></span><br></pre></td></tr></table></figure>
<p>这种方式不推荐，因为：</p>
<ul>
<li>需要模型类在加载环境中同名存在</li>
<li>pickle 安全风险</li>
<li>环境变化时容易失效</li>
</ul>
<p><strong>工程实践：推荐保存 state_dict。</strong></p>
<h4 id="8-1-2-训练断点：保存优化器状态（必要时）"><a href="#8-1-2-训练断点：保存优化器状态（必要时）" class="headerlink" title="8.1.2 训练断点：保存优化器状态（必要时）"></a>8.1.2 训练断点：保存优化器状态（必要时）</h4><p>训练中断？不想重来？只需保存 optimizer 状态：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&quot;epoch&quot;</span>: epoch,</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict()</span><br><span class="line">&#125;, <span class="string">&quot;ckpt.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ckpt = torch.load(<span class="string">&quot;ckpt.pth&quot;</span>)</span><br><span class="line">model.load_state_dict(ckpt[<span class="string">&quot;model&quot;</span>])</span><br><span class="line">optimizer.load_state_dict(ckpt[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">start_epoch = ckpt[<span class="string">&quot;epoch&quot;</span>] + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>适用于：</p>
<ul>
<li>大模型训练</li>
<li>昂贵训练任务</li>
<li>云上训练（spot 实例随时中断）</li>
</ul>
<h3 id="8-2-PyTorch-Lightning：让训练循环从“脚本”变成“系统”"><a href="#8-2-PyTorch-Lightning：让训练循环从“脚本”变成“系统”" class="headerlink" title="8.2 PyTorch Lightning：让训练循环从“脚本”变成“系统”"></a>8.2 PyTorch Lightning：让训练循环从“脚本”变成“系统”</h3><p>原生 PyTorch 灵活，但工程端常见问题：</p>
<ul>
<li>训练循环重复代码多</li>
<li>逻辑（模型）与工程（训练）混杂</li>
<li>hyper-parameter 到处漂移</li>
<li>多 GPU 分布式写起来复杂</li>
<li>日志、回调、checkpoint 杂乱</li>
</ul>
<p>Lightning 的理念：<strong>把科学部分（模型）与工程部分（训练逻辑）分离。</strong></p>
<h4 id="8-2-1-LightningModule：模型定义更清晰"><a href="#8-2-1-LightningModule：模型定义更清晰" class="headerlink" title="8.2.1 LightningModule：模型定义更清晰"></a>8.2.1 LightningModule：模型定义更清晰</h4><p>一个 Lightning 模型只负责：</p>
<ul>
<li>forward</li>
<li>loss</li>
<li>backward（Lightning 管）</li>
<li>optimizer</li>
<li>metrics</li>
</ul>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> L</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LitMLP</span>(L.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = SimpleMLP(<span class="number">784</span>, <span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="variable language_">self</span>.loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        x, y = batch</span><br><span class="line">        pred = <span class="variable language_">self</span>(x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>))</span><br><span class="line">        loss = <span class="variable language_">self</span>.loss_fn(pred, y)</span><br><span class="line">        <span class="variable language_">self</span>.log(<span class="string">&quot;train_loss&quot;</span>, loss)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.optim.Adam(<span class="variable language_">self</span>.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<p>训练只需：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = L.Trainer(max_epochs=<span class="number">5</span>, accelerator=<span class="string">&quot;gpu&quot;</span>, devices=<span class="number">1</span>)</span><br><span class="line">trainer.fit(model, train_loader, val_loader)</span><br></pre></td></tr></table></figure>
<h4 id="8-2-2-Lightning-自动处理的工程事务"><a href="#8-2-2-Lightning-自动处理的工程事务" class="headerlink" title="8.2.2 Lightning 自动处理的工程事务"></a>8.2.2 Lightning 自动处理的工程事务</h4><p>Lightning 可以替我们做：</p>
<ul>
<li>混合精度 AMP</li>
<li>GPU &#x2F; 多机多卡</li>
<li>梯度累积</li>
<li>梯度裁剪</li>
<li>checkpoint 自动保存</li>
<li>可重启训练</li>
<li>回调机制</li>
<li>日志集成（TensorBoard &#x2F; W&amp;B）</li>
<li>训练 loop 管理</li>
</ul>
<p>写的代码变少了，但工程质量更高、更专业。</p>
<h3 id="8-3-Hydra：真正可复现的配置管理系统"><a href="#8-3-Hydra：真正可复现的配置管理系统" class="headerlink" title="8.3 Hydra：真正可复现的配置管理系统"></a>8.3 Hydra：真正可复现的配置管理系统</h3><p>深度学习项目常见问题：</p>
<ul>
<li>超参数散落在代码、命令行、config 文件</li>
<li>复制一份 config 就乱套</li>
<li>实验版本无法复现</li>
<li>尝试不同参数组合时必须改 Python 代码</li>
</ul>
<p>Hydra 的理念：<strong>让配置成为“一等公民”。</strong></p>
<h4 id="8-3-1-典型-Hydra-配置结构"><a href="#8-3-1-典型-Hydra-配置结构" class="headerlink" title="8.3.1 典型 Hydra 配置结构"></a>8.3.1 典型 Hydra 配置结构</h4><p>configs&#x2F;train.yaml：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">hidden_dim:</span> <span class="number">256</span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">64</span></span><br><span class="line">  <span class="attr">num_workers:</span> <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@hydra.main(<span class="params">config_path=<span class="string">&quot;configs&quot;</span>, config_name=<span class="string">&quot;train&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">cfg</span>):</span><br><span class="line">    model = SimpleMLP(<span class="number">784</span>, cfg.model.hidden_dim, <span class="number">10</span>)</span><br><span class="line">    loader = build_dataloader(cfg.data.batch_size)</span><br></pre></td></tr></table></figure>
<p>想修改配置？</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run.py model.hidden_dim=512 data.batch_size=128</span><br></pre></td></tr></table></figure>
<p>不用改代码，不用复制 config，完全可复现。</p>
<h3 id="8-4-实验追踪：让科研从“文件系统”进入“系统平台”"><a href="#8-4-实验追踪：让科研从“文件系统”进入“系统平台”" class="headerlink" title="8.4 实验追踪：让科研从“文件系统”进入“系统平台”"></a>8.4 实验追踪：让科研从“文件系统”进入“系统平台”</h3><p>没有实验追踪的训练过程是不可控的。我们可能会遇到：</p>
<ul>
<li>哪次实验最好？</li>
<li>上次的学习率是多少？</li>
<li>batch size 用的多少？</li>
<li>最佳 checkpoint 在哪？</li>
<li>团队成员怎么共享结果？</li>
</ul>
<p>现代训练强烈建议使用：</p>
<ul>
<li><strong>Weights &amp; Biases（W&amp;B）</strong></li>
<li><strong>MLflow</strong></li>
<li><strong>TensorBoard（轻量）</strong></li>
</ul>
<h4 id="8-4-1-W-B-最小示例"><a href="#8-4-1-W-B-最小示例" class="headerlink" title="8.4.1 W&amp;B 最小示例"></a>8.4.1 W&amp;B 最小示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">wandb.init(project=<span class="string">&quot;mnist-demo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    loss = train_step(x, y)</span><br><span class="line">    wandb.log(&#123;<span class="string">&quot;loss&quot;</span>: loss&#125;)</span><br></pre></td></tr></table></figure>
<p>Lightning 中更简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> WandbLogger</span><br><span class="line"></span><br><span class="line">logger = WandbLogger(project=<span class="string">&quot;mnist-demo&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer = L.Trainer(logger=logger)</span><br><span class="line">trainer.fit(model, train_loader)</span><br></pre></td></tr></table></figure>
<p>W&amp;B 自动记录loss&#x2F;acc 曲线、模型权重、配置参数、GPU&#x2F;CPU 资源、模型对比、自动可视化。W&amp;B 是目前最适合科研与中大规模工程团队的实验平台。</p>
<h3 id="8-5-模型导出：从-PyTorch-到跨平台推理"><a href="#8-5-模型导出：从-PyTorch-到跨平台推理" class="headerlink" title="8.5 模型导出：从 PyTorch 到跨平台推理"></a>8.5 模型导出：从 PyTorch 到跨平台推理</h3><p>训练好模型，只是旅程的 <strong>一半</strong>。接下来我们需要：</p>
<ul>
<li>在 web 推理？</li>
<li>在 TensorRT 上优化？</li>
<li>在手机端运行？</li>
<li>在多语言环境（C++&#x2F;Java）调用？</li>
<li>与其他框架集成？</li>
</ul>
<p>PyTorch 提供了两条路线：</p>
<h4 id="8-5-1-ONNX：工业界最通用的深度学习交换格式"><a href="#8-5-1-ONNX：工业界最通用的深度学习交换格式" class="headerlink" title="8.5.1 ONNX：工业界最通用的深度学习交换格式"></a>8.5.1 ONNX：工业界最通用的深度学习交换格式</h4><p>导出模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.onnx.export(model, example_input, <span class="string">&quot;model.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ONNX 可用于：</p>
<ul>
<li>推理引擎（TensorRT、ONNX Runtime）</li>
<li>移动端</li>
<li>C++ 推理</li>
<li>云端服务</li>
</ul>
<p>ONNX 会将模型转换成静态图格式，便于进一步优化。</p>
<h4 id="8-5-2-TorchScript（部署侧）"><a href="#8-5-2-TorchScript（部署侧）" class="headerlink" title="8.5.2 TorchScript（部署侧）"></a>8.5.2 TorchScript（部署侧）</h4><p>尽管 TorchScript 在训练中热度降低，但在部署侧仍然非常有价值：</p>
<ul>
<li>C++ LibTorch 推理</li>
<li>移动端</li>
<li>无 Python 的推理环境</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scripted = torch.jit.script(model)</span><br><span class="line">scripted.save(<span class="string">&quot;model.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="8-5-3-TensorRT：NVIDIA-GPU-的终极加速方式"><a href="#8-5-3-TensorRT：NVIDIA-GPU-的终极加速方式" class="headerlink" title="8.5.3 TensorRT：NVIDIA GPU 的终极加速方式"></a>8.5.3 TensorRT：NVIDIA GPU 的终极加速方式</h4><p>典型流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyTorch model → ONNX → TensorRT Engine → 高性能 GPU 推理</span><br></pre></td></tr></table></figure>
<p>TensorRT 能带来：</p>
<ul>
<li>2×–10× 推理加速</li>
<li>FP16 &#x2F; INT8 量化</li>
<li>Kernel 融合</li>
</ul>
<p>适用于大模型部署、推理服务。</p>
<h3 id="8-6-TorchServe：生产级-PyTorch-推理服务"><a href="#8-6-TorchServe：生产级-PyTorch-推理服务" class="headerlink" title="8.6 TorchServe：生产级 PyTorch 推理服务"></a>8.6 TorchServe：生产级 PyTorch 推理服务</h3><p>如果我们想将模型部署为 API 服务：</p>
<ul>
<li>高并发</li>
<li>多模型多版本</li>
<li>自动 batch</li>
<li>GPU 高效利用</li>
</ul>
<p>那么官方给你的答案是：<strong>TorchServe（正式生产级）</strong><br>目录结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_store/</span><br><span class="line">    mnist.mar      ← 打包模型</span><br><span class="line">config/</span><br><span class="line">    config.properties</span><br><span class="line">serve.py</span><br></pre></td></tr></table></figure>
<p>基本流程：</p>
<ol>
<li><strong>将模型打包为 .mar 文件</strong></li>
<li><strong>通过 torchserve –start 启动服务</strong></li>
<li><strong>通过 REST API 调用预测接口</strong></li>
</ol>
<p>TorchServe 是深度学习服务化的工业级方案。</p>
<p><strong>工程闭环总结：PyTorch 的真正力量不只在训练</strong><br>在这一整套工程流程中，我们已经从实验走向部署，构建起一个真正可落地的深度学习管线。PyTorch 的工程生态可分为几个关键层面：在训练阶段，state_dict 提供轻量而透明的权重管理方式，Lightning 让训练结构化并稳定可控，而 AMP、torch.compile 与 DDP 则构成了现代训练不可或缺的性能体系；在配置与复现方面，Hydra 负责统一且可组合的配置管理，W&amp;B 与 MLflow 则提供系统化的实验追踪，使每一个实验都能被记录、对比与复现；在部署阶段，ONNX 实现跨平台兼容，TorchScript 支持 C++ 与移动端运行，TensorRT 在 GPU 上提供极致加速，而 TorchServe 则让模型真正运行在高并发的生产环境中。</p>
<p>一句话总结：<strong>PyTorch 并不是一个“训练框架”，而是一套完整的深度学习工程生态系统。</strong></p>
<h2 id="9-实战整合：从模型训练到实验追踪的完整项目"><a href="#9-实战整合：从模型训练到实验追踪的完整项目" class="headerlink" title="9. 实战整合：从模型训练到实验追踪的完整项目"></a>9. 实战整合：从模型训练到实验追踪的完整项目</h2><p>在前 1–8 章中，我们从最基础的张量与自动微分开始，逐步了解了模块化的模型构建方式、高性能数据管线、动态计算图的调试能力、混合精度与 torch.compile 加速策略，并进入工程化生态，包括 Lightning、Hydra 和日志系统。</p>
<p>这些能力都为本章做铺垫——<strong>将所有知识整合成一个真正可用、可复现、可扩展、可部署的深度学习工程项目。</strong></p>
<p>本章我们将学习如何构建一个专业级深度学习工程体系：</p>
<ul>
<li>清晰的项目结构</li>
<li>完整的数据管线</li>
<li>模块化模型定义</li>
<li>工程化训练流程（Lightning + AMP + compile）</li>
<li>实验可复现（Hydra 配置）</li>
<li>自动保存最佳模型</li>
<li>自动导出 TorchScript 部署模型（model.pt）</li>
<li>本地推理脚本</li>
<li>在线 API 推理（FastAPI）</li>
</ul>
<p>这也是项目中的完整链路，最终产物可以直接实际应用。</p>
<h3 id="9-1-为什么“完整项目”比“能训练模型”更重要？"><a href="#9-1-为什么“完整项目”比“能训练模型”更重要？" class="headerlink" title="9.1 为什么“完整项目”比“能训练模型”更重要？"></a>9.1 为什么“完整项目”比“能训练模型”更重要？</h3><p>许多初学者的训练代码往往是这样的，一个 800~2000 行的 train.py，模型、数据、训练循环、配置全部混合，改一点全盘崩溃，实验无法复现，无法部署，更别说团队协作。但在实际项目中，必须满足五个标准：</p>
<ol>
<li><strong>清晰结构（模块化）</strong> 模型、数据、训练、配置、部署解耦。</li>
<li><strong>可复现（Hydra 配置 + Logging）</strong> 所有超参数有记录，实验可重现。</li>
<li><strong>可扩展（更换模型&#x2F;数据不破坏现有代码）</strong></li>
<li><strong>可加速（AMP、compile、DDP、多进程 DataLoader）</strong></li>
<li><strong>可部署（ckpt → TorchScript &#x2F; ONNX → 服务）</strong></li>
</ol>
<h3 id="9-2-项目结构设计"><a href="#9-2-项目结构设计" class="headerlink" title="9.2 项目结构设计"></a>9.2 项目结构设计</h3><p>一个成熟的深度学习工程项目必须明确职责、形成模块化结构，本项目结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">project/</span><br><span class="line">│── configs/                     # Hydra 配置  </span><br><span class="line">│   └── config.yaml</span><br><span class="line">│</span><br><span class="line">│── src/</span><br><span class="line">│   ├── data.py                  # 数据管线</span><br><span class="line">│   ├── model.py                 # 模型定义</span><br><span class="line">│   ├── lit_module.py            # LightningModule </span><br><span class="line">│   ├── train.py                 # 构建 Trainer + Checkpoint</span><br><span class="line">│   ├── utils.py                 # 工具函数</span><br><span class="line">│   └── __init__.py</span><br><span class="line">│</span><br><span class="line">│── deploy/</span><br><span class="line">│   ├── export.py                # ckpt → model.pt / model.onnx</span><br><span class="line">│   ├── inference.py             # 本地推理脚本</span><br><span class="line">│   └── api.py                   # FastAPI 在线推理</span><br><span class="line">│</span><br><span class="line">│── run.py                       # 项目入口（训练 + 自动导出）</span><br><span class="line">│── requirements.txt</span><br><span class="line">│── README.md</span><br></pre></td></tr></table></figure>
<p><strong>这样架构就可以做到：</strong></p>
<ul>
<li>修改模型 → 只改 model.py</li>
<li>修改数据 → 只改 data.py</li>
<li>修改训练参数 → 只改配置 config.yaml</li>
<li>调用训练 → 只用 run.py</li>
<li>部署模型 → 只看 deploy&#x2F; 目录</li>
</ul>
<p>结构清晰、稳定、可扩展。</p>
<h3 id="9-3-数据管线：并行加载-数据增强-高吞吐"><a href="#9-3-数据管线：并行加载-数据增强-高吞吐" class="headerlink" title="9.3 数据管线：并行加载 + 数据增强 + 高吞吐"></a>9.3 数据管线：并行加载 + 数据增强 + 高吞吐</h3><p>CIFAR-10 数据管线模块负责：</p>
<ul>
<li>数据下载</li>
<li>数据增强（随机裁剪、水平翻转）</li>
<li>多进程加载</li>
<li>训练&#x2F;验证集区分</li>
</ul>
<p>设计目标：高吞吐量 + 工程可维护。</p>
<h3 id="9-4-模型模块化设计：SimpleCNN"><a href="#9-4-模型模块化设计：SimpleCNN" class="headerlink" title="9.4 模型模块化设计：SimpleCNN"></a>9.4 模型模块化设计：SimpleCNN</h3><p>模型定义模块 model.py 专注于一件事：</p>
<blockquote>
<p>“给定输入 x，定义如何得到输出 logits。”</p>
</blockquote>
<p>SimpleCNN 作为示例模型已足够清晰：Conv → ReLU → Conv → ReLU → Pool → FC → 分类。</p>
<p><strong>模型作用：</strong></p>
<ul>
<li>输入一张 32×32 RGB 图像</li>
<li>输出 10 类的概率（飞机&#x2F;汽车&#x2F;猫&#x2F;狗等 CIFAR10 类别）</li>
<li>是一个可替换组件（可以改成 ResNet、MobileNet、ViT）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于 CIFAR-10 的极简 CNN 网络。&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span> = <span class="number">10</span></span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 卷积特征提取模块：  </span></span><br><span class="line">        <span class="comment"># 输入：3x32x32  </span></span><br><span class="line">        <span class="comment"># 经过 Conv+ReLU+Conv+ReLU+MaxPool2d(2) 后，输出：64x16x16  </span></span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Sequential(  </span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),  <span class="comment"># 输出 32x32x32  </span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),  </span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 输出 64x32x32   </span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),  </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),                             <span class="comment"># 下采样一半：64x16x16  </span></span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 全连接分类层：  </span></span><br><span class="line">        <span class="comment"># 将 64x16x16 展平成一个向量，然后映射到 num_classes        </span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">64</span> * <span class="number">16</span> * <span class="number">16</span>, num_classes)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        <span class="comment"># x: [B, 3, 32, 32]  </span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)               <span class="comment"># [B, 64, 16, 16]  </span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)      <span class="comment"># [B, 64*16*16]  </span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)                 <span class="comment"># [B, num_classes]  </span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="9-5-Lightning-训练模块：训练逻辑从脚本中解放"><a href="#9-5-Lightning-训练模块：训练逻辑从脚本中解放" class="headerlink" title="9.5 Lightning 训练模块：训练逻辑从脚本中解放"></a>9.5 Lightning 训练模块：训练逻辑从脚本中解放</h3><p>在 lit_module.py 中，LightningModule 负责：</p>
<ul>
<li>前向传播</li>
<li>训练步骤（loss 计算）</li>
<li>验证步骤</li>
<li>记录指标</li>
<li>定义优化器</li>
</ul>
<p>Lightning 自动处理：混合精度、多设备训练、日志回调、梯度累积、checkpoint 触发。工程训练流程大幅简化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitClassifier</span>(L.LightningModule):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;将 nn.Module 包装成 LightningModule，托管训练细节。&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, lr: <span class="built_in">float</span></span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 真实的 PyTorch 模型（SimpleCNN）  </span></span><br><span class="line">        <span class="variable language_">self</span>.model = model  </span><br><span class="line">        <span class="comment"># 学习率  </span></span><br><span class="line">        <span class="variable language_">self</span>.lr = lr  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 将超参数保存到 checkpoint 中，方便恢复和可视化  </span></span><br><span class="line">        <span class="variable language_">self</span>.save_hyperparameters(ignore=[<span class="string">&quot;model&quot;</span>])  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;推理 / 前向过程，直接调用内部模型。&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(x)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;单个训练 step 的逻辑。  </span></span><br><span class="line"><span class="string">        Args:            </span></span><br><span class="line"><span class="string">	        batch: 一个 batch 的数据，包含 (x, y)。  </span></span><br><span class="line"><span class="string">            batch_idx: 当前 batch 的索引。  </span></span><br><span class="line"><span class="string">        Returns:            </span></span><br><span class="line"><span class="string">	        loss 张量，Lightning 会自动帮你做 backward。  </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>        </span><br><span class="line">        x, y = batch  </span><br><span class="line">        logits = <span class="variable language_">self</span>(x)  </span><br><span class="line">        loss = F.cross_entropy(logits, y)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># self.log 会自动将指标记录到 logger（TensorBoard / W&amp;B 等）  </span></span><br><span class="line">        <span class="variable language_">self</span>.log(<span class="string">&quot;train_loss&quot;</span>, loss, prog_bar=<span class="literal">True</span>, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>)  </span><br><span class="line">        <span class="keyword">return</span> loss  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;单个验证 step 的逻辑。&quot;&quot;&quot;</span>  </span><br><span class="line">        x, y = batch  </span><br><span class="line">        logits = <span class="variable language_">self</span>(x)  </span><br><span class="line">        val_loss = F.cross_entropy(logits, y)  </span><br><span class="line">        <span class="variable language_">self</span>.log(<span class="string">&quot;val_loss&quot;</span>, val_loss, prog_bar=<span class="literal">True</span>, on_step=<span class="literal">False</span>, on_epoch=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;配置优化器（和可选的学习率调度器）。&quot;&quot;&quot;</span>  </span><br><span class="line">        optimizer = optim.Adam(<span class="variable language_">self</span>.parameters(), lr=<span class="variable language_">self</span>.lr)  </span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br></pre></td></tr></table></figure>
<h3 id="9-6-Hydra-配置：真正的可复现训练"><a href="#9-6-Hydra-配置：真正的可复现训练" class="headerlink" title="9.6 Hydra 配置：真正的可复现训练"></a>9.6 Hydra 配置：真正的可复现训练</h3><p>整个训练系统的超参数都由配置管理：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主配置文件：集中管理模型 / 数据 / 训练相关超参数  </span></span><br><span class="line"><span class="comment"># 可以通过命令行覆盖，例如：  </span></span><br><span class="line"><span class="comment">#   python run.py data.batch_size=128 train.lr=5e-4  </span></span><br><span class="line"><span class="attr">model:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">simple_cnn</span>  </span><br><span class="line">  <span class="attr">num_classes:</span> <span class="number">10</span>          <span class="comment"># CIFAR-10 有 10 个类别  </span></span><br><span class="line">  </span><br><span class="line"><span class="attr">data:</span>  </span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">64</span>           <span class="comment"># 每个 batch 的样本数  </span></span><br><span class="line">  <span class="attr">num_workers:</span> <span class="number">4</span>           <span class="comment"># DataLoader 后台进程数量  </span></span><br><span class="line">  <span class="attr">data_dir:</span> <span class="string">./data</span>         <span class="comment"># 数据集存放路径（会自动下载到这里）  </span></span><br><span class="line">  </span><br><span class="line"><span class="attr">train:</span>  </span><br><span class="line">  <span class="attr">max_epochs:</span> <span class="number">2</span>           <span class="comment"># 训练轮数  </span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">1e-3</span>                 <span class="comment"># 学习率  </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 训练硬件 &amp; 加速配置  </span></span><br><span class="line">  <span class="attr">precision:</span> <span class="number">32</span><span class="string">-true</span>       <span class="comment"># 默认用最稳定的 float32，可改为 16-mixed  </span></span><br><span class="line">  <span class="attr">accelerator:</span> <span class="string">auto</span>        <span class="comment"># auto: 自动选择 gpu / mps / cpu  </span></span><br><span class="line">  <span class="attr">devices:</span> <span class="number">1</span>               <span class="comment"># 使用的设备个数（比如多卡时可以设置成 2 / 4）  </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 梯度累积：显存较小时可以开启  </span></span><br><span class="line">  <span class="attr">accumulate_grad_batches:</span> <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 是否对模型进行 torch.compile 加速（需要 PyTorch 2.0+）  </span></span><br><span class="line">  <span class="attr">compile:</span> <span class="literal">false</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 实验追踪（Weights &amp; Biases），默认关闭  </span></span><br><span class="line">  <span class="attr">use_wandb:</span> <span class="literal">false</span>  </span><br><span class="line">  <span class="attr">wandb_project:</span> <span class="string">cifar10-project</span></span><br></pre></td></tr></table></figure>
<p>我们可以轻松覆盖参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run.py train.max_epochs=30 data.batch_size=128</span><br></pre></td></tr></table></figure>
<p>不需要修改任何 Python 代码。</p>
<h3 id="9-7-工程化训练：自动保存-自动导出模型"><a href="#9-7-工程化训练：自动保存-自动导出模型" class="headerlink" title="9.7 工程化训练：自动保存 + 自动导出模型"></a>9.7 工程化训练：自动保存 + 自动导出模型</h3><p>核心逻辑在 run.py：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@hydra.main(<span class="params">config_path=<span class="string">&quot;configs&quot;</span>, config_name=<span class="string">&quot;config&quot;</span>, version_base=<span class="literal">None</span></span>)  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">cfg: DictConfig</span>):  </span><br><span class="line">    <span class="comment"># 0. 设定随机种子，保证实验可复现  </span></span><br><span class="line">    seed_everything(<span class="number">42</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 1. 数据：只负责构建 DataLoader    </span></span><br><span class="line">    train_loader, val_loader = build_dataloader(  </span><br><span class="line">        batch_size=cfg.data.batch_size,  </span><br><span class="line">        num_workers=cfg.data.num_workers,  </span><br><span class="line">        data_dir=cfg.data.data_dir,  </span><br><span class="line">    )  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 2. 模型：nn.Module + LightningModule  </span></span><br><span class="line">    lit_model = build_model(cfg)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 3. Trainer：Lightning 接管训练循环 / 日志 / AMP 等  </span></span><br><span class="line">    trainer = build_trainer(cfg)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 4. 启动训练  </span></span><br><span class="line">    trainer.fit(lit_model, train_loader, val_loader)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 5. 训练结束后：自动从 best.ckpt 导出 TorchScript 模型 model.pt    </span></span><br><span class="line">    best_ckpt_path = os.path.join(<span class="string">&quot;checkpoints&quot;</span>, <span class="string">&quot;best.ckpt&quot;</span>)  </span><br><span class="line">    <span class="keyword">if</span> os.path.exists(best_ckpt_path):  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[INFO] 训练完成，发现 best.ckpt: <span class="subst">&#123;best_ckpt_path&#125;</span>&quot;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] 正在从 best.ckpt 导出 TorchScript 模型为 model.pt ...&quot;</span>)  </span><br><span class="line">        export_torchscript(  </span><br><span class="line">            checkpoint_path=best_ckpt_path,  </span><br><span class="line">            out=<span class="string">&quot;model.pt&quot;</span>,  </span><br><span class="line">            num_classes=cfg.model.num_classes,  </span><br><span class="line">        )  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] 导出完成，可以直接使用 deploy/inference.py 或 deploy/api.py。&quot;</span>)  </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[WARN] 未找到 best.ckpt，跳过自动导出 model.pt。预期路径: checkpoints/best.ckpt&quot;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:  </span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>训练日志（max_epochs先改为2）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">GPU available: True (mps), used: True</span><br><span class="line">TPU available: False, using: 0 TPU cores</span><br><span class="line">  | Name  | Type      | Params | Mode  | FLOPs</span><br><span class="line">----------------------------------------------------</span><br><span class="line">0 | model | SimpleCNN | 183 K  | train | 0    </span><br><span class="line">----------------------------------------------------</span><br><span class="line">183 K     Trainable params</span><br><span class="line">0         Non-trainable params</span><br><span class="line">183 K     Total params</span><br><span class="line">0.733     Total estimated model params size (MB)</span><br><span class="line">8         Modules in train mode</span><br><span class="line">0         Modules in eval mode</span><br><span class="line">0         Total Flops</span><br><span class="line">Epoch 1: 100%|█████████████████████████████████████████████████████████████| 782/782 [01:03&lt;00:00, 12.26it/s, v_num=1, train_loss_step=0.990, val_loss=1.070, train_loss_epoch=1.280]`Trainer.fit` stopped: `max_epochs=2` reached.                                               </span><br><span class="line">Epoch 1: 100%|█████████████████████████████████████████████████████████████| 782/782 [01:03&lt;00:00, 12.26it/s, v_num=1, train_loss_step=0.990, val_loss=1.070, train_loss_epoch=1.280]</span><br><span class="line">[INFO] 训练完成，发现 best.ckpt: checkpoints/best.ckpt</span><br><span class="line">[INFO] 正在从 best.ckpt 导出 TorchScript 模型为 model.pt ...</span><br><span class="line">[OK] TorchScript 导出成功: model.pt</span><br><span class="line">[INFO] 导出完成，可以直接使用 deploy/inference.py 或 deploy/api.py。</span><br></pre></td></tr></table></figure>
<p>训练结束后自动生成：</p>
<ul>
<li><strong>best.ckpt</strong>（训练权重，Lightning 格式）</li>
<li><strong>model.pt</strong> （TorchScript，可部署格式）</li>
</ul>
<p>这一步完模型可以脱离 PyTorch 训练环境了，就是一个可以使用的模型了。</p>
<h3 id="9-8-训练出的模型有什么用？"><a href="#9-8-训练出的模型有什么用？" class="headerlink" title="9.8 训练出的模型有什么用？"></a>9.8 训练出的模型有什么用？</h3><p>训练出的模型是“图像理解系统”的核心——一个 <strong>CIFAR-10 图像分类器</strong>。它能做什么？<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/JKr9SK.png" alt="JKr9SK"></p>
<center>图 9-1 CIFAR-10数据集</center>

<p>输入一张如上图十分类的 RGB 图像，模型可自动识别其类别。后续可以换模型结构，换数据集，换任务（分类 → 检测&#x2F;分割），达到不一样的目标。本质上这可作为视觉任务基底的工程框架。</p>
<h3 id="9-9-模型部署：让模型成为真实服务"><a href="#9-9-模型部署：让模型成为真实服务" class="headerlink" title="9.9 模型部署：让模型成为真实服务"></a>9.9 模型部署：让模型成为真实服务</h3><p>我们工程自带 deploy&#x2F; 目录，包含两个非常核心的部署模块。</p>
<h4 id="9-9-1-本地推理（inference-py）"><a href="#9-9-1-本地推理（inference-py）" class="headerlink" title="9.9.1 本地推理（inference.py）"></a>9.9.1 本地推理（inference.py）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/inference.py --image xxx.jpg --model_path model.pt</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 输入一张青蛙图片</span><br><span class="line">python deploy/inference.py --image test_image/gg.jpg --model_path model.pt  </span><br><span class="line">[RESULT] 图片 test_image/gg.jpg 的预测类别 index 为: 6</span><br><span class="line"></span><br><span class="line">python deploy/inference.py --image test_image/car.jpg --model_path model.pt</span><br><span class="line">[RESULT] 图片 test_image/car.jpg 的预测类别 index 为: 1</span><br></pre></td></tr></table></figure>
<p>功能：</p>
<ul>
<li>加载 TorchScript 模型</li>
<li>自动预处理图像</li>
<li>输出预测类别</li>
</ul>
<p>用途：</p>
<ul>
<li>快速验证部署模型是否正常</li>
<li>开发测试</li>
<li>离线批量推理</li>
</ul>
<h4 id="9-9-2-在线-API-推理（FastAPI）"><a href="#9-9-2-在线-API-推理（FastAPI）" class="headerlink" title="9.9.2 在线 API 推理（FastAPI）"></a>9.9.2 在线 API 推理（FastAPI）</h4><p>运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">uvicorn deploy.api:app --host 0.0.0.0 --port=8000 --reload</span><br><span class="line"></span><br><span class="line">INFO:     Will watch for changes in these directories: [&#x27;/xxx/dl_cifar10_project_autosave_export&#x27;]</span><br><span class="line">INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)</span><br><span class="line">INFO:     Started reloader process [40746] using StatReload</span><br><span class="line">INFO:     Started server process [40748]</span><br><span class="line">INFO:     Waiting for application startup.</span><br><span class="line">[INFO] 模型已加载: model.pt</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></table></figure>
<p>启动一个 REST 推理服务。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST -F &quot;file=@test_image/gg.jpg&quot; http://localhost:8000/predict</span><br><span class="line">&#123;&quot;prediction&quot;:6&#125;%</span><br></pre></td></tr></table></figure>
<p><strong>POST &#x2F;predict</strong></p>
<ul>
<li>上传一张图片</li>
<li>返回分类结果</li>
<li>前端 &#x2F; APP &#x2F; 其它服务可以直接调用</li>
<li>TorchScript 使推理速度快且稳定</li>
</ul>
<p>这是项目部署的标准形式。</p>
<h3 id="9-10-导出-ONNX：让模型跨平台"><a href="#9-10-导出-ONNX：让模型跨平台" class="headerlink" title="9.10 导出 ONNX：让模型跨平台"></a>9.10 导出 ONNX：让模型跨平台</h3><p>deploy&#x2F;export.py 还能导出 ONNX：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python deploy/export.py</span><br><span class="line"></span><br><span class="line">[OK] ONNX 导出成功: model.onnx</span><br><span class="line">[OK] TorchScript 导出成功: model.pt</span><br></pre></td></tr></table></figure>
<p>即可得到：</p>
<ul>
<li>model.onnx</li>
<li>model.pt</li>
</ul>
<p>ONNX 可以部署在：</p>
<ul>
<li>TensorRT（NVIDIA）</li>
<li>ONNX Runtime</li>
<li>OpenVINO</li>
<li>iOS &#x2F; Android &#x2F; Web</li>
<li>C++ 程序</li>
</ul>
<p>这让模型可具备跨平台能力。</p>
<h3 id="9-11-全流程工程图"><a href="#9-11-全流程工程图" class="headerlink" title="9.11 全流程工程图"></a>9.11 全流程工程图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Hydra 配置（config.yaml）</span><br><span class="line">            ↓</span><br><span class="line">run.py （统一调度：数据 + 模型 + Trainer）</span><br><span class="line">            ↓</span><br><span class="line">DataLoader （并行加载 + 数据增强）</span><br><span class="line">            ↓</span><br><span class="line">模型构建（SimpleCNN）</span><br><span class="line">            ↓</span><br><span class="line">LightningModule（训练逻辑）</span><br><span class="line">            ↓</span><br><span class="line">Trainer（AMP + compile + DDP + Logging）</span><br><span class="line">            ↓</span><br><span class="line">Checkpoint（自动保存 best.ckpt）</span><br><span class="line">            ↓</span><br><span class="line">自动导出 TorchScript（model.pt）</span><br><span class="line">            ↓</span><br><span class="line">本地推理（inference.py）</span><br><span class="line">            ↓</span><br><span class="line">在线 API（FastAPI）</span><br><span class="line">            ↓</span><br><span class="line">ONNX 导出 → 全平台部署</span><br></pre></td></tr></table></figure>
<p>这个流程已经完全符合现代深度学习工程体系。</p>
<p><strong>小结</strong><br>这一章完成了整篇内容的工程化收束，我们已经看到 PyTorch 如何从零散的代码生长为系统化的训练框架；如何从单机上的一次性试验扩展为可复现、可追踪的实验体系；如何让原本松散的 Python 脚本演化为结构清晰、可维护、可协作的项目工程；如何将朴素的单机训练进一步提升到带有 AMP、compile 等现代加速机制的高性能训练；以及如何让学术环境中的原型模型真正走向生产部署，成为可交付的工程成果。</p>
<p>总结一句：<strong>真正的深度学习，不只在训练模型，而在构建系统。</strong></p>
<h2 id="10-总结：从“写代码”到“构建系统”"><a href="#10-总结：从“写代码”到“构建系统”" class="headerlink" title="10. 总结：从“写代码”到“构建系统”"></a>10. 总结：从“写代码”到“构建系统”</h2><p>PyTorch 提供的是一整套从数值到结构，再到工程与系统的完整途径。前九章的内容看似分散：张量、模块、梯度、调试、性能、工程……。但它们从根本上围绕同一件事展开：<strong>让一个可微的计算世界，逐步演化成一个可组织、可优化、可调试、可扩展的系统。</strong> 以下是对整篇文章的总结：</p>
<ol>
<li>梯度如何让系统能够“学习”<br> 第一章展示了一个最小训练流程。在那条简单的路径中，数据进入模型，进行预测，产生误差，梯度反向流动，参数随之变化。这是深度学习的原型，一个系统只要能够感受偏差并做出修正，就具备了学习的基本能力。</li>
<li>基本单位张量<br> 第二章讲了 PyTorch 的基础结构。Tensor 并不是普通数组，它同时携带数值、设备信息以及梯度追踪能力。当 requires_grad 被打开，它便成为计算图中的一个“节点”。这些节点在运算时互相连接，形成一张可微的图。整个系统的可学习性就在这张图中不断流动。</li>
<li>Module可嵌套组合的结构<br> 第三章将注意力从“计算”带到了“组织”，Module 是构建神经网络的基础单位，它能够容纳参数、定义前向路径、组合子模块。一个复杂的网络，不过是 Module 的层层嵌套，结构的存在，让网络能够表达更多层级、概念以及空间关系。</li>
<li>数据如何进入模型<br> 第四章讲述了数据流动，Dataset 负责告诉系统每个样本的意义，DataLoader 则负责把数据源源不断地传递给模型。它不仅是数据传输工具，还包含缓存、随机化、多线程等机制。</li>
<li>动态计算图让模型有了可塑性<br> 第五章讲了 PyTorch 的核心，动态图意味着计算结构由数据即时决定，每一次前向传播都会产生一条全新的计算路径。grad_fn 记录了每个节点的来路，backward 则沿着这张图反向行走，完成误差的传递。detach 可以切断图，no_grad 可以冻结图。这一切构成了系统最核心的机制：<strong>能够对自身的“计算历史”进行可控的回溯。</strong></li>
<li>调试<br> 第六章讲了如何排查错误。梯度可能爆炸，也可能消失；loss 可能发散，也可能完全不动；某些层可能停止更新，某些算子可能破坏计算图。这些现象不是随机，而是线索。通过检测梯度、分析激活、检查图结构、监控数值，我们能够一步步定位问题。调试能力让系统从“能运行”走向“能稳定地成长”。</li>
<li>性能优化<br> 第七章把我们带入现代深度学习的核心挑战。混合精度减少显存并提升吞吐；编译加速让 Python 级的执行变成图级优化；kernel 融合减少调度开销；数据流水线优化提高输入带宽；profiler 帮助找出瓶颈所在。这一章告诉我们，智能并不会自动变快，需要借助底层优化才能释放模型的真正潜力。</li>
<li>工程生态<br> 第八章让深度学习从“实验”走向“工程”。Hydra 让配置井然有序；Lightning 抽离了繁琐的训练样板；W&amp;B 记录模型的一切变化；checkpoint 保存状态，使训练能够中断与恢复；部署工具让模型走向现实世界。工程化不是额外负担，而是智能系统能够被维护、扩展、协作和复现的基础。</li>
<li>端到端项目：计算、结构、数据、优化、工程<br> 第九章将所有内容汇聚成一条贯穿全流程的路径。Hydra 管理配置，run.py 作为调度中心；DataLoader 提供数据；model.py 定义结构；LightningModule 描述训练逻辑；Trainer 控制优化过程，同时连接 AMP、compile、DDP 等性能模块；W&amp;B 管理实验；最终产出一个可复现、可部署的系统。</li>
</ol>
<p>如果全篇内容提炼成一句核心思想：<strong>一个深度学习系统，是由可微分的计算、可组合的结构、可调试的反馈、可扩展的性能和可管理的工程共同构成的。</strong></p>
<h2 id="11-备注"><a href="#11-备注" class="headerlink" title="11. 备注"></a>11. 备注</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/keychankc/dl_code_for_blog/tree/main/044-dl_cifar10_project_autosave_export">第九章完整代码</a></li>
</ul>
<script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';	mermaid.initialize({startOnLoad: true, flowchart: {curve: 'linear'}}); </script>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/" title="PyTorch：从搭积木到构建系统">https://www.keychan.xyz/2025/12/02/044-pytorch-building-blocks-building-systems/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E6%A8%A1%E5%9D%97%E5%8C%96/" rel="tag"># 模块化</a>
              <a href="/tags/Tensor/" rel="tag"># Tensor</a>
              <a href="/tags/%E6%A2%AF%E5%BA%A6%E8%B0%83%E8%AF%95/" rel="tag"># 梯度调试</a>
              <a href="/tags/%E8%AE%AD%E7%BB%83%E5%B7%A5%E7%A8%8B%E5%8C%96/" rel="tag"># 训练工程化</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/11/25/043-rnn-to-transformer/" rel="prev" title="从 RNN 到 Transformer：时间建模的变革">
                  <i class="fa fa-angle-left"></i> 从 RNN 到 Transformer：时间建模的变革
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/12/05/045-asynchronous-transformation-of-uikit-project/" rel="next" title="UIKit 项目异步改造：Combine、async/await、@Published">
                  UIKit 项目异步改造：Combine、async/await、@Published <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">483k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/12/02/044-pytorch-building-blocks-building-systems/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
