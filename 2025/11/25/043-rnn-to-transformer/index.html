<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. 时间依赖与梯度消失：序列建模的困境在很多经典的图像任务中，我们通常假设不同样本之间是独立同分布的（i.i.d.）——也就是说，一张图片与另一张图片在统计上是相互独立的，模型只需要把一张图片看作一个整体输入来处理即可。卷积网络（CNN）则利用图像内部像素之间强烈的局部相关性，通过卷积核在空间上提取局部到全局的层级特征。 但在处理序列任务（sequence modeling）时情况就完全不同了：">
<meta property="og:type" content="article">
<meta property="og:title" content="从 RNN 到 Transformer：时间建模的变革">
<meta property="og:url" content="https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. 时间依赖与梯度消失：序列建模的困境在很多经典的图像任务中，我们通常假设不同样本之间是独立同分布的（i.i.d.）——也就是说，一张图片与另一张图片在统计上是相互独立的，模型只需要把一张图片看作一个整体输入来处理即可。卷积网络（CNN）则利用图像内部像素之间强烈的局部相关性，通过卷积核在空间上提取局部到全局的层级特征。 但在处理序列任务（sequence modeling）时情况就完全不同了：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/GBf857.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/X4blMh.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/Nipdlp.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/oDJU38.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/VrecJh.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/dKsrGQ.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/RutzqG.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/DFIUM9.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/qfWHXc.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/gVAa5V.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/qKDAFa.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/Xg0WWB.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_1_val_wmae.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_2_trend_compare.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_3_mae_over_time.png">
<meta property="article:published_time" content="2025-11-25T09:50:12.000Z">
<meta property="article:modified_time" content="2025-11-25T13:19:30.364Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="循环神经网络">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="位置编码">
<meta property="article:tag" content="注意力机制">
<meta property="article:tag" content="序列建模">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/GBf857.png">


<link rel="canonical" href="https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/","path":"2025/11/25/043-rnn-to-transformer/","title":"从 RNN 到 Transformer：时间建模的变革"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从 RNN 到 Transformer：时间建模的变革 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%97%B6%E9%97%B4%E4%BE%9D%E8%B5%96%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9A%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E7%9A%84%E5%9B%B0%E5%A2%83"><span class="nav-text">1. 时间依赖与梯度消失：序列建模的困境</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%BA%8F%E5%88%97%E7%9A%84%E6%9C%AC%E8%B4%A8%EF%BC%9A%E6%97%B6%E9%97%B4%E4%B8%8A%E7%9A%84%E4%BE%9D%E8%B5%96"><span class="nav-text">1.1 序列的本质：时间上的依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E6%97%B6%E9%97%B4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%9A%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92%E7%9A%84%E4%BB%A3%E4%BB%B7"><span class="nav-text">1.2 时间反向传播：信息传递的代价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E7%9B%B4%E8%A7%89%E7%B1%BB%E6%AF%94%EF%BC%9A%E4%BC%A0%E8%AF%9D%E6%B8%B8%E6%88%8F%E4%B8%AD%E7%9A%84%E2%80%9C%E4%BF%A1%E6%81%AF%E5%A4%B1%E7%9C%9F%E2%80%9D"><span class="nav-text">1.3 直觉类比：传话游戏中的“信息失真”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E6%97%B6%E9%97%B4%E5%B1%95%E5%BC%80%E4%B8%8E%E6%A2%AF%E5%BA%A6%E8%A1%B0%E5%87%8F"><span class="nav-text">1.4 时间展开与梯度衰减</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%97%A8%E6%8E%A7%E6%9C%BA%E5%88%B6%EF%BC%9ALSTM-%E4%B8%8E-GRU-%E7%9A%84%E7%BB%93%E6%9E%84%E7%AA%81%E7%A0%B4"><span class="nav-text">2. 门控机制：LSTM 与 GRU 的结构突破</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-LSTM%EF%BC%9A%E9%80%9A%E8%BF%87%E2%80%9C%E9%97%A8%E2%80%9D%E6%9D%A5%E6%8E%A7%E5%88%B6%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8"><span class="nav-text">2.1 LSTM：通过“门”来控制信息流动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-GRU%EF%BC%9A%E7%BB%93%E6%9E%84%E7%AE%80%E5%8C%96%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%89%88%E6%9C%AC"><span class="nav-text">2.2 GRU：结构简化的轻量化版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E4%BB%8E%E9%97%A8%E6%8E%A7%E5%88%B0%E2%80%9C%E6%AE%8B%E5%B7%AE%E6%80%9D%E6%83%B3%E2%80%9D%E7%9A%84%E5%90%AF%E7%A4%BA"><span class="nav-text">2.3 从门控到“残差思想”的启示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E4%BB%8E-Seq2Seq-%E5%88%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%9A%E5%AF%B9%E9%BD%90%E7%9A%84%E8%AF%9E%E7%94%9F"><span class="nav-text">3. 从 Seq2Seq 到注意力机制：对齐的诞生</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Seq2Seq-%E7%9A%84%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88"><span class="nav-text">3.1 Seq2Seq 的信息瓶颈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BC%95%E5%85%A5%EF%BC%9A%E5%8A%A8%E6%80%81%E7%9A%84%E4%BF%A1%E6%81%AF%E9%80%9A%E9%81%93"><span class="nav-text">3.2 注意力机制的引入：动态的信息通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%9A%E5%AF%B9%E9%BD%90%EF%BC%88Alignment%EF%BC%89"><span class="nav-text">3.3 注意力的直观理解：对齐（Alignment）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Encoder%E2%80%93Decoder-%E4%B8%8E-Attention-%E7%9A%84%E8%9E%8D%E5%90%88%E7%BB%93%E6%9E%84"><span class="nav-text">3.4 Encoder–Decoder 与 Attention 的融合结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Self-Attention%EF%BC%9A%E4%BB%8E%E5%AF%B9%E9%BD%90%E4%BB%96%E4%BA%BA%E5%88%B0%E5%AF%B9%E9%BD%90%E8%87%AA%E5%B7%B1"><span class="nav-text">4. Self-Attention：从对齐他人到对齐自己</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%BB%8E%E2%80%9C%E5%AF%B9%E9%BD%90%E4%BB%96%E4%BA%BA%E2%80%9D%E5%88%B0%E2%80%9C%E5%AF%B9%E9%BD%90%E8%87%AA%E5%B7%B1%E2%80%9D"><span class="nav-text">4.1 从“对齐他人”到“对齐自己”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Self-Attention-%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="nav-text">4.2 Self-Attention 的数学原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E7%BC%A9%E6%94%BE%E9%A1%B9%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%9A%E7%A8%B3%E5%AE%9A%E6%A2%AF%E5%BA%A6"><span class="nav-text">4.3 缩放项的作用：稳定梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Self-Attention-%E7%9A%84%E5%B9%B6%E8%A1%8C%E4%BC%98%E5%8A%BF"><span class="nav-text">4.4 Self-Attention 的并行优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Mask-%E6%9C%BA%E5%88%B6%EF%BC%9A%E4%BF%9D%E6%8C%81%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-text">4.5 Mask 机制：保持时间一致性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Transformer%EF%BC%9A%E5%B9%B6%E8%A1%8C%E5%8C%96%E7%9A%84%E5%85%A8%E5%B1%80%E5%BB%BA%E6%A8%A1%E6%9E%B6%E6%9E%84"><span class="nav-text">5. Transformer：并行化的全局建模架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E6%A8%A1%E5%9E%8B%E6%80%BB%E8%A7%88%EF%BC%9AEncoder-Decoder-%E5%A0%86%E5%8F%A0%E7%BB%93%E6%9E%84"><span class="nav-text">5.1 模型总览：Encoder-Decoder 堆叠结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Encoder-%E7%BB%93%E6%9E%84%EF%BC%9A%E5%B9%B6%E8%A1%8C%E5%BB%BA%E6%A8%A1%E4%B8%8E%E5%B1%82%E5%86%85%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">5.2 Encoder 结构：并行建模与层内归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Decoder-%E7%BB%93%E6%9E%84%EF%BC%9AMasked-Attention-Encoder-Decoder-Attention"><span class="nav-text">5.3 Decoder 结构：Masked Attention + Encoder-Decoder Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%9A%E8%AE%A9%E6%A8%A1%E5%9E%8B%E2%80%9C%E6%84%9F%E7%9F%A5%E9%A1%BA%E5%BA%8F%E2%80%9D"><span class="nav-text">5.4 位置编码：让模型“感知顺序”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-Transformer-%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%90%AF%E7%A4%BA"><span class="nav-text">5.5 Transformer 的优势与启示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E6%80%A7%E8%83%BD%EF%BC%9AO-T-vs-O-T%C2%B2-%E7%9A%84%E6%96%B0%E5%B9%B3%E8%A1%A1"><span class="nav-text">6. 复杂度与性能：O(T) vs O(T²) 的新平衡</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-RNN%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E4%B8%B2%E8%A1%8C%E7%93%B6%E9%A2%88"><span class="nav-text">6.1 RNN：线性复杂度的串行瓶颈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-Transformer%EF%BC%9A%E5%B9%B3%E6%96%B9%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E5%B9%B6%E8%A1%8C%E9%9D%A9%E5%91%BD"><span class="nav-text">6.2 Transformer：平方复杂度的并行革命</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%AF%B9%E6%AF%94%EF%BC%9A%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%AD%98%E5%82%A8"><span class="nav-text">6.3 复杂度对比：计算与存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-%E6%96%B0%E7%9A%84%E5%B9%B3%E8%A1%A1%EF%BC%9A%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E7%9A%84%E6%94%B9%E8%BF%9B%E6%96%B9%E5%90%91"><span class="nav-text">6.4 新的平衡：长序列建模的改进方向</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-1-Linear-Attention"><span class="nav-text">6.4.1  Linear Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-2-Longformer-BigBird"><span class="nav-text">6.4.2 Longformer &#x2F; BigBird</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-3-Performer"><span class="nav-text">6.4.3 Performer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E6%AF%94%E8%BE%83-LSTM-%E4%B8%8E-Transformer-%E5%9C%A8%E7%94%B5%E5%95%86%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E4%B8%AD%E7%9A%84%E8%A1%A8%E7%8E%B0"><span class="nav-text">7. 比较 LSTM 与 Transformer 在电商销量预测中的表现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E5%AE%9E%E9%AA%8C%E8%83%8C%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87"><span class="nav-text">7.1 实验背景与目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="nav-text">7.2 实验设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%89%B9%E5%BE%81"><span class="nav-text">7.2.1 数据与特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-text">7.2.2 模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-3-%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="nav-text">7.2.3 训练与评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-4-%E5%AE%8C%E6%95%B4%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81"><span class="nav-text">7.2.4 完整实验代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%88%86%E6%9E%90"><span class="nav-text">7.3 实验结果与分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%90%AF%E7%A4%BA"><span class="nav-text">7.4 结论与启示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E5%B0%8F%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B%EF%BC%9A%E4%BB%8E%E8%AE%B0%E5%BF%86%E5%88%B0%E7%90%86%E8%A7%A3"><span class="nav-text">8. 小结与展望：从记忆到理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E4%B8%89%E9%87%8D%E9%9D%A9%E5%91%BD%E7%9A%84%E8%84%89%E7%BB%9C"><span class="nav-text">8.1 三重革命的脉络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E7%BB%9F%E4%B8%80%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%9A%E4%BB%8E%E8%AE%B0%E5%BF%86%E5%88%B0%E7%90%86%E8%A7%A3"><span class="nav-text">8.2 统一的思想：从记忆到理解</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="从 RNN 到 Transformer：时间建模的变革 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从 RNN 到 Transformer：时间建模的变革
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-25 17:50:12 / 修改时间：21:19:30" itemprop="dateCreated datePublished" datetime="2025-11-25T17:50:12+08:00">2025-11-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/11/25/043-rnn-to-transformer/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/11/25/043-rnn-to-transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>50 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-时间依赖与梯度消失：序列建模的困境"><a href="#1-时间依赖与梯度消失：序列建模的困境" class="headerlink" title="1. 时间依赖与梯度消失：序列建模的困境"></a>1. 时间依赖与梯度消失：序列建模的困境</h2><p>在很多经典的图像任务中，我们通常假设<strong>不同样本之间是独立同分布的（i.i.d.）</strong>——也就是说，一张图片与另一张图片在统计上是相互独立的，模型只需要把一张图片看作一个整体输入来处理即可。卷积网络（CNN）则利用图像内部像素之间强烈的局部相关性，通过卷积核在空间上提取局部到全局的层级特征。</p>
<p>但在处理<strong>序列任务（sequence modeling）时情况就完全不同了：语言、语音、时间序列信号都具有明显的时间依赖</strong>，同一个序列内部，不同时间步之间往往高度相关。要正确预测下一个值、下一个词或下一个声音，模型必须<strong>记住同一条序列中之前发生过什么</strong>。这一点正是循环神经网络（Recurrent Neural Network, RNN）诞生的核心动机。</p>
<span id="more"></span>
<h3 id="1-1-序列的本质：时间上的依赖"><a href="#1-1-序列的本质：时间上的依赖" class="headerlink" title="1.1 序列的本质：时间上的依赖"></a>1.1 序列的本质：时间上的依赖</h3><p>在 RNN 中，模型通过一个循环结构不断接收输入，并将前一时刻的“隐藏状态”传递给当前时刻，从而建立时间依赖。其核心公式为：<br>$$h_t &#x3D; f(Wx_t + Uh_{t-1})$$<br>其中，$x_t$ 表示当前输入，$h_{t-1}$ 表示上一时刻的隐藏状态，$W$ 与 $U$ 为可学习的权重矩阵，$f(\cdot)$ 是非线性激活函数（如 $\tanh$ 或 ReLU）。可以看到，$h_t$ 的计算既依赖当前输入，又依赖历史状态，这使得网络能够“记忆”过去。换句话说，RNN 不仅在空间上建模特征，更在时间上捕捉依赖关系——它拥有“短期记忆”。</p>
<h3 id="1-2-时间反向传播：信息传递的代价"><a href="#1-2-时间反向传播：信息传递的代价" class="headerlink" title="1.2 时间反向传播：信息传递的代价"></a>1.2 时间反向传播：信息传递的代价</h3><p>在训练过程中，RNN 通常采用 <strong>时间反向传播（Backpropagation Through Time, BPTT）</strong>。顾名思义，这种方法会将网络在时间上展开成多层结构（每一层对应一个时间步），然后将误差信号从最后时刻逐步反向传递至最初输入。然而，这种“时间展开”带来了一个严重问题：<strong>梯度链式相乘</strong>。</p>
<p>由于每一步的梯度都要经过上一层的非线性变换，当时间步较长时，梯度会被连续乘上许多小于 1 的数（例如激活函数 $\tanh$ 的导数通常在 0～1 之间），结果是：</p>
<ul>
<li>若导数平均小于 1，梯度会<strong>指数式衰减</strong>（梯度消失）；</li>
<li>若导数平均大于 1，梯度会<strong>指数式膨胀</strong>（梯度爆炸）。</li>
</ul>
<p>这意味着，RNN 很难学习长距离的时间依赖——信息在传播过程中被“稀释”或“放大”，无法有效回传到早期时间步。</p>
<blockquote>
<p>严格地说，梯度是否会消失&#x2F;爆炸，取决于时间方向上 Jacobian 链 $\prod_t \frac{\partial h_t}{\partial h_{t-1}}$ 的<strong>谱半径（最大特征值的大小）</strong>。这里只用“每步导数的平均大小”来做直观类比，便于把握现象本质，并不构成严格的数学分析。</p>
</blockquote>
<h3 id="1-3-直觉类比：传话游戏中的“信息失真”"><a href="#1-3-直觉类比：传话游戏中的“信息失真”" class="headerlink" title="1.3 直觉类比：传话游戏中的“信息失真”"></a>1.3 直觉类比：传话游戏中的“信息失真”</h3><p>理解梯度消失，可以用一个简单类比：想象一群人围成一圈玩“传话游戏”，第一个人对第二个人耳语一句话，第二个人再转告第三个……当消息传到最后一个人时，往往已经面目全非。</p>
<p>在 RNN 中，信息沿时间方向逐步传递，每一次传播都会引入少量误差或衰减。当时间步过长时，这种累积效应导致最初的信息几乎完全丢失。因此，RNN 在理论上具备记忆能力，但在实践中往往只能“记住最近的几步”。</p>
<h3 id="1-4-时间展开与梯度衰减"><a href="#1-4-时间展开与梯度衰减" class="headerlink" title="1.4 时间展开与梯度衰减"></a>1.4 时间展开与梯度衰减</h3><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/GBf857.png" width="50%" />
<center> 图1-1 RNN 时间展开结构示意图</center>

<p>图中左侧展示了循环神经网络在单个时间步的结构：当前输入 $x_t$ 与前一时刻隐藏状态 $h_{t-1}$ 共同进入循环单元 $A$，得到当前隐藏状态 $h_t$。右侧则展示了“时间展开（Unfolding in Time）”的形式：同一个循环单元 $A$ 在每个时间步共享参数 $W, U, V$，依次处理输入序列 $x_1, x_2,\dots,x_t$，并输出对应的隐藏状态 $h_1, h_2,\dots,h_t$。这种展开方式揭示了 RNN 如何通过时间维度的连接实现记忆与状态传递。</p>
<p><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/X4blMh.png" alt="X4blMh"></p>
<center>图 1-2 梯度随时间步衰减的趋势曲线</center>

<p>可以把这张图理解成“梯度在时间上传递时的命运曲线”。横轴是时间步，也就是我们在 RNN 中不断向前展开的序列长度；纵轴是梯度的大小，用对数坐标画出来，这样能清楚看到变化的趋势。<br>蓝色的线代表当梯度在每一步都被乘上一个小于 1 的数时（比如 0.85），它会<strong>越传越小，最后几乎变成 0</strong>——这就是我们说的“梯度消失”；橙色虚线是理想情况，乘的因子大约等于 1，梯度能<strong>稳定地传递</strong>下去；绿色虚线则相反，乘的因子稍微大于 1（比如 1.05），结果就是<strong>越传越大，最后爆炸</strong>。图中还标出了背景区域：下面那块橙色区域是“梯度几乎消失”的范围，上面那块蓝色区域是“梯度爆炸”的风险区。所以只要 λ（乘的这个因子）偏离 1 一点点，时间长了问题就被放大成指数级。<br>这张图告诉我们一个简单但重要的事实：<strong>在长序列或深层结构中，梯度要么消失、要么爆炸，很难自然保持稳定。</strong> 这也正是后来 LSTM、GRU 等结构被提出的原因——它们通过“门控机制”来控制梯度的流动，让信息能在时间上传得更远。</p>
<p><strong>小结</strong><br>循环神经网络的思想在于“让模型记得过去”，但其训练机制却天然存在“遗忘”的风险。由于时间反向传播导致的梯度衰减，RNN 往往只能学习短期依赖。这一局限成为序列建模的最大瓶颈，也直接促生了 LSTM、GRU 等改进结构——它们的设计目标，就是<strong>让记忆能跨越时间衰减，保留更长的上下文信息</strong>。</p>
<h2 id="2-门控机制：LSTM-与-GRU-的结构突破"><a href="#2-门控机制：LSTM-与-GRU-的结构突破" class="headerlink" title="2. 门控机制：LSTM 与 GRU 的结构突破"></a>2. 门控机制：LSTM 与 GRU 的结构突破</h2><p>在上一章中，我们看到普通 RNN 在长序列建模中会面临<strong>梯度消失与梯度爆炸</strong>的问题——模型要么“记不住过去”，要么“被历史淹没”。</p>
<p>为了解决这一困境，研究者们引入了一种“<strong>受控记忆机制（Gated Mechanism）</strong>”，让网络能够<strong>有选择地记忆与遗忘信息</strong>。其中最具代表性的结构，就是 <strong>LSTM（Long Short-Term Memory）</strong> 和 <strong>GRU（Gated Recurrent Unit）</strong>。</p>
<h3 id="2-1-LSTM：通过“门”来控制信息流动"><a href="#2-1-LSTM：通过“门”来控制信息流动" class="headerlink" title="2.1 LSTM：通过“门”来控制信息流动"></a>2.1 LSTM：通过“门”来控制信息流动</h3><p>LSTM 的核心思想，是为传统 RNN 增加<strong>门（Gate）结构</strong>，在信息传递的每个时间步上，对“保留什么”“丢弃什么”进行精细调控。与普通 RNN 相比，LSTM 多了三个关键门控单元：</p>
<ol>
<li><strong>遗忘门（Forget Gate）</strong>：决定前一时刻的状态 $C_{t-1}$ 中哪些信息应该被丢弃。<br> $f_t &#x3D; \sigma(W_f [h_{t-1}, x_t] + b_f)$，当 $f_t$ 接近 0 时，该部分信息被遗忘；当 $f_t$ 接近 1 时，则被完整保留。</li>
<li><strong>输入门（Input Gate）</strong>：控制当前输入 $x_t$ 有多少新信息被写入。<br> $i_t &#x3D; \sigma(W_i [h_{t-1}, x_t] + b_i), \quad \tilde{C}t &#x3D; \tanh(W_c [h{t-1}, x_t] + b_c)$。最终的记忆更新为：$C_t &#x3D; f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$</li>
<li><strong>输出门（Output Gate）</strong>：决定当前时刻输出 $h_t$中哪些信息被暴露给下一层。<br> $o_t &#x3D; \sigma(W_o [h_{t-1}, x_t] + b_o), \quad h_t &#x3D; o_t \odot \tanh(C_t)$</li>
</ol>
<p>通过这三个门，LSTM 形成了一条贯穿时间维度的 <strong>“记忆通道（Cell State）”</strong>。这条通路允许梯度在时间上直接传播，而不被反复缩放或爆炸，从而大幅缓解了梯度消失问题。换句话说，LSTM 让网络学会“<strong>什么时候记，什么时候忘</strong>”，而不是被动地受制于时间长度。这也是它在语言建模、语音识别等长期依赖任务中取得成功的根本原因。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/Nipdlp.png" width="50%" /></p>
<center>图 2-1 LSTM 单元结构示意图</center>

<p>如上图展示了长短期记忆网络（LSTM, Long Short-Term Memory）单元的内部结构及信息流向。<br>输入 $x_t$ 与前一时刻的输出 $h_{t-1}$ 一起作为当前时间步的输入信号，分别经过遗忘门 $f_t$、输入门 $i_t$、候选记忆单元 $\tilde{C}_t$ 和输出门 $o_t$ 的非线性变换，控制信息在细胞状态中的保留、更新与输出。图中上方贯穿整单元的粗线表示细胞状态（Cell State）$C_t$ 的主通路，它允许信息在时间维度上以较少衰减的方式传递，被形象地称为“梯度高速公路（Gradient Highway）”。图中多个乘法节点（×）表示各门控对信息流的选择性调节，加法节点（+）实现旧状态与新候选信息的融合。右侧的 $\tanh$ 与输出门 $o_t$ 共同决定当前隐藏状态（输出）$h_t$，并将其传递至下一时间步或上层网络。该结构直观地体现了 LSTM 通过门控机制与状态通路设计，在结构上实现长期记忆保留与梯度稳定传播的原理。</p>
<h3 id="2-2-GRU：结构简化的轻量化版本"><a href="#2-2-GRU：结构简化的轻量化版本" class="headerlink" title="2.2 GRU：结构简化的轻量化版本"></a>2.2 GRU：结构简化的轻量化版本</h3><p>虽然 LSTM 在性能上优越，但其结构复杂、计算量大。为了提高训练效率，<strong>GRU（Gated Recurrent Unit）</strong> 在 LSTM 的基础上进行了简化：</p>
<ul>
<li>将“遗忘门”和“输入门”合并为一个 <strong>更新门（Update Gate）</strong>；</li>
<li>将“输出门”与隐状态更新逻辑融合，去掉了独立的 Cell State。</li>
</ul>
<p>GRU 的更新逻辑本质上就是一种“可控混合”：它在旧记忆 $h_{t-1}$ 与新候选记忆 $\tilde{h}_t$ 之间进行加权选择。这种设计虽然省略了独立的 Cell State，但依然能保持长期依赖能力。因此在许多任务中，GRU 的表现与 LSTM 相差无几，甚至在小模型或低资源场景下更具优势。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/oDJU38.png" width="80%" /></p>
<center>图 2-2 RNN vs LSTM 的性能对比示意</center>

<h3 id="2-3-从门控到“残差思想”的启示"><a href="#2-3-从门控到“残差思想”的启示" class="headerlink" title="2.3 从门控到“残差思想”的启示"></a>2.3 从门控到“残差思想”的启示</h3><p>值得注意的是，LSTM 中的 Cell State 通路在<strong>梯度流动的视角</strong>上，与后来 ResNet 中的残差连接有一定的相似性：两者都试图提供一条相对“平滑”的信息通路，让重要信息可以跨越多步传播，而不过度被每一层的非线性扰动所破坏。LSTM 通过 $$C_t &#x3D; f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$这种<strong>门控加权和</strong>的形式，在 $f_t \approx 1$、$i_t \approx 0$ 时，为 Cell State 提供了一条弱非线性、弱衰减的“记忆通路”。虽然这并不等价于 ResNet 中严格意义上的 $y &#x3D; x + F(x)$ 残差结构，但在“为深层&#x2F;长时依赖提供稳定梯度路径”这一设计目标上，两者有着高度一致的思路。因此，LSTM 不仅是序列建模的一次突破，也为深度网络设计提供了重要启发：<strong>稳定的梯度流 + 信息直通通道 &#x3D; 可训练的深度结构。</strong></p>
<p><strong>小结</strong><br>LSTM 和 GRU 通过在结构中显式引入“门控”机制，为神经网络的记忆与遗忘提供了可学习的控制。LSTM 的三门设计更具表达力，而 GRU 则以更简洁的形式实现近似功能。它们共同的核心思想是：<br><strong>让网络主动决定哪些信息该记住、哪些该忘记，从而打破时间依赖带来的梯度瓶颈。</strong></p>
<h2 id="3-从-Seq2Seq-到注意力机制：对齐的诞生"><a href="#3-从-Seq2Seq-到注意力机制：对齐的诞生" class="headerlink" title="3. 从 Seq2Seq 到注意力机制：对齐的诞生"></a>3. 从 Seq2Seq 到注意力机制：对齐的诞生</h2><p>在深度学习早期，<strong>Seq2Seq（Sequence to Sequence）模型</strong> 是机器翻译等序列任务的主流框架。它通过一个 <strong>Encoder-Decoder</strong> 结构，将输入序列编码成一个固定长度的向量，再由解码器生成输出序列。这种结构首次让模型具备了“读一句话、再写一句话”的能力，被广泛应用于机器翻译、对话系统、摘要生成等任务中。然而，Seq2Seq 模型也存在一个根本性的瓶颈——<strong>信息压缩问题</strong>。</p>
<h3 id="3-1-Seq2Seq-的信息瓶颈"><a href="#3-1-Seq2Seq-的信息瓶颈" class="headerlink" title="3.1 Seq2Seq 的信息瓶颈"></a>3.1 Seq2Seq 的信息瓶颈</h3><p>在标准 Seq2Seq 模型中，<strong>Encoder</strong> 逐步读取输入序列（如一句话的每个单词），并将其最终编码为一个固定维度的向量 c。随后，<strong>Decoder</strong> 仅凭这个向量去生成完整的目标句子。<br>$$h_t^{enc} &#x3D; f(x_t, h_{t-1}^{enc}), \quad c &#x3D; h_T^{enc}, \quad y_t &#x3D; g(y_{t-1}, h_{t-1}^{dec}, c)$$<br>虽然这种方式在短句上效果良好，但随着输入长度增加，编码器必须在有限的向量中“压缩”整个句子的语义，这导致信息丢失严重。举例来说，当模型翻译长句时，它往往能正确生成句首，却在句尾出现遗漏或错译。这就是所谓的 <strong>“固定向量瓶颈（Fixed-length Bottleneck）”</strong> —— 模型试图用一个向量概括整段语义，却牺牲了上下文的细粒度。</p>
<h3 id="3-2-注意力机制的引入：动态的信息通道"><a href="#3-2-注意力机制的引入：动态的信息通道" class="headerlink" title="3.2 注意力机制的引入：动态的信息通道"></a>3.2 注意力机制的引入：动态的信息通道</h3><p>为了解决这一问题，<strong>Bahdanau 等人在 2014 年提出了注意力机制（Attention Mechanism）</strong>。核心思想是：</p>
<blockquote>
<p>解码时，模型不必依赖单一的上下文向量，而是<strong>根据当前生成的词，动态选择输入序列中最相关的部分</strong>。</p>
</blockquote>
<p>也就是说，每当解码器要输出一个新词时，它会计算当前状态与输入各部分的“相关程度”，并以此为权重对输入向量加权求和——得到一个<strong>动态上下文向量</strong>。<br>$$\text{Attention}(Q,K,V) &#x3D; \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$<br>其中：</p>
<ul>
<li>$Q（Query）$：来自解码器当前的隐状态；</li>
<li>$K, V$：来自编码器所有时刻的输出；</li>
<li>相关性通过内积 $QK^\top$ 计算，并经 Softmax 转为权重分布。</li>
</ul>
<p>最终的加权结果让模型在每个时间步都能“聚焦”到输入句子的不同部分。换句话说，<strong>注意力机制赋予了模型“选择性记忆”的能力</strong>，让信息流动不再被单一向量限制，而是形成一种<strong>可变的、位置相关的通道</strong>。</p>
<h3 id="3-3-注意力的直观理解：对齐（Alignment）"><a href="#3-3-注意力的直观理解：对齐（Alignment）" class="headerlink" title="3.3 注意力的直观理解：对齐（Alignment）"></a>3.3 注意力的直观理解：对齐（Alignment）</h3><p>在机器翻译场景中，注意力机制的效果可以通过<strong>对齐热力图（Alignment Heatmap）</strong> 来直观展示。横轴表示输入语言的单词，纵轴表示输出语言的单词。每个格子的颜色深浅表示当前输出词与某个输入词之间的注意力权重大小。</p>
<p>例如，当模型翻译英语句子 “I love deep learning” 为中文时，生成“我”时主要关注 “I”，生成“学习”时则聚焦 “learning”。这种“输入-输出位置的软对齐”是传统 Seq2Seq 无法实现的，它让网络能自然捕捉到跨语言、跨时间的语义对应关系。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/VrecJh.png" width="70%" /></p>
<center>图 3-1 翻译任务中的注意力对齐热力图</center>

<p>图中横轴为英文源句单词，纵轴为中文目标句单词，颜色深浅表示注意力权重大小。深色格子表示生成对应中文词时，模型更关注的英文位置。该热力图直观展示了注意力机制在翻译过程中实现输入-输出动态对齐的过程。</p>
<h3 id="3-4-Encoder–Decoder-与-Attention-的融合结构"><a href="#3-4-Encoder–Decoder-与-Attention-的融合结构" class="headerlink" title="3.4 Encoder–Decoder 与 Attention 的融合结构"></a>3.4 Encoder–Decoder 与 Attention 的融合结构</h3><p>在引入注意力机制后，原本单一的上下文向量 c 被动态计算替代。在解码阶段，每一步的上下文由注意力模块生成并传入解码器：<br>$$c_t &#x3D; \sum_i \alpha_{t,i} h_i^{enc}, \quad y_t &#x3D; g(y_{t-1}, h_{t-1}^{dec}, c_t)$$<br>这使得模型在每一时刻都能重新“查看”输入序列，从而有效突破长序列的依赖瓶颈。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/dKsrGQ.png" alt="dKsrGQ"></p>
<center>图 3-2 Encoder–Decoder 与 Attention 模块交互示意图</center>

<p>图中左侧为编码器（Encoder），生成一系列隐藏状态 $h_{1}, h_{2}, …, h_{T}$；每个解码步 t 的解码器（Decoder）通过注意力权重 $\alpha_{t,i}$ 对所有编码器状态进行加权求和，得到上下文向量 $c_t &#x3D; \sum_i \alpha_{t,i} h_i^{enc}$。该上下文向量与上一步解码状态共同决定当前输出 $y_t$。注意力机制使模型在生成每个词时都能动态“回看”整个输入序列，从而有效缓解长序列依赖问题。</p>
<p><strong>小结：从记忆到对齐</strong><br>注意力机制的引入标志着神经网络从“被动记忆”到“主动选择”的转变。它让模型在每次预测时都能灵活访问输入序列的不同部分，从而在长序列任务中保持稳定性能。</p>
<p>Seq2Seq 存在信息瓶颈：单向压缩导致长程依赖丢失；注意力通过“相关性加权”让模型动态访问输入；注意力即“软对齐”，让模型具备了灵活的语义映射能力。这为后来的 <strong>Transformer 架构</strong> 奠定了理论基础，也揭开了“注意力统一视角”的新篇章。</p>
<h2 id="4-Self-Attention：从对齐他人到对齐自己"><a href="#4-Self-Attention：从对齐他人到对齐自己" class="headerlink" title="4. Self-Attention：从对齐他人到对齐自己"></a>4. Self-Attention：从对齐他人到对齐自己</h2><p>在上一章节中，我们看到<strong>注意力机制（Attention）</strong> 让模型在编码与解码之间建立了“对齐”关系：Decoder 每一步都可以选择性地关注 Encoder 的不同部分。然而，随着任务复杂度提升，研究者们开始思考——<strong>如果序列内部的不同位置也能彼此关注，会怎样？</strong> 这正是 <strong>Self-Attention（自注意力机制）</strong> 的出发点。</p>
<h3 id="4-1-从“对齐他人”到“对齐自己”"><a href="#4-1-从“对齐他人”到“对齐自己”" class="headerlink" title="4.1 从“对齐他人”到“对齐自己”"></a>4.1 从“对齐他人”到“对齐自己”</h3><p>传统的 Seq2Seq 注意力机制关注的是<strong>两个序列之间的依赖</strong>：Encoder 提供信息，Decoder 从中选取最相关的部分。而 Self-Attention 则将这种机制<strong>扩展到单个序列内部</strong>——每个位置（单词、token）都能直接“看到”序列中的其他位置，从而捕捉全局关系。</p>
<p>举例来说，句子 “The animal didn’t cross the street because it was too tired.” 在预测 “it” 时，模型需要知道它指的是 “animal” 而不是 “street”。这类依赖跨越较远，传统 RNN 难以保持，而 Self-Attention 能通过内在的“全连接式对齐”轻松捕获。</p>
<p>核心思想是每个词不再只依赖局部上下文，而是通过注意力分布，与序列中所有其他词建立联系。</p>
<h3 id="4-2-Self-Attention-的数学原理"><a href="#4-2-Self-Attention-的数学原理" class="headerlink" title="4.2 Self-Attention 的数学原理"></a>4.2 Self-Attention 的数学原理</h3><p>设输入序列为矩阵 $X \in \mathbb{R}^{T \times d}$，其中 T 是序列长度，d 是嵌入维度。Self-Attention 的计算流程可表示为：$$\text{SA}(X) &#x3D; \text{softmax}\left(\frac{XW_Q(XW_K)^\top}{\sqrt{d_k}}\right)XW_V$$其中：</p>
<ul>
<li>$W_Q, W_K, W_V$ 分别是可学习的 <strong>Query、Key、Value</strong> 映射矩阵；</li>
<li>$Q &#x3D; XW_Q，K &#x3D; XW_K，V &#x3D; XW_V$；</li>
<li>相似度矩阵 $QK^\top$ 表示每个词对其他词的关注程度；</li>
<li>归一化项 $\sqrt{d_k}$ 用于防止内积值过大导致梯度爆炸。</li>
</ul>
<p>最终结果是每个词对其他所有词的加权组合，即：<strong>输出的每个位置，都是对整个输入序列的“重构”与“聚合”。</strong></p>
<h3 id="4-3-缩放项的作用：稳定梯度"><a href="#4-3-缩放项的作用：稳定梯度" class="headerlink" title="4.3 缩放项的作用：稳定梯度"></a>4.3 缩放项的作用：稳定梯度</h3><p>在未缩放的情况下，若 $d_k$ 较大（例如 512），内积 $QK^\top$ 的方差会随维度增大而膨胀。这会导致 softmax 输出趋于极端（过于尖锐或饱和），从而使梯度几乎为零或震荡剧烈。</p>
<p>引入 $1 &#x2F; \sqrt{d_k}$ 的缩放项后，可在数值上将方差重新压回合适范围，从而保持训练稳定。这一看似微小的改动，是 Transformer 架构成功训练的关键之一。</p>
<h3 id="4-4-Self-Attention-的并行优势"><a href="#4-4-Self-Attention-的并行优势" class="headerlink" title="4.4 Self-Attention 的并行优势"></a>4.4 Self-Attention 的并行优势</h3><p>RNN 需要逐步处理时间步（$O(T)$），而 Self-Attention 能同时计算所有位置间的相关性，因此整体计算复杂度为 $O(T^2)$，但可以<strong>完全并行化</strong>。这意味着：</p>
<ul>
<li>训练速度大幅提升（适合 GPU&#x2F;TPU 批量计算）；</li>
<li>信息传播路径缩短，每个词能直接访问全局上下文；</li>
<li>模型在捕捉长程依赖时更高效。</li>
</ul>
<p>缺点是：当序列过长时（如上千 token），计算与内存消耗会快速增加，这也催生了后续的 <strong>Sparse Attention</strong>、<strong>Linear Attention</strong> 等改进方向。</p>
<h3 id="4-5-Mask-机制：保持时间一致性"><a href="#4-5-Mask-机制：保持时间一致性" class="headerlink" title="4.5 Mask 机制：保持时间一致性"></a>4.5 Mask 机制：保持时间一致性</h3><p>在语言建模或生成式任务（如翻译解码、对话生成）中，模型在预测当前词时<strong>不应该看到未来的词</strong>，否则就破坏了因果性。为此，Self-Attention 中通常引入 <strong>Causal Mask（因果遮罩）</strong>：</p>
<ul>
<li>在 <strong>Encoder</strong> 侧的 self-attention 中，通常是<strong>双向的</strong>：每个位置可以关注整个序列（适合做理解&#x2F;表征学习，比如 BERT）；</li>
<li>在 <strong>Decoder</strong> 或自回归语言模型中，self-attention 则采用<strong>单向因果 Mask</strong>：在计算 $QK^\top$ 时，把“当前时间步之后”的位置的得分强制置为 $-\infty$，softmax 后对应权重为 0。</li>
</ul>
<p>这样，位置  的 Query 只能看到  的 Key&#x2F;Value 信息，这样既能通过 self-attention 访问完整的历史上下文，又保证了生成过程中严格的<strong>时间因果性（causality）</strong>，不会“偷看未来”。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/RutzqG.png" width="70%" /></p>
<center>图 4-1 Self-Attention 词间关系矩阵示意图</center>

<p>该图展示了句子 <strong>“The cat sat on the mat”</strong> 在 <strong>自注意力（Self-Attention）机制</strong> 下的词间关注关系。横轴与纵轴分别代表相同序列中的词（列为 <em>Key</em>，行为 <em>Query</em>），每个格子的颜色深浅表示某个词在计算自身表示时对另一词的关注权重（Attention Weight）。从图中可以观察到：</p>
<ul>
<li>对角线部分颜色较深，说明各词倾向于自关注（Self-Focus），这是模型基础的稳定特征。</li>
<li>“cat” 对 “sat” 的关注度较高，反映出语义上的动作-主体联系；</li>
<li>“on”“the”“mat” 之间也形成较强关联，对应介词短语的内部依赖。</li>
</ul>
<p>这种词间的“软对齐（Soft Alignment）”使得模型能够在内部自动捕捉句法和语义结构，无需显式规则即可建立上下文依赖，这为 Transformer 等架构提供了更强的表达能力。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/DFIUM9.png" width="70%" /></p>
<center>图 4-2 Causal Mask 结构可视化</center>

<p>该图展示了 <strong>自回归语言模型（Auto-Regressive Language Model）</strong> 中的 <strong>因果遮罩机制（Causal Mask）</strong>。矩阵的 <strong>行（Query）</strong> 表示当前生成位置 t，<strong>列（Key）</strong> 表示可被关注的词位置。在每个时间步 t：</p>
<ul>
<li><strong>蓝色区域</strong> 表示模型可以关注的历史信息（包括当前位置自身）；</li>
<li><strong>灰色区域</strong> 表示未来位置，被 Mask 掉以防止模型“偷看”尚未生成的词；</li>
<li><strong>红色虚线框</strong> 展示了示例 t&#x3D;5，此时模型只能看到第 1–5 个位置的内容。</li>
</ul>
<p>这种 <strong>下三角结构（含对角线）</strong> 体现了生成式任务中的 <strong>单向注意力约束（Unidirectional Attention Constraint）</strong>。它确保模型在预测下一个词时，仅依赖过去和当前上下文，从而保持<strong>时间因果性（Causality）和生成一致性</strong>。</p>
<p><strong>小结：从记忆到全局建模</strong><br>Self-Attention 的提出，是深度学习从“时间依赖”到“结构依赖”的重大转变。它不再沿时间顺序传播信息，而是构建全局关系图。Self-Attention 不仅解决了长程依赖问题，也为后来的 <strong>Transformer</strong> 模型奠定了核心基石。从此，网络的“记忆”不再依赖循环，而由<strong>注意力矩阵</strong>来表达序列的内部结构。</p>
<h2 id="5-Transformer：并行化的全局建模架构"><a href="#5-Transformer：并行化的全局建模架构" class="headerlink" title="5. Transformer：并行化的全局建模架构"></a>5. Transformer：并行化的全局建模架构</h2><p>当 Self-Attention 解决了序列中远距离依赖的问题后，一个更脑洞的想法随之诞生——如果我们完全去掉循环（RNN）与卷积（CNN），只用注意力机制，能否建立一个端到端的强大模型？2017 年，Vaswani 等人提出了 <strong>Transformer</strong> 架构，用一句话概括它的核心思想：<strong>“Attention is All You Need.”</strong></p>
<p>Transformer 通过纯注意力机制实现了全局信息建模，并通过高度的并行化，使训练效率与性能同时达到新的高度。</p>
<h3 id="5-1-模型总览：Encoder-Decoder-堆叠结构"><a href="#5-1-模型总览：Encoder-Decoder-堆叠结构" class="headerlink" title="5.1 模型总览：Encoder-Decoder 堆叠结构"></a>5.1 模型总览：Encoder-Decoder 堆叠结构</h3><p>Transformer 延续了 Seq2Seq 的基本思路，整体仍由 <strong>Encoder</strong> 与 <strong>Decoder</strong> 两部分组成：</p>
<ul>
<li><strong>Encoder：</strong> 负责对输入序列进行表征建模；</li>
<li><strong>Decoder：</strong> 根据编码结果逐步生成输出序列。</li>
</ul>
<p>不同于传统 RNN 的串行结构，Transformer 将 Encoder 与 Decoder 拆分为若干个可堆叠的模块（Blocks），每个模块都包含相同的内部结构，使得整个模型层次清晰、可并行计算。</p>
<h3 id="5-2-Encoder-结构：并行建模与层内归一化"><a href="#5-2-Encoder-结构：并行建模与层内归一化" class="headerlink" title="5.2 Encoder 结构：并行建模与层内归一化"></a>5.2 Encoder 结构：并行建模与层内归一化</h3><p>一个典型的 Transformer Encoder Block 包含三个核心组件：</p>
<ol>
<li><strong>Multi-Head Self-Attention（多头自注意力）</strong><br> 让每个词同时从多个“视角”关注整个序列。每个头（head）通过独立的 $W_Q, W_K, W_V$ 学习不同类型的关系：语义、语法或位置依赖。所有头的输出被拼接（concatenate）后，再经线性变换整合为统一表示。$\text{MultiHead}(Q,K,V) &#x3D; \text{Concat}(\text{head}_1, …, \text{head}_h)W_O$ ，其中每个 $\text{head}_i &#x3D; \text{Attention}(QW_Q^i, KW_K^i, VW_V^i)$</li>
<li><strong>Position-wise Feed Forward（逐位置前馈网络）</strong><br> 在每个位置独立地进行两层全连接变换：$\text{FFN}(x) &#x3D; \text{ReLU}(xW_1 + b_1)W_2 + b_2$，作用相当于在每个 token 上局部非线性变换，使表示更具表达力。</li>
<li><strong>Add &amp; Norm（残差连接与层归一化）</strong><br> 每个子层（Self-Attention 或 FFN）都通过残差连接与 LayerNorm 保持梯度稳定：$\text{output} &#x3D; \text{LayerNorm}(x + \text{Sublayer}(x))$，这种结构让信息可以直接跨层流动，减少梯度消失问题，与 ResNet 的思想一脉相承。</li>
</ol>
<img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/qfWHXc.png" width="30%" />

<center>图 5-1 Transformer Encoder Block 结构示意图</center>

<p>该图展示了 <strong>Transformer 编码器（Encoder Block）</strong> 的内部结构及信息流动方向。整体结构自下而上依次包含：</p>
<ol>
<li><strong>输入向量（Input Embedding &#x2F; Hidden States）</strong> —— 表示来自前一层或词嵌入的输入特征；</li>
<li><strong>多头自注意力层（Multi-Head Self-Attention）</strong> —— 通过多个注意力头并行建模不同维度的特征依赖，获取全局上下文关系；</li>
<li><strong>残差连接与层归一化（Add &amp; Norm）</strong> —— 将输入与注意力输出相加并归一化，保持信息流动与训练稳定；</li>
<li><strong>前馈网络（Feed Forward Network, FFN）</strong> —— 对每个位置独立进行两层非线性变换，增强特征表达能力；</li>
<li><strong>再次的残差连接与层归一化（Add &amp; Norm）</strong> —— 输出稳定的上下文表示，用于后续编码层堆叠。</li>
</ol>
<p>该模块是 Transformer 的基本单元，多个 Encoder Block 堆叠后即可实现高效的 <strong>全局依赖建模</strong> 与 <strong>深层语义抽象</strong>。</p>
<h3 id="5-3-Decoder-结构：Masked-Attention-Encoder-Decoder-Attention"><a href="#5-3-Decoder-结构：Masked-Attention-Encoder-Decoder-Attention" class="headerlink" title="5.3 Decoder 结构：Masked Attention + Encoder-Decoder Attention"></a>5.3 Decoder 结构：Masked Attention + Encoder-Decoder Attention</h3><p>Decoder 的设计在 Encoder 基础上做了两处关键改动：</p>
<ol>
<li><strong>Masked Multi-Head Attention（掩码自注意力）</strong><br> 用 <strong>Causal Mask</strong> 限制每个位置只能关注自己及之前的词，防止“看见未来”。</li>
<li><strong>Encoder-Decoder Attention（跨层注意力）</strong><br> 让 Decoder 能基于 Encoder 的输出向量进行查询，从而“对齐输入语义”。数学上等价于 Attention(Q&#x3D;Decoder, K&#x3D;V&#x3D;Encoder)。</li>
</ol>
<p>这两部分与 Feed Forward 层同样通过残差与归一化组成完整的 Decoder Block。最终输出层则接一个线性映射与 Softmax，用于生成概率分布。</p>
<h3 id="5-4-位置编码：让模型“感知顺序”"><a href="#5-4-位置编码：让模型“感知顺序”" class="headerlink" title="5.4 位置编码：让模型“感知顺序”"></a>5.4 位置编码：让模型“感知顺序”</h3><p>由于 Transformer 完全去除了循环结构，模型本身无法区分序列的位置信息。因此，需要引入<strong>位置编码（Positional Encoding）</strong>，在输入嵌入中加入与位置相关的固定或可学习向量。一种常用形式是基于正弦函数的周期编码：<br>$$PE_{(pos, 2i)} &#x3D; \sin\left(\frac{pos}{10000^{2i&#x2F;d}}\right), \quad PE_{(pos, 2i+1)} &#x3D; \cos\left(\frac{pos}{10000^{2i&#x2F;d}}\right)$$<br>它允许模型以几何方式捕捉相对位置信息，这种基于正弦&#x2F;余弦的编码本质上是一种<strong>绝对位置编码</strong>：每个位置 $pos$ 都对应一个固定的向量。但由于不同频率的正弦波在叠加后具有良好的组合性质，模型在学习时可以很方便地从这些绝对编码中<strong>推断出相对位置信息</strong>（例如“相隔多少步”）。直观地看，可以把位置编码理解成在每个 token 上打上一个“节拍标签”，让模型在完全并行计算的前提下，依然能够区分“谁在前、谁在后”。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/gVAa5V.png" width="50%" /></p>
<center>图 5-2 多头注意力的多视角示意图</center>

<p>该图展示了 Transformer 中 <strong>多头注意力机制（Multi-Head Attention）</strong> 的基本结构。输入序列的 <strong>Query（Q）</strong>、<strong>Key（K）</strong> 和 <strong>Value（V）</strong> 首先分别经过线性变换，被分成多个子空间以形成多个“注意力头（heads）”。每个注意力头独立执行 <strong>缩放点积注意力（Scaled Dot-Product Attention）</strong>，即通过计算 $\mathrm{softmax}(QK^T &#x2F; \sqrt{d_k})V$ 来捕捉不同类型的依赖关系。这些头可被视为从不同角度关注输入序列的特征：有的捕捉语法结构（如主谓关系），有的关注语义联系（如指代或同义）。随后，各头输出的上下文向量被 <strong>拼接（Concat）</strong> 并再次通过线性层整合，形成更丰富、更全面的表示。这种多视角并行建模的机制，使 Transformer 能够在多个子空间中同时理解上下文信息，从而显著增强模型的表达能力与泛化性。</p>
<h3 id="5-5-Transformer-的优势与启示"><a href="#5-5-Transformer-的优势与启示" class="headerlink" title="5.5 Transformer 的优势与启示"></a>5.5 Transformer 的优势与启示</h3><p>Transformer 的设计兼具<strong>结构优雅</strong>与<strong>计算高效</strong>：</p>
<ul>
<li><strong>并行计算：</strong> Self-Attention 可同时计算所有位置间关系；</li>
<li><strong>长程建模：</strong> 任意两词可直接交互，无需递归传递；</li>
<li><strong>残差与归一化：</strong> 保证深层网络的训练稳定；</li>
<li><strong>多头机制：</strong> 让模型在多个表示子空间中学习不同的关系模式。</li>
</ul>
<p>正是这些设计，使 Transformer 成为 NLP、CV、语音等多个领域的统一架构。从“时间递推”到“全局建模”，Transformer 不仅改变了神经网络的结构范式，也重塑了深度学习的认知方式。</p>
<p><strong>小结</strong><br>Transformer 通过多头注意力与层级堆叠，彻底实现了<strong>信息交互的并行化与全局化</strong>。其成功不在于复杂，而在于统一：<strong>统一的注意力机制、统一的层结构、统一的训练范式。</strong> 它既是 Seq2Seq 的继承者，又是后续 BERT、GPT、ViT 等一切基于注意力架构的前身。</p>
<h2 id="6-复杂度与性能：O-T-vs-O-T²-的新平衡"><a href="#6-复杂度与性能：O-T-vs-O-T²-的新平衡" class="headerlink" title="6. 复杂度与性能：O(T) vs O(T²) 的新平衡"></a>6. 复杂度与性能：O(T) vs O(T²) 的新平衡</h2><p>Transformer 的成功不仅在于性能突破，也在于对计算效率与模型表达力的重新权衡。RNN 与 Transformer 分别代表了两种极端的计算范式——<strong>一个以时间递推为核心（O(T)），一个以全局并行为核心（O(T²)）。</strong> 要理解两者的差异，我们必须回到算法复杂度的根本。</p>
<h3 id="6-1-RNN：线性复杂度的串行瓶颈"><a href="#6-1-RNN：线性复杂度的串行瓶颈" class="headerlink" title="6.1 RNN：线性复杂度的串行瓶颈"></a>6.1 RNN：线性复杂度的串行瓶颈</h3><p>在循环神经网络（RNN）中，序列被逐步处理：第 $t$ 个时刻的输出依赖于第 $t-1$ 个隐状态。这意味着每一步都必须等待上一步完成，<strong>无法并行展开</strong>。时间复杂度为：$O(T)$</p>
<p>虽然计算量与序列长度成线性关系，但由于依赖链过长，RNN 在现代硬件（GPU&#x2F;TPU）上利用率极低。尤其在长序列任务中，梯度传播困难、训练时间漫长。可以这么理解，RNN 就像“接力跑”，每一棒必须等前一棒跑完。即使单次计算不多，整体效率仍受限于顺序依赖。</p>
<h3 id="6-2-Transformer：平方复杂度的并行革命"><a href="#6-2-Transformer：平方复杂度的并行革命" class="headerlink" title="6.2 Transformer：平方复杂度的并行革命"></a>6.2 Transformer：平方复杂度的并行革命</h3><p>Transformer 通过 Self-Attention 实现了所有位置间的全连接交互。在每个 Attention 层中，需要计算 Query 与 Key 的两两相似度矩阵：$$QK^\top \in \mathbb{R}^{T \times T}$$这一步的复杂度为 $O(T^2)$，显存占用同样为平方级。理论上更“昂贵”，但 Transformer 的关键在于——<strong>所有这些计算可同时完成</strong>。也就是说，虽然计算量更大，但<strong>训练速度反而更快</strong>，因为它充分利用了 GPU 的并行矩阵运算能力。Transformer 像是一场“团队协作”的头脑风暴，所有人可以同时交流；RNN 则是一场“单线程”的传话游戏。</p>
<h3 id="6-3-复杂度对比：计算与存储"><a href="#6-3-复杂度对比：计算与存储" class="headerlink" title="6.3 复杂度对比：计算与存储"></a>6.3 复杂度对比：计算与存储</h3><table>
<thead>
<tr>
<th><strong>模型</strong></th>
<th><strong>时间复杂度</strong></th>
<th><strong>并行性</strong></th>
<th><strong>显存占用</strong></th>
<th><strong>长序列表现</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>RNN</strong></td>
<td>O(T)</td>
<td>低（顺序执行）</td>
<td>低</td>
<td>差（梯度消失）</td>
</tr>
<tr>
<td><strong>Transformer</strong></td>
<td>O(T²)</td>
<td>高（全局并行）</td>
<td>高</td>
<td>优（捕获全局依赖）</td>
</tr>
</tbody></table>
<p>Transformer 在“并行性”上彻底碾压 RNN，但代价是显存消耗与时间复杂度成平方增长。因此，在极长序列任务（如视频、DNA、长文本）中，<strong>标准 Transformer 的成本非常高</strong>。</p>
<p>在这里我们刻意只突出对<strong>序列长度 $T$ 的依赖关系</strong>，为了便于比较，把隐藏维度 $d$、注意力头数等常数因子都省略掉。更精细的复杂度分析会写成 RNN 的 $O(T d^2)$、Transformer 的 $O(T^2 d + T d^2)$ 等，但在“长序列 vs 并行性”的讨论中，这里用简化的 O(T) &#x2F; O(T²) 记号已经足以揭示两者在随 $T$ 增长时的本质差异。</p>
<h3 id="6-4-新的平衡：长序列建模的改进方向"><a href="#6-4-新的平衡：长序列建模的改进方向" class="headerlink" title="6.4 新的平衡：长序列建模的改进方向"></a>6.4 新的平衡：长序列建模的改进方向</h3><p>为应对这一挑战，研究者提出了多种改进变体，核心目标都是在保留全局感受的同时<strong>降低复杂度</strong>。</p>
<h4 id="6-4-1-Linear-Attention"><a href="#6-4-1-Linear-Attention" class="headerlink" title="6.4.1  Linear Attention"></a>6.4.1  Linear Attention</h4><p>将注意力的 softmax(QKᵀ) 改写为核函数形式，使计算可分解为线性复杂度。复杂度：$O(T \cdot d)$，适合超长输入。  </p>
<h4 id="6-4-2-Longformer-BigBird"><a href="#6-4-2-Longformer-BigBird" class="headerlink" title="6.4.2 Longformer &#x2F; BigBird"></a>6.4.2 Longformer &#x2F; BigBird</h4><p>引入“<strong>稀疏注意力（Sparse Attention）</strong>”，仅在局部窗口或部分全局位置计算注意力。能兼顾局部依赖与长程建模。</p>
<h4 id="6-4-3-Performer"><a href="#6-4-3-Performer" class="headerlink" title="6.4.3 Performer"></a>6.4.3 Performer</h4><p>使用随机特征近似 softmax 函数，保持近似全局表达的同时降低内存成本。这些方法的共同目标是：<strong>在 O(T) 与 O(T²) 之间找到更优的平衡点。</strong><br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/qKDAFa.png" width="80%" /></p>
<center>图 6-1 时间复杂度与显存占用对比图</center>

<p>如上图<strong>RNN 曲线（绿色）</strong> 随序列长度呈近似线性增长 O(T)，因其每个时间步的计算仅依赖前一状态，虽然难以并行，但复杂度增长较平缓；<strong>Transformer 曲线（红色）</strong> 呈平方增长 O($T^2$)，因为自注意力机制在每层需要计算所有位置两两之间的注意力权重，导致计算量与显存需求在长序列情况下迅速攀升。<br>该图直观反映了两类结构在 <strong>计算复杂度层面</strong> 的本质区别：RNN 计算受时间依赖限制但资源消耗较低，而 Transformer 虽能全局并行建模，但在长序列处理上代价更高。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/Xg0WWB.png" width="80%" /></p>
<center>图 6-2 不同序列长度下的训练速度曲线</center>

<p>该图对比了 <strong>RNN</strong> 与 <strong>Transformer</strong> 在不同序列长度下的训练效率变化趋势。可以观察到：</p>
<ul>
<li><strong>RNN（绿色曲线）</strong> 的训练时间随着序列长度 T 近似线性上升。由于其结构上需要按时间步顺序迭代，无法实现并行化计算，因此在长序列场景下训练速度显著下降。</li>
<li><strong>Transformer（红色曲线）</strong> 在中短序列范围内保持相对稳定的训练时间，得益于其全局自注意力机制可在时间步间并行计算；但当序列极长时，注意力计算的 O(T^2) 复杂度使训练速度开始下降。</li>
</ul>
<p>该图直观展示了两种结构在 <strong>计算并行性与长序列代价</strong> 之间的权衡关系：Transformer 在中短序列任务上具有显著效率优势，而 RNN 在资源受限或较短序列任务中仍具竞争力。</p>
<p><strong>小结：效率与表达力的抉择</strong><br>RNN 与 Transformer 代表了两种极端思路：</p>
<ul>
<li>RNN 在时间维度高效，但缺乏并行性；</li>
<li>Transformer 全局建模强大，却带来计算负担。</li>
</ul>
<p>现代研究正致力于融合两者的优点，让模型既能“看得远”，又能“算得快”。</p>
<h2 id="7-比较-LSTM-与-Transformer-在电商销量预测中的表现"><a href="#7-比较-LSTM-与-Transformer-在电商销量预测中的表现" class="headerlink" title="7. 比较 LSTM 与 Transformer 在电商销量预测中的表现"></a>7. 比较 LSTM 与 Transformer 在电商销量预测中的表现</h2><h3 id="7-1-实验背景与目标"><a href="#7-1-实验背景与目标" class="headerlink" title="7.1 实验背景与目标"></a>7.1 实验背景与目标</h3><p>在时间序列预测领域，电商销量是一个典型且具有商业价值的任务。不同的神经网络结构在捕捉季节性、趋势性以及节假日波动时表现各异。</p>
<p>本实验通过对比 <strong>LSTM（长短期记忆网络）</strong> 与 <strong>Transformer（基于自注意力的序列模型）</strong>，探讨它们在<strong>销量预测任务</strong>中的建模差异与性能权衡。本实验不试图“证明某一结构一定更优”，而是希望在一个真实电商销量序列上，<strong>观察 LSTM 与 Transformer 在相同特征、相似参数规模下的建模差异</strong>，包括：</p>
<ul>
<li>在中长期趋势与季节性建模上，二者的表现有何不同？</li>
<li>在训练稳定性与误差分布上，各自呈现出怎样的特征？</li>
</ul>
<h3 id="7-2-实验设置"><a href="#7-2-实验设置" class="headerlink" title="7.2 实验设置"></a>7.2 实验设置</h3><h4 id="7-2-1-数据与特征"><a href="#7-2-1-数据与特征" class="headerlink" title="7.2.1 数据与特征"></a>7.2.1 数据与特征</h4><p>数据集：使用 <a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/dataset/352/online+retail"><strong>UCI Online Retail（2010–2011 英国零售销售记录）</strong></a>。从中提取英国的每日销售额，并按周聚合，构建一个典型的销量时间序列。特征工程包括：</p>
<ul>
<li><strong>基础时间特征</strong>：星期、月份、是否周末；</li>
<li><strong>傅里叶季节特征</strong>：周周期与年周期；</li>
<li><strong>滞后与滑动窗口特征</strong>：lag1, lag2, rolling4；</li>
<li><strong>目标变量</strong>：每周销售额（非对数形式）。</li>
</ul>
<p>需要特别说明的是：在采用周频聚合之后，可用样本长度相对较短，因此本实验更接近一个<strong>性质的对比示例</strong>，用于展示两类模型在相同管线下的行为模式，而不是追求在该数据集上给出具有统计显著性的“最优”结论。</p>
<p>数据按时间顺序划分为训练集（70%）、验证集（15%）与测试集（15%）。</p>
<h4 id="7-2-2-模型结构"><a href="#7-2-2-模型结构" class="headerlink" title="7.2.2 模型结构"></a>7.2.2 模型结构</h4><p><strong>模型 1：LSTM（局部记忆型）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim=<span class="number">192</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 多层 LSTM 堆叠</span></span><br><span class="line">        <span class="variable language_">self</span>.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,</span><br><span class="line">                            batch_first=<span class="literal">True</span>, dropout=dropout)</span><br><span class="line">        <span class="comment"># 仅取最后一步输出预测下一时刻销售额</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out, _ = <span class="variable language_">self</span>.lstm(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(out[:, -<span class="number">1</span>, :])</span><br></pre></td></tr></table></figure>
<p><strong>模型 2：Transformer（全局注意力型）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, d_model=<span class="number">256</span>, nhead=<span class="number">8</span>, num_layers=<span class="number">3</span>, ff=<span class="number">512</span>, dropout=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 线性升维到 d_model 维</span></span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(input_dim, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.in_norm = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.pe = PositionalEncoding(d_model)   <span class="comment"># 加入位置信息</span></span><br><span class="line">        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,</span><br><span class="line">                                         dim_feedforward=ff, dropout=dropout,</span><br><span class="line">                                         batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.TransformerEncoder(enc, num_layers=num_layers,</span><br><span class="line">                                             norm=nn.LayerNorm(d_model))</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.in_norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.pe(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x[:, -<span class="number">1</span>, :])</span><br></pre></td></tr></table></figure>
<h4 id="7-2-3-训练与评估"><a href="#7-2-3-训练与评估" class="headerlink" title="7.2.3 训练与评估"></a>7.2.3 训练与评估</h4><table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>LSTM</strong></th>
<th><strong>Transformer</strong></th>
</tr>
</thead>
<tbody><tr>
<td>批量大小</td>
<td>64</td>
<td>64</td>
</tr>
<tr>
<td>窗口长度</td>
<td>60</td>
<td>60</td>
</tr>
<tr>
<td>优化器</td>
<td>AdamW</td>
<td>AdamW</td>
</tr>
<tr>
<td>损失函数</td>
<td>HuberLoss</td>
<td>HuberLoss</td>
</tr>
<tr>
<td>调度策略</td>
<td>ReduceLROnPlateau</td>
<td>Warmup + Cosine Annealing</td>
</tr>
<tr>
<td>评价指标</td>
<td>MAE &#x2F; MAPE &#x2F; sMAPE &#x2F; Weighted MAE</td>
<td>同左</td>
</tr>
</tbody></table>
<p>其中 <strong>Weighted MAE（WMAE）</strong> 是我们根据电商场景定制的一个加权指标：在实际业务中，高销量周往往更重要——预测在这些时间段的误差，会对营收、库存、物流带来更大影响。因此，我们按照“销量相对于中位数的比例”给每个样本一个权重，并在区间 <code>[1, 5]</code> 内截断，以避免少数极端峰值完全主导整体指标。这样得到的 WMAE 既能反映整体误差水平，又适度强调了高销量区间的预测质量。</p>
<h4 id="7-2-4-完整实验代码"><a href="#7-2-4-完整实验代码" class="headerlink" title="7.2.4 完整实验代码"></a>7.2.4 完整实验代码</h4><p>运行说明：</p>
<ul>
<li>直接保存为 exp_ch7_retail.py 并运行；</li>
<li>若首次运行下载失败，请手动下载 Online_Retail.xlsx 放在脚本目录；</li>
<li>自动生成 3 张图（验证曲线、预测趋势、误差随时间）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">UCI Online Retail — LSTM vs Transformer (v4)  </span></span><br><span class="line"><span class="string">目标：在同一条周频销量序列上，对比 LSTM 与 Transformer 的建模行为</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>  </span><br><span class="line"><span class="keyword">import</span> os  </span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path  </span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve  </span><br><span class="line"><span class="keyword">import</span> urllib.error  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> matplotlib  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> font_manager  </span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader  </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 0) 全局 &amp; 字体  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line">SEED = <span class="number">42</span>  </span><br><span class="line">np.random.seed(SEED)  </span><br><span class="line">torch.manual_seed(SEED)  </span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[INFO] device: <span class="subst">&#123;device&#125;</span>&quot;</span>)  </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment"># ========= 中文字体（稳妥版，可按需修改） =========</span></span><br><span class="line">candidate_fonts = [  </span><br><span class="line">    <span class="string">&quot;SimHei&quot;</span>, <span class="string">&quot;Noto Sans CJK SC&quot;</span>, <span class="string">&quot;Microsoft YaHei&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;PingFang SC&quot;</span>, <span class="string">&quot;Hiragino Sans GB&quot;</span>, <span class="string">&quot;Heiti SC&quot;</span>  </span><br><span class="line">]  </span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 处理坐标轴负号  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_chinese_font</span>():  </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> candidate_fonts:  </span><br><span class="line">        <span class="keyword">try</span>:  </span><br><span class="line">            plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [f]  </span><br><span class="line">            font_manager.findfont(f, fallback_to_default=<span class="literal">False</span>)  </span><br><span class="line">            <span class="keyword">return</span>  </span><br><span class="line">        <span class="keyword">except</span> Exception:  </span><br><span class="line">            <span class="keyword">pass</span>  </span><br><span class="line">set_chinese_font()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 1) 下载 / 本地数据  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line">DATA_URL_XLSX = <span class="string">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx&quot;</span>  </span><br><span class="line">DATA_LOCAL = Path(<span class="string">&quot;Online_Retail.xlsx&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ensure_dataset</span>():  </span><br><span class="line">    <span class="keyword">if</span> DATA_LOCAL.exists():  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[INFO] 本地已存在：<span class="subst">&#123;DATA_LOCAL.resolve()&#125;</span>&quot;</span>)  </span><br><span class="line">        <span class="keyword">return</span>  </span><br><span class="line">    <span class="keyword">try</span>:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] 正在从 UCI 下载数据集 ...&quot;</span>)  </span><br><span class="line">        urlretrieve(DATA_URL_XLSX, DATA_LOCAL.as_posix())  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] 下载完成&quot;</span>)  </span><br><span class="line">    <span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[WARNING] 下载失败：&quot;</span>, e)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;请手动下载并放到当前目录，文件名：Online_Retail.xlsx&quot;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;链接：&quot;</span>, DATA_URL_XLSX)  </span><br><span class="line">        <span class="keyword">raise</span> SystemExit(<span class="string">&quot;[EXIT] 未找到数据文件，程序终止。&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 2) 读取与清洗（周频UK + IQR）  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_and_prepare</span>():  </span><br><span class="line">    ensure_dataset()  </span><br><span class="line">    df_raw = pd.read_excel(DATA_LOCAL.as_posix())  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 去退货/负值/缺失 + 仅 UK    </span></span><br><span class="line">    df = (df_raw[~df_raw[<span class="string">&quot;InvoiceNo&quot;</span>].astype(<span class="built_in">str</span>).<span class="built_in">str</span>.startswith(<span class="string">&quot;C&quot;</span>)]  </span><br><span class="line">          .dropna(subset=[<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>, <span class="string">&quot;Country&quot;</span>]))  </span><br><span class="line">    df = df[(df[<span class="string">&quot;Quantity&quot;</span>] &gt; <span class="number">0</span>) &amp; (df[<span class="string">&quot;UnitPrice&quot;</span>] &gt; <span class="number">0</span>)]  </span><br><span class="line">    df = df[df[<span class="string">&quot;Country&quot;</span>] == <span class="string">&quot;United Kingdom&quot;</span>]  </span><br><span class="line">  </span><br><span class="line">    df[<span class="string">&quot;date&quot;</span>] = df[<span class="string">&quot;InvoiceDate&quot;</span>].dt.date  </span><br><span class="line">    df[<span class="string">&quot;sales&quot;</span>] = df[<span class="string">&quot;Quantity&quot;</span>] * df[<span class="string">&quot;UnitPrice&quot;</span>]  </span><br><span class="line">    daily = df.groupby(<span class="string">&quot;date&quot;</span>)[<span class="string">&quot;sales&quot;</span>].<span class="built_in">sum</span>().to_frame()  </span><br><span class="line">    daily.index = pd.to_datetime(daily.index)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 周频（周五），并做 IQR 裁剪极端值（winsorize）  </span></span><br><span class="line">    weekly = daily.resample(<span class="string">&quot;W-FRI&quot;</span>).<span class="built_in">sum</span>()  </span><br><span class="line">    q1, q3 = weekly[<span class="string">&quot;sales&quot;</span>].quantile([<span class="number">0.25</span>, <span class="number">0.75</span>])  </span><br><span class="line">    iqr = q3 - q1  </span><br><span class="line">    low, high = <span class="built_in">max</span>(<span class="number">0</span>, q1 - <span class="number">3</span>*iqr), q3 + <span class="number">3</span>*iqr  </span><br><span class="line">    weekly[<span class="string">&quot;sales&quot;</span>] = weekly[<span class="string">&quot;sales&quot;</span>].clip(low, high)  </span><br><span class="line">    daily = weekly  <span class="comment"># 后续继续用 daily  </span></span><br><span class="line">    <span class="comment"># ===== 外生特征（基础周期） =====    </span></span><br><span class="line">    dti = daily.index  </span><br><span class="line">    dow = dti.weekday  </span><br><span class="line">    mon = dti.month  </span><br><span class="line">    daily[<span class="string">&quot;weekday_sin&quot;</span>] = np.sin(<span class="number">2</span>*np.pi * dow / <span class="number">7.0</span>)  </span><br><span class="line">    daily[<span class="string">&quot;weekday_cos&quot;</span>] = np.cos(<span class="number">2</span>*np.pi * dow / <span class="number">7.0</span>)  </span><br><span class="line">    daily[<span class="string">&quot;month_sin&quot;</span>]   = np.sin(<span class="number">2</span>*np.pi * mon / <span class="number">12.0</span>)  </span><br><span class="line">    daily[<span class="string">&quot;month_cos&quot;</span>]   = np.cos(<span class="number">2</span>*np.pi * mon / <span class="number">12.0</span>)  </span><br><span class="line">    daily[<span class="string">&quot;is_weekend&quot;</span>]  = (dow &gt;= <span class="number">5</span>).astype(<span class="string">&quot;float32&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ===== 傅里叶季节特征（周/年）k=1..3 =====  </span></span><br><span class="line">    t = np.arange(<span class="built_in">len</span>(dti))  </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>):  </span><br><span class="line">        daily[<span class="string">f&quot;fourier_w_sin_<span class="subst">&#123;k&#125;</span>&quot;</span>] = np.sin(<span class="number">2</span>*np.pi*k*t/<span class="number">7.0</span>)  </span><br><span class="line">        daily[<span class="string">f&quot;fourier_w_cos_<span class="subst">&#123;k&#125;</span>&quot;</span>] = np.cos(<span class="number">2</span>*np.pi*k*t/<span class="number">7.0</span>)  </span><br><span class="line">        daily[<span class="string">f&quot;fourier_y_sin_<span class="subst">&#123;k&#125;</span>&quot;</span>] = np.sin(<span class="number">2</span>*np.pi*k*t/<span class="number">52.0</span>)  </span><br><span class="line">        daily[<span class="string">f&quot;fourier_y_cos_<span class="subst">&#123;k&#125;</span>&quot;</span>] = np.cos(<span class="number">2</span>*np.pi*k*t/<span class="number">52.0</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ===== 周频 lag / rolling =====    </span></span><br><span class="line">    daily[<span class="string">&quot;lag1&quot;</span>] = daily[<span class="string">&quot;sales&quot;</span>].shift(<span class="number">1</span>)  </span><br><span class="line">    daily[<span class="string">&quot;lag2&quot;</span>] = daily[<span class="string">&quot;sales&quot;</span>].shift(<span class="number">2</span>)  </span><br><span class="line">    daily[<span class="string">&quot;rolling4&quot;</span>] = daily[<span class="string">&quot;sales&quot;</span>].rolling(<span class="number">4</span>).mean()  </span><br><span class="line">  </span><br><span class="line">    daily = daily.dropna().reset_index().rename(columns=&#123;<span class="string">&quot;index&quot;</span>: <span class="string">&quot;date&quot;</span>&#125;)  </span><br><span class="line">    <span class="keyword">return</span> daily  <span class="comment"># 包含：sales + 多个外生  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 3) 滑窗  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_dataset_multi</span>(<span class="params">y_arr, feat_arr, window=<span class="number">180</span></span>):  </span><br><span class="line">    X, y = [], []  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_arr) - window):  </span><br><span class="line">        y_win = y_arr[i:i+window].reshape(-<span class="number">1</span>, <span class="number">1</span>)     <span class="comment"># 过去目标也作为特征  </span></span><br><span class="line">        f_win = feat_arr[i:i+window, :]  </span><br><span class="line">        X.append(np.concatenate([y_win, f_win], axis=<span class="number">1</span>))  <span class="comment"># [T, 1+F]  </span></span><br><span class="line">        y.append(y_arr[i+window])  </span><br><span class="line">    <span class="keyword">return</span> np.asarray(X, np.float32), np.asarray(y, np.float32)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_windows</span>(<span class="params">daily, window=<span class="number">180</span></span>):  </span><br><span class="line">    n = <span class="built_in">len</span>(daily)  </span><br><span class="line">    i1, i2 = <span class="built_in">int</span>(n*<span class="number">0.70</span>), <span class="built_in">int</span>(n*<span class="number">0.85</span>)  </span><br><span class="line">  </span><br><span class="line">    y_all = daily[<span class="string">&quot;sales&quot;</span>].values.astype(<span class="string">&quot;float32&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    feat_cols = [<span class="string">&quot;weekday_sin&quot;</span>,<span class="string">&quot;weekday_cos&quot;</span>,<span class="string">&quot;month_sin&quot;</span>,<span class="string">&quot;month_cos&quot;</span>,<span class="string">&quot;is_weekend&quot;</span>,  </span><br><span class="line">                 <span class="string">&quot;lag1&quot;</span>,<span class="string">&quot;lag2&quot;</span>,<span class="string">&quot;rolling4&quot;</span>] + \  </span><br><span class="line">                [<span class="string">f&quot;<span class="subst">&#123;p&#125;</span>_<span class="subst">&#123;k&#125;</span>&quot;</span> <span class="keyword">for</span> p <span class="keyword">in</span> (<span class="string">&quot;fourier_w_sin&quot;</span>,<span class="string">&quot;fourier_w_cos&quot;</span>,<span class="string">&quot;fourier_y_sin&quot;</span>,<span class="string">&quot;fourier_y_cos&quot;</span>)  </span><br><span class="line">                 <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">4</span>)]  </span><br><span class="line">    feats_all = daily[feat_cols].values.astype(<span class="string">&quot;float32&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练段有 i1 个样本，滑窗需要 window &lt; i1 才能产生样本  </span></span><br><span class="line">    max_win_train = <span class="built_in">max</span>(<span class="number">8</span>, i1 - <span class="number">1</span>)  <span class="comment"># 至少保留 8，避免过小  </span></span><br><span class="line">    window_eff = <span class="built_in">min</span>(window, max_win_train)  </span><br><span class="line">    <span class="keyword">if</span> window_eff &lt; window:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[WARN] window=<span class="subst">&#123;window&#125;</span> 对当前序列过大，已自动降为 window=<span class="subst">&#123;window_eff&#125;</span> &quot;</span>  </span><br><span class="line">              <span class="string">f&quot;(训练段长度=<span class="subst">&#123;i1&#125;</span>)&quot;</span>)  </span><br><span class="line">    window = window_eff  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 仅用训练集拟合缩放器  </span></span><br><span class="line">    scaler_y = RobustScaler()  </span><br><span class="line">    y_tr = scaler_y.fit_transform(y_all[:i1].reshape(-<span class="number">1</span>,<span class="number">1</span>)).ravel()  </span><br><span class="line">    y_va = scaler_y.transform(y_all[i1:i2].reshape(-<span class="number">1</span>,<span class="number">1</span>)).ravel()  </span><br><span class="line">    y_te = scaler_y.transform(y_all[i2:].reshape(-<span class="number">1</span>,<span class="number">1</span>)).ravel()  </span><br><span class="line">  </span><br><span class="line">    scaler_x = RobustScaler()  </span><br><span class="line">    feats_tr = scaler_x.fit_transform(feats_all[:i1])  </span><br><span class="line">    feats_va = scaler_x.transform(feats_all[i1:i2])  </span><br><span class="line">    feats_te = scaler_x.transform(feats_all[i2:])  </span><br><span class="line">  </span><br><span class="line">    X_tr, y_tr_t = make_dataset_multi(y_tr, feats_tr, window)  </span><br><span class="line">    X_va, y_va_t = make_dataset_multi(  </span><br><span class="line">        np.r_[y_tr[-window:], y_va],  </span><br><span class="line">        np.vstack([feats_tr[-window:], feats_va]),  </span><br><span class="line">        window  </span><br><span class="line">    )  </span><br><span class="line">    X_te, y_te_t = make_dataset_multi(  </span><br><span class="line">        np.r_[np.r_[y_tr[-window:], y_va][-window:], y_te],  </span><br><span class="line">        np.vstack([np.vstack([feats_tr[-window:], feats_va])[-window:], feats_te]),  </span><br><span class="line">        window  </span><br><span class="line">    )  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_loader</span>(<span class="params">X, y, batch=<span class="number">64</span>, shuffle=<span class="literal">False</span></span>):  </span><br><span class="line">        X_t = torch.tensor(X, dtype=torch.float32).to(device)  </span><br><span class="line">        y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(-<span class="number">1</span>).to(device)  </span><br><span class="line">        <span class="keyword">return</span> DataLoader(TensorDataset(X_t, y_t), batch_size=batch, shuffle=shuffle, drop_last=<span class="literal">False</span>)  </span><br><span class="line">  </span><br><span class="line">    tr_loader = to_loader(X_tr, y_tr_t, batch=<span class="number">64</span>, shuffle=<span class="literal">True</span>)  </span><br><span class="line">    va_loader = to_loader(X_va, y_va_t, batch=<span class="number">128</span>, shuffle=<span class="literal">False</span>)  </span><br><span class="line">    te_loader = to_loader(X_te, y_te_t, batch=<span class="number">128</span>, shuffle=<span class="literal">False</span>)  </span><br><span class="line">    <span class="keyword">return</span> tr_loader, va_loader, te_loader, scaler_y, (<span class="number">1</span> + feats_tr.shape[<span class="number">1</span>])  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 4) 模型  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim=<span class="number">192</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.2</span></span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,  </span><br><span class="line">                            batch_first=<span class="literal">True</span>, dropout=dropout)  </span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_dim, <span class="number">1</span>)  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        out, _ = <span class="variable language_">self</span>.lstm(x)  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(out[:, -<span class="number">1</span>, :])  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">10000</span></span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        pe = torch.zeros(max_len, d_model)  </span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float32).unsqueeze(<span class="number">1</span>)  </span><br><span class="line">        div = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-np.log(<span class="number">10000.0</span>)/d_model))  </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * div)  </span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * div)  </span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;pe&quot;</span>, pe.unsqueeze(<span class="number">0</span>))  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>), :]  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, d_model=<span class="number">256</span>, nhead=<span class="number">8</span>, num_layers=<span class="number">3</span>, ff=<span class="number">512</span>, dropout=<span class="number">0.2</span></span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(input_dim, d_model)  </span><br><span class="line">        <span class="variable language_">self</span>.in_norm = nn.LayerNorm(d_model)  </span><br><span class="line">        <span class="variable language_">self</span>.pe   = PositionalEncoding(d_model)  </span><br><span class="line">        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,  </span><br><span class="line">                                         dim_feedforward=ff, dropout=dropout,  </span><br><span class="line">                                         batch_first=<span class="literal">True</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.TransformerEncoder(enc, num_layers=num_layers, norm=nn.LayerNorm(d_model))  </span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, <span class="number">1</span>)  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)  </span><br><span class="line">        x = <span class="variable language_">self</span>.in_norm(x)  </span><br><span class="line">        x = <span class="variable language_">self</span>.pe(x)  </span><br><span class="line">        x = <span class="variable language_">self</span>.encoder(x)  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x[:, -<span class="number">1</span>, :])  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 5) 评估 &amp; 训练  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mae</span>(<span class="params">a,b</span>): <span class="keyword">return</span> <span class="built_in">float</span>(np.mean(np.<span class="built_in">abs</span>(a-b)))  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mape</span>(<span class="params">a,b,eps=<span class="number">1e-6</span></span>): <span class="keyword">return</span> <span class="built_in">float</span>(np.mean(np.<span class="built_in">abs</span>((a-b)/np.clip(np.<span class="built_in">abs</span>(a),eps,<span class="literal">None</span>)))*<span class="number">100</span>)  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">smape</span>(<span class="params">a,b,eps=<span class="number">1e-6</span></span>): <span class="keyword">return</span> <span class="built_in">float</span>(np.mean(<span class="number">2</span>*np.<span class="built_in">abs</span>(a-b)/(np.<span class="built_in">abs</span>(a)+np.<span class="built_in">abs</span>(b)+eps))*<span class="number">100</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_mae</span>(<span class="params">y_true, y_pred</span>):  </span><br><span class="line">    w = np.clip(y_true / <span class="built_in">max</span>(np.median(y_true), <span class="number">1.0</span>), <span class="number">1.0</span>, <span class="number">5.0</span>)  </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(np.mean(w * np.<span class="built_in">abs</span>(y_true - y_pred)))  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EarlyStopper</span>:  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patience=<span class="number">12</span>, min_delta=<span class="number">50.0</span></span>):  </span><br><span class="line">        <span class="variable language_">self</span>.patience = patience  </span><br><span class="line">        <span class="variable language_">self</span>.min_delta = min_delta  </span><br><span class="line">        <span class="variable language_">self</span>.best = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.count = <span class="number">0</span>  </span><br><span class="line">        <span class="variable language_">self</span>.stop  = <span class="literal">False</span>  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, val</span>):  </span><br><span class="line">        <span class="keyword">if</span> val &lt; <span class="variable language_">self</span>.best - <span class="variable language_">self</span>.min_delta:  </span><br><span class="line">            <span class="variable language_">self</span>.best = val; <span class="variable language_">self</span>.count = <span class="number">0</span>  </span><br><span class="line">        <span class="keyword">else</span>:  </span><br><span class="line">            <span class="variable language_">self</span>.count += <span class="number">1</span>  </span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.count &gt;= <span class="variable language_">self</span>.patience:  </span><br><span class="line">                <span class="variable language_">self</span>.stop = <span class="literal">True</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_val_wmae_orig</span>(<span class="params">model, val_loader, scaler_y</span>):  </span><br><span class="line">    model.<span class="built_in">eval</span>()  </span><br><span class="line">    y_true_s, y_pred_s = [], []  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> val_loader:  </span><br><span class="line">            y_true_s.append(yb.cpu().numpy())  </span><br><span class="line">            y_pred_s.append(model(xb).cpu().numpy())  </span><br><span class="line">    y_true_s = np.vstack(y_true_s); y_pred_s = np.vstack(y_pred_s)  </span><br><span class="line">    y_true = scaler_y.inverse_transform(y_true_s)  </span><br><span class="line">    y_pred = scaler_y.inverse_transform(y_pred_s)  </span><br><span class="line">    <span class="keyword">return</span> weighted_mae(y_true, y_pred)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">model, tr_loader, va_loader, scaler_y, epochs=<span class="number">80</span>, lr=<span class="number">3e-4</span>, warmup=<span class="number">10</span>, use_warm_cos=<span class="literal">False</span></span>):  </span><br><span class="line">    model.to(device)  </span><br><span class="line">    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=<span class="number">1e-4</span>)  </span><br><span class="line">    loss_fn = nn.HuberLoss(delta=<span class="number">1.0</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> use_warm_cos:  </span><br><span class="line">        total_steps = epochs * <span class="built_in">len</span>(tr_loader)  </span><br><span class="line">        warm_steps = <span class="built_in">max</span>(<span class="number">1</span>, warmup * <span class="built_in">len</span>(tr_loader))  </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">lr_lambda</span>(<span class="params">step</span>):  </span><br><span class="line">            <span class="keyword">if</span> step &lt; warm_steps:  </span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">max</span>(<span class="number">1e-3</span>, step / warm_steps)  </span><br><span class="line">            progress = (step - warm_steps) / <span class="built_in">max</span>(<span class="number">1</span>, total_steps - warm_steps)  </span><br><span class="line">            <span class="keyword">return</span> <span class="number">0.5</span>*(<span class="number">1</span> + np.cos(np.pi*progress))*<span class="number">0.9</span> + <span class="number">0.1</span>  </span><br><span class="line">        sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)  </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.5</span>, patience=<span class="number">4</span>)  </span><br><span class="line">  </span><br><span class="line">    es = EarlyStopper(patience=<span class="number">12</span>, min_delta=<span class="number">50.0</span>)  </span><br><span class="line">    hist = []; step = <span class="number">0</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):  </span><br><span class="line">        model.train()  </span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> tr_loader:  </span><br><span class="line">            step += <span class="number">1</span>  </span><br><span class="line">            opt.zero_grad()  </span><br><span class="line">            pred = model(xb)  </span><br><span class="line">            loss = loss_fn(pred, yb)  </span><br><span class="line">            loss.backward()  </span><br><span class="line">            nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)  </span><br><span class="line">            opt.step()  </span><br><span class="line">            <span class="keyword">if</span> use_warm_cos: sched.step()  </span><br><span class="line">  </span><br><span class="line">        val_wmae = _val_wmae_orig(model, va_loader, scaler_y)  </span><br><span class="line">        hist.append(val_wmae)  </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> use_warm_cos: sched.step(val_wmae)  </span><br><span class="line">        <span class="keyword">if</span> e % <span class="number">5</span> == <span class="number">0</span>:  </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;[Epoch <span class="subst">&#123;e:02d&#125;</span>] val_WMAE(orig)=<span class="subst">&#123;val_wmae:<span class="number">.1</span>f&#125;</span>, lr=<span class="subst">&#123;opt.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]:<span class="number">.1</span>e&#125;</span>&quot;</span>)  </span><br><span class="line">        es.step(val_wmae)  </span><br><span class="line">        <span class="keyword">if</span> es.stop:  </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;[EarlyStopping] at epoch <span class="subst">&#123;e&#125;</span>, best_WMAE=<span class="subst">&#123;es.best:<span class="number">.1</span>f&#125;</span>&quot;</span>)  </span><br><span class="line">            <span class="keyword">break</span>  </span><br><span class="line">    <span class="keyword">return</span> hist  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_loader</span>(<span class="params">model, loader</span>):  </span><br><span class="line">    model.<span class="built_in">eval</span>()  </span><br><span class="line">    outs = []  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">        <span class="keyword">for</span> xb, _ <span class="keyword">in</span> loader:  </span><br><span class="line">            outs.append(model(xb).cpu().numpy())  </span><br><span class="line">    <span class="keyword">return</span> np.vstack(outs)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="comment"># 6) 主程序  </span></span><br><span class="line"><span class="comment"># ---------------------------  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():  </span><br><span class="line">    daily = load_and_prepare()  </span><br><span class="line">    tr_loader, va_loader, te_loader, scaler_y, input_dim = build_windows(daily, window=<span class="number">180</span>)  </span><br><span class="line">  </span><br><span class="line">    lstm  = LSTMModel(input_dim=input_dim)  </span><br><span class="line">    trans = TransformerModel(input_dim=input_dim)  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n[TRAIN] LSTM ...&quot;</span>)  </span><br><span class="line">    hist_lstm  = fit(lstm,  tr_loader, va_loader, scaler_y, epochs=<span class="number">80</span>, lr=<span class="number">3e-4</span>, use_warm_cos=<span class="literal">False</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n[TRAIN] Transformer ...&quot;</span>)  </span><br><span class="line">    hist_trans = fit(trans, tr_loader, va_loader, scaler_y, epochs=<span class="number">80</span>, lr=<span class="number">3e-4</span>, warmup=<span class="number">10</span>, use_warm_cos=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ------------ 预测到原始域 ------------    </span></span><br><span class="line">    y_true_s = []  </span><br><span class="line">    <span class="keyword">for</span> _, yb <span class="keyword">in</span> te_loader:  </span><br><span class="line">        y_true_s.append(yb.cpu().numpy())  </span><br><span class="line">    y_true_s = np.vstack(y_true_s)  </span><br><span class="line">  </span><br><span class="line">    y_lstm_s  = predict_loader(lstm, te_loader)  </span><br><span class="line">    y_trans_s = predict_loader(trans, te_loader)  </span><br><span class="line">  </span><br><span class="line">    y_true  = scaler_y.inverse_transform(y_true_s).ravel()  </span><br><span class="line">    y_lstm  = scaler_y.inverse_transform(y_lstm_s).ravel()  </span><br><span class="line">    y_trans = scaler_y.inverse_transform(y_trans_s).ravel()  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n[METRIC - Test (original domain)]&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;MAE   - LSTM: <span class="subst">&#123;mae(y_true, y_lstm):<span class="number">.2</span>f&#125;</span> | Transformer: <span class="subst">&#123;mae(y_true, y_trans):<span class="number">.2</span>f&#125;</span>&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;MAPE  - LSTM: <span class="subst">&#123;mape(y_true, y_lstm):<span class="number">.2</span>f&#125;</span>% | Transformer: <span class="subst">&#123;mape(y_true, y_trans):<span class="number">.2</span>f&#125;</span>%&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;sMAPE - LSTM: <span class="subst">&#123;smape(y_true, y_lstm):<span class="number">.2</span>f&#125;</span>% | Transformer: <span class="subst">&#123;smape(y_true, y_trans):<span class="number">.2</span>f&#125;</span>%&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;WMAE  - LSTM: <span class="subst">&#123;weighted_mae(y_true, y_lstm):<span class="number">.2</span>f&#125;</span> | Transformer: <span class="subst">&#123;weighted_mae(y_true, y_trans):<span class="number">.2</span>f&#125;</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># ------------ 绘图 ------------    </span></span><br><span class="line">    Path(<span class="string">&quot;figs&quot;</span>).mkdir(exist_ok=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 7-1 验证 WMAE 收敛  </span></span><br><span class="line">    plt.figure(figsize=(<span class="number">7.6</span>, <span class="number">4.2</span>))  </span><br><span class="line">    plt.plot(hist_lstm,  label=<span class="string">&quot;LSTM 验证 WMAE（原始域）&quot;</span>,       lw=<span class="number">2</span>, color=<span class="string">&quot;#1f77b4&quot;</span>)  </span><br><span class="line">    plt.plot(hist_trans, label=<span class="string">&quot;Transformer 验证 WMAE（原始域）&quot;</span>, lw=<span class="number">2</span>, color=<span class="string">&quot;#ff7f0e&quot;</span>)  </span><br><span class="line">    plt.xlabel(<span class="string">&quot;Epoch&quot;</span>); plt.ylabel(<span class="string">&quot;验证集 WMAE&quot;</span>)  </span><br><span class="line">    plt.title(<span class="string">&quot;图 7-1 训练收敛曲线（原始域加权 MAE）&quot;</span>)  </span><br><span class="line">    plt.legend(); plt.tight_layout()  </span><br><span class="line">    plt.savefig(<span class="string">&quot;figs/fig_7_1_val_wmae.png&quot;</span>, dpi=<span class="number">200</span>); plt.close()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 7-2 趋势对比（近 200）  </span></span><br><span class="line">    N_SHOW = <span class="built_in">min</span>(<span class="number">200</span>, <span class="built_in">len</span>(y_true))  </span><br><span class="line">    plt.figure(figsize=(<span class="number">10.5</span>, <span class="number">4.6</span>))  </span><br><span class="line">    plt.plot(y_true[-N_SHOW:],  label=<span class="string">&quot;真实销量&quot;</span>,        color=<span class="string">&quot;gray&quot;</span>,    lw=<span class="number">2</span>)  </span><br><span class="line">    plt.plot(y_lstm[-N_SHOW:],  label=<span class="string">&quot;LSTM 预测&quot;</span>,       color=<span class="string">&quot;#1f77b4&quot;</span>, lw=<span class="number">2</span>, ls=<span class="string">&quot;--&quot;</span>)  </span><br><span class="line">    plt.plot(y_trans[-N_SHOW:], label=<span class="string">&quot;Transformer 预测&quot;</span>, color=<span class="string">&quot;#ff7f0e&quot;</span>, lw=<span class="number">2</span>)  </span><br><span class="line">    plt.xlabel(<span class="string">&quot;时间步&quot;</span>); plt.ylabel(<span class="string">&quot;销量&quot;</span>)  </span><br><span class="line">    plt.title(<span class="string">&quot;图 7-2 销量预测趋势对比（近 200 点）&quot;</span>)  </span><br><span class="line">    plt.legend(); plt.tight_layout()  </span><br><span class="line">    plt.savefig(<span class="string">&quot;figs/fig_7_2_trend_compare.png&quot;</span>, dpi=<span class="number">200</span>); plt.close()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 7-3 误差随时间  </span></span><br><span class="line">    err_l = np.<span class="built_in">abs</span>(y_true - y_lstm)  </span><br><span class="line">    err_t = np.<span class="built_in">abs</span>(y_true - y_trans)  </span><br><span class="line">    plt.figure(figsize=(<span class="number">10.5</span>, <span class="number">4.6</span>))  </span><br><span class="line">    plt.plot(err_l, label=<span class="string">&quot;LSTM |误差|&quot;</span>,        color=<span class="string">&quot;#1f77b4&quot;</span>, alpha=<span class="number">0.9</span>)  </span><br><span class="line">    plt.plot(err_t, label=<span class="string">&quot;Transformer |误差|&quot;</span>, color=<span class="string">&quot;#ff7f0e&quot;</span>, alpha=<span class="number">0.9</span>)  </span><br><span class="line">    plt.xlabel(<span class="string">&quot;时间步&quot;</span>); plt.ylabel(<span class="string">&quot;|误差|&quot;</span>)  </span><br><span class="line">    plt.title(<span class="string">&quot;图 7-3 误差变化曲线（原始域 MAE 随时间变化）&quot;</span>)  </span><br><span class="line">    plt.legend(); plt.tight_layout()  </span><br><span class="line">    plt.savefig(<span class="string">&quot;figs/fig_7_3_mae_over_time.png&quot;</span>, dpi=<span class="number">200</span>); plt.close()  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n[DONE] 图像保存于 ./figs ：&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; - figs/fig_7_1_val_wmae.png&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; - figs/fig_7_2_trend_compare.png&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; - figs/fig_7_3_mae_over_time.png&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:  </span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>相关日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[INFO] device: cpu</span><br><span class="line">[INFO] 本地已存在：xxx/Online_Retail.xlsx</span><br><span class="line">[WARN] window=180 对当前序列过大，已自动降为 window=34 (训练段长度=35)</span><br><span class="line"></span><br><span class="line">[TRAIN] LSTM ...</span><br><span class="line">[Epoch 05] val_WMAE(orig)=83189.5, lr=3.0e-04</span><br><span class="line">[Epoch 10] val_WMAE(orig)=84458.0, lr=3.0e-04</span><br><span class="line">[Epoch 15] val_WMAE(orig)=85364.3, lr=1.5e-04</span><br><span class="line">[EarlyStopping] at epoch 18, best_WMAE=83100.6</span><br><span class="line"></span><br><span class="line">[TRAIN] Transformer ...</span><br><span class="line">[Epoch 05] val_WMAE(orig)=70214.7, lr=1.5e-04</span><br><span class="line">[Epoch 10] val_WMAE(orig)=78437.5, lr=3.0e-04</span><br><span class="line">[Epoch 15] val_WMAE(orig)=92474.3, lr=3.0e-04</span><br><span class="line">[Epoch 20] val_WMAE(orig)=80502.5, lr=2.9e-04</span><br><span class="line">[EarlyStopping] at epoch 24, best_WMAE=64493.8</span><br><span class="line"></span><br><span class="line">[METRIC - Test (original domain)]</span><br><span class="line">MAE   - LSTM: 170133.31 | Transformer: 162398.03</span><br><span class="line">MAPE  - LSTM: 54.69% | Transformer: 51.82%</span><br><span class="line">sMAPE - LSTM: 76.93% | Transformer: 71.91%</span><br><span class="line">WMAE  - LSTM: 201046.81 | Transformer: 192369.91</span><br><span class="line"></span><br><span class="line">[DONE] 图像保存于 ./figs ：</span><br><span class="line"> - figs/fig_7_1_val_wmae.png</span><br><span class="line"> - figs/fig_7_2_trend_compare.png</span><br><span class="line"> - figs/fig_7_3_mae_over_time.png</span><br></pre></td></tr></table></figure>
<h3 id="7-3-实验结果与分析"><a href="#7-3-实验结果与分析" class="headerlink" title="7.3 实验结果与分析"></a>7.3 实验结果与分析</h3><p>实验最终在同一时间序列、相同特征输入条件下进行。下面总结了测试集的主要指标结果。</p>
<table>
<thead>
<tr>
<th><strong>模型</strong></th>
<th><strong>MAE ↓</strong></th>
<th><strong>MAPE ↓</strong></th>
<th><strong>sMAPE ↓</strong></th>
<th><strong>WMAE ↓</strong></th>
</tr>
</thead>
<tbody><tr>
<td>LSTM</td>
<td>170 133</td>
<td>54.69 %</td>
<td>76.93 %</td>
<td>201 047</td>
</tr>
<tr>
<td>Transformer</td>
<td><strong>162 398</strong></td>
<td><strong>51.82 %</strong></td>
<td><strong>71.91 %</strong></td>
<td><strong>192 370</strong></td>
</tr>
</tbody></table>
<p>在当前设置下，Transformer 在四项指标上都略优于 LSTM，数值差距大致在 4–6% 左右。考虑到样本量有限、且时间粒度为周频，这些结果更适合被理解为一种<strong>趋势性观察</strong>，而不是严格的统计结论。结合图 7-1&#x2F;7-2&#x2F;7-3 可以看到：Transformer 在捕捉整体趋势与节奏性波动时略显平滑，对较大的销量起伏有一定缓冲；而 LSTM 在局部短期段落上的拟合能力依然具有竞争力。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_1_val_wmae.png" width="80%" /></p>
<center>图 7-1 训练收敛曲线（原始域加权 MAE）</center>
  
<p>LSTM 的验证损失下降较为平稳，但在 20 轮后趋于停滞；Transformer 早期波动较大，随后在较低损失区间稳定，说明注意力机制在有限样本下仍具一定正则作用。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_2_trend_compare.png" width="80%" /><br>    <center>图 7-2 销量预测趋势对比（近 200 点）</center></p>
<p>在整体趋势上，两者均能跟踪销售的季节变化。Transformer 在节假日后的销售回落段响应更灵敏，预测曲线相比略贴近真实值。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/uPic/fig_7_3_mae_over_time.png" width="80%" /></p>
<center>图 7-3 误差变化曲线（原始域 MAE 随时间变化）</center>

<p>误差在高销售周（峰值）附近显著增大，说明两种模型在极端销量下仍存在欠拟合。但 Transformer 的误差曲线相对更平滑，极端值波动较小，体现了更好的<strong>鲁棒性</strong>。</p>
<p>总体而言，这一轮实验显示：<br><strong>Transformer 在捕捉中长期趋势与复杂季节性时略有优势，而 LSTM 在短期平稳段仍具竞争力。</strong></p>
<h3 id="7-4-结论与启示"><a href="#7-4-结论与启示" class="headerlink" title="7.4 结论与启示"></a>7.4 结论与启示</h3><ol>
<li><strong>预测精度</strong><ul>
<li>两种模型在总体指标上差异有限，Transformer 仅小幅领先。</li>
<li>当销量波动较大或存在多周期叠加时，Transformer 的表现更稳定。</li>
</ul>
</li>
<li><strong>训练效率与稳定性</strong><ul>
<li>Transformer 在 GPU 上能更充分利用并行计算资源，训练速度略快。</li>
<li>LSTM 收敛更平滑，但对学习率与窗口长度更敏感。</li>
</ul>
</li>
<li><strong>模型偏好</strong><ul>
<li>LSTM 适合<strong>短期预测</strong>或样本量有限场景，结构简单、鲁棒性好；</li>
<li>Transformer 更适合<strong>中长周期预测</strong>，尤其是在引入多种外生特征时，注意力层能自适应调整关注区域。</li>
</ul>
</li>
<li><strong>改进方向</strong><br> 若希望进一步扩大性能差距，可尝试：<br> - 引入多层注意力或混合注意力结构；<br> - 使用更长的输入窗口（在数据充足时）；<br> - 增加节假日、促销、价格等离散外生变量；<br> - 或采用 <strong>LSTM + Transformer 混合架构</strong>：前者建模短期局部模式，后者建模长期依赖。</li>
</ol>
<p><strong>结论：</strong></p>
<ul>
<li>在当前这条周频电商销量时间序列上，Transformer 相比 LSTM 在误差指标上呈现出<strong>轻微而稳定的优势</strong>，更容易捕捉中长期的趋势与季节性波动；</li>
<li>LSTM 结构更简洁，在短期模式较为平稳的区间内依然表现良好，也往往是时间序列任务中可靠的基线模型；</li>
<li>由于数据长度与实验规模有限，这里的结论主要提供一个 <strong>“方法对比的定性参考”</strong>，在更大规模、更多品类和更长时间跨度的真实业务场景中，仍需要结合实际数据重新评估两类结构的优劣。</li>
</ul>
<p>因此，如果任务更关注长期节奏、季节性与多种外生变量之间的复杂交互，Transformer 具有较大潜力；而在资源受限或希望快速建立可用基线时，LSTM 依然是一个简单而稳健的选择。</p>
<h2 id="8-小结与展望：从记忆到理解"><a href="#8-小结与展望：从记忆到理解" class="headerlink" title="8. 小结与展望：从记忆到理解"></a>8. 小结与展望：从记忆到理解</h2><p>在深度学习的发展历程中，时间序列建模经历了从 <strong>“记忆”到“理解”</strong> 的根本性变革。这不仅是一场技术的迭代，更是一场关于“时间”如何被机器感知与表达的认知革命。</p>
<h3 id="8-1-三重革命的脉络"><a href="#8-1-三重革命的脉络" class="headerlink" title="8.1 三重革命的脉络"></a>8.1 三重革命的脉络</h3><ol>
<li><strong>算法革命：LSTM 解决了记忆遗忘的问题</strong><br> 早期的 RNN 虽然能够处理序列输入，但由于梯度消失与爆炸，模型难以保留长期信息。LSTM 的提出，通过引入<strong>门控机制（Gate Mechanism）与细胞状态（Cell State）</strong>，让神经网络学会“何时记、何时忘”，从而延长了时间依赖的有效跨度。这标志着深度学习从“被动记忆”进入了“主动控制记忆”的阶段。</li>
<li><strong>结构革命：Attention 实现了全局建模</strong><br> 注意力机制打破了序列的线性限制，让模型在每一步都能动态地“聚焦”于输入的不同部分。通过计算 Query 与 Key 的相似度，Attention 建立了一种<strong>可学习的关系图谱</strong>，使模型能够捕捉全局上下文，而不受时间顺序的束缚。这一思想，为后来的 Transformer 奠定了结构基础。</li>
<li><strong>范式革命：Transformer 将时间转化为关系</strong><br> Transformer 进一步摒弃了循环结构，把原本“沿时间传播”的依赖转换为 <strong>“通过关系传播”的全局建模</strong>。在这个新范式下，时间不再是信息传递的唯一维度，模型可以在任意位置间建立直接联系。</li>
</ol>
<p><strong>Transformer 的革命性在于：它重新定义了“时间”。</strong> 时间不再是顺序的，而是结构的；模型不再被时间限制，而能在关系中理解语义。</p>
<h3 id="8-2-统一的思想：从记忆到理解"><a href="#8-2-统一的思想：从记忆到理解" class="headerlink" title="8.2 统一的思想：从记忆到理解"></a>8.2 统一的思想：从记忆到理解</h3><p>如果说 RNN 是“记忆机器”，LSTM 是“有选择的记忆机器”，那么 Attention 与 Transformer 则让机器具备了“理解能力”。</p>
<ul>
<li><strong>RNN</strong> 学会了“记住过去”；</li>
<li><strong>LSTM</strong> 学会了“有选择地记忆”；</li>
<li><strong>Attention</strong> 学会了“聚焦重要”；</li>
<li><strong>Transformer</strong> 则学会了“理解关系”。</li>
</ul>
<p>从时间依赖到关系建模，这一演化轨迹本质上是模型认知层次的跃升：从线性思维到结构思维，从被动记忆到主动理解。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/" title="从 RNN 到 Transformer：时间建模的变革">https://www.keychan.xyz/2025/11/25/043-rnn-to-transformer/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 循环神经网络</a>
              <a href="/tags/LSTM/" rel="tag"># LSTM</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" rel="tag"># 位置编码</a>
              <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="tag"># 注意力机制</a>
              <a href="/tags/%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" rel="tag"># 序列建模</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/11/18/042-flight-control-pid-simulation-verification/" rel="prev" title="「无人机系列④」飞行控制算法入门——从PID到仿真验证">
                  <i class="fa fa-angle-left"></i> 「无人机系列④」飞行控制算法入门——从PID到仿真验证
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/12/02/044-pytorch-building-blocks-building-systems/" rel="next" title="PyTorch：从搭积木到构建系统">
                  PyTorch：从搭积木到构建系统 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">334k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/11/25/043-rnn-to-transformer/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
