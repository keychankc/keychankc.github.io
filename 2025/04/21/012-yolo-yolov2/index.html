<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"keychankc.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1.YOLOv2改进概述YOLOv2 的改进围绕 ​稳定性​（BN、位置预测）、灵活性​（全卷积、多尺度）、数据驱动​（锚框聚类）展开，同时通过结构优化（Darknet-19、Passthrough）平衡速度与精度，为后续YOLO版本再改进奠定基础。如上图是YOLOv2的新特性和mAP（mean Average Precision，平均精度均值）之间的相关性。">
<meta property="og:type" content="article">
<meta property="og:title" content="[YOLO系列②] YOLOv2十大改进点解析">
<meta property="og:url" content="https://keychankc.github.io/2025/04/21/012-yolo-yolov2/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1.YOLOv2改进概述YOLOv2 的改进围绕 ​稳定性​（BN、位置预测）、灵活性​（全卷积、多尺度）、数据驱动​（锚框聚类）展开，同时通过结构优化（Darknet-19、Passthrough）平衡速度与精度，为后续YOLO版本再改进奠定基础。如上图是YOLOv2的新特性和mAP（mean Average Precision，平均精度均值）之间的相关性。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/202504161606.jpg">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/202504102313.jpg">
<meta property="article:published_time" content="2025-04-21T08:29:12.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.538Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="目标检测">
<meta property="article:tag" content="YOLO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/202504161606.jpg">


<link rel="canonical" href="https://keychankc.github.io/2025/04/21/012-yolo-yolov2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://keychankc.github.io/2025/04/21/012-yolo-yolov2/","path":"2025/04/21/012-yolo-yolov2/","title":"[YOLO系列②] YOLOv2十大改进点解析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[YOLO系列②] YOLOv2十大改进点解析 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-YOLOv2%E6%94%B9%E8%BF%9B%E6%A6%82%E8%BF%B0"><span class="nav-text">1.YOLOv2改进概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Batch-Normalization%EF%BC%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89"><span class="nav-text">2.Batch Normalization（批量归一化）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">1.归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-BN%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-text">2.BN的优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BA%94%E7%94%A8"><span class="nav-text">3.应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-hi-res-classifier%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%89"><span class="nav-text">3.hi-res classifier（高分辨率分类器）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%97%AE%E9%A2%98%E7%82%B9"><span class="nav-text">1.问题点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AE%9E%E7%8E%B0"><span class="nav-text">2.实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="nav-text">3.为什么不直接用高分辨率训练？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-convolutional%EF%BC%88%E5%85%A8%E5%8D%B7%E7%A7%AF%EF%BC%89"><span class="nav-text">4.convolutional（全卷积）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-text">1.全连接层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%A8%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">2.全卷积层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-anchor-boxes%EF%BC%88%E9%94%9A%E6%A1%86%EF%BC%89"><span class="nav-text">5.anchor boxes（锚框）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%88%E9%AA%8C%E6%A1%86"><span class="nav-text">1.先验框</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A1%86%E7%BB%93%E6%9E%84"><span class="nav-text">2.框结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-K-Means-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-text">3.K-Means 聚类方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-new-network%EF%BC%88%E6%96%B0%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%89"><span class="nav-text">6.new network（新网络结构）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Darknet-19"><span class="nav-text">1.Darknet-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-YOLOv2-%E4%B8%ADDarknet-19%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-text">2.YOLOv2 中Darknet-19的网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-YOLOv2%E7%9A%84%E6%A3%80%E6%B5%8B%E5%A4%B4"><span class="nav-text">3.YOLOv2的检测头</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-dimension-priors%EF%BC%88%E7%BB%B4%E5%BA%A6%E5%85%88%E9%AA%8C%EF%BC%89"><span class="nav-text">7.dimension priors（维度先验）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%AD%A5%E9%AA%A4"><span class="nav-text">1.步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%B8%8Eanchor-boxes%E5%85%B3%E7%B3%BB"><span class="nav-text">2. 与anchor boxes关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-location-prediction%EF%BC%88%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B%E4%BC%98%E5%8C%96%EF%BC%89"><span class="nav-text">8.location prediction（位置预测优化）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%97%AE%E9%A2%98%E7%82%B9-1"><span class="nav-text">1.问题点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-text">2.解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-passthrough%EF%BC%88%E9%80%9A%E9%81%93%E8%BF%9E%E6%8E%A5%EF%BC%89"><span class="nav-text">9.passthrough（通道连接）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%97%AE%E9%A2%98%E7%82%B9-2"><span class="nav-text">1.问题点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF"><span class="nav-text">2.解决思路</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-multi-scale%EF%BC%88%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="nav-text">10.multi-scale（多尺度训练）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91"><span class="nav-text">1.实现逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BC%98%E7%82%B9"><span class="nav-text">2.优点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-hi-res-detector%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%A3%80%E6%B5%8B%E5%99%A8%EF%BC%89"><span class="nav-text">11.hi-res detector（高分辨率检测器）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%B5%81%E7%A8%8B"><span class="nav-text">1.两段式流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%9C%89%E6%95%88%E6%9E%9C"><span class="nav-text">2.有效果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-%E6%80%BB%E7%BB%93"><span class="nav-text">12.总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Batch-Normalization%EF%BC%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">1.Batch Normalization（批量归一化）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Hi-Res-Classifier%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">2.Hi-Res Classifier（高分辨率分类器）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Convolutional%EF%BC%88%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">3.Convolutional（全卷积结构）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Anchor-Boxes%EF%BC%88%E9%94%9A%E6%A1%86%E6%9C%BA%E5%88%B6%EF%BC%89%E2%80%8B"><span class="nav-text">4. Anchor Boxes（锚框机制）​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-New-Network-Darknet-19"><span class="nav-text">5. New Network: Darknet-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Dimension-Priors%EF%BC%88%E7%BB%B4%E5%BA%A6%E5%85%88%E9%AA%8C%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">6. Dimension Priors（维度先验）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Location-Prediction%EF%BC%88%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B%E4%BC%98%E5%8C%96%EF%BC%89%E2%80%8B"><span class="nav-text">7. Location Prediction（位置预测优化）​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Passthrough-Layer%EF%BC%88%E9%80%9A%E9%81%93%E8%BF%9E%E6%8E%A5%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">8. Passthrough Layer（通道连接）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-Multi-Scale-Training%EF%BC%88%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">9. Multi-Scale Training（多尺度训练）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-Hi-Res-Detector%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%A3%80%E6%B5%8B%E5%99%A8%EF%BC%89%E2%80%8B%E2%80%8B"><span class="nav-text">10. Hi-Res Detector（高分辨率检测器）​​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-%E6%95%B4%E4%BD%93%E6%94%B9%E8%BF%9B%E6%95%88%E6%9E%9C%E2%80%8B"><span class="nav-text">11. 整体改进效果​</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/keychankc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://keychankc.github.io/2025/04/21/012-yolo-yolov2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="[YOLO系列②] YOLOv2十大改进点解析 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [YOLO系列②] YOLOv2十大改进点解析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-04-21 16:29:12" itemprop="dateCreated datePublished" datetime="2025-04-21T16:29:12+08:00">2025-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/04/21/012-yolo-yolov2/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/04/21/012-yolo-yolov2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-YOLOv2改进概述"><a href="#1-YOLOv2改进概述" class="headerlink" title="1.YOLOv2改进概述"></a>1.YOLOv2改进概述</h2><p>YOLOv2 的改进围绕 ​<strong>稳定性</strong>​（BN、位置预测）、<strong>灵活性</strong>​（全卷积、多尺度）、<strong>数据驱动</strong>​（锚框聚类）展开，同时通过<strong>结构优化</strong>（Darknet-19、Passthrough）平衡速度与精度，为后续YOLO版本再改进奠定基础。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/202504161606.jpg"><br>如上图是YOLOv2的新特性和mAP（mean Average Precision，平均精度均值）之间的相关性。</p>
<span id="more"></span>
<h2 id="2-Batch-Normalization（批量归一化）"><a href="#2-Batch-Normalization（批量归一化）" class="headerlink" title="2.Batch Normalization（批量归一化）"></a>2.Batch Normalization（批量归一化）</h2><h3 id="1-归一化"><a href="#1-归一化" class="headerlink" title="1.归一化"></a>1.归一化</h3><p><strong>归一化</strong>​ 是把一组数据“按比例缩放”到统一的范围​内（比如 0到1 或 -1到1），就像把不同单位的尺子换成同一把尺子来测量，让数据之间可以公平比较。</p>
<h3 id="2-BN的优势"><a href="#2-BN的优势" class="headerlink" title="2.BN的优势"></a>2.BN的优势</h3><p>在YOLO的深度网络中，前面层的权重改变会影响后面层的输入分布，这叫做<strong>协变量偏移（Internal Covariate Shift）</strong>。这种偏移会导致网络训练不稳定，甚至训练失败。而对每一层的输入上，进行归一化处理（均值为0，方差为1）吼，就可以在每个 mini-batch 中，把每个特征的数值拉回到一个「正常」的分布范围，这样做有很多好处：</p>
<ol>
<li><strong>加快训练速度</strong>：因为每层的输入更稳定，网络收敛更快</li>
<li><strong>允许更大学习率</strong>：降低了爆炸&#x2F;梯度消失的风险</li>
<li><strong>减少对初始化的依赖</strong>：不用精心设置初始权重也能正常训练</li>
<li><strong>具有轻微的正则化效果</strong>：可以减少对 Dropout 的依赖</li>
<li><strong>提升最终精度</strong>：尤其是对像 YOLO 这样的大型模型，效果非常明显</li>
</ol>
<h3 id="3-应用"><a href="#3-应用" class="headerlink" title="3.应用"></a>3.应用</h3><p>Batch Normalization在现代神经网络中应用还是非常广泛的。</p>
<table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>网络名称</strong></th>
<th><strong>是否使用 BN</strong></th>
</tr>
</thead>
<tbody><tr>
<td>图像分类</td>
<td>ResNet &#x2F; VGG-BN &#x2F; Inception &#x2F; DenseNet</td>
<td>✅</td>
</tr>
<tr>
<td>检测</td>
<td>YOLOv2&#x2F;3&#x2F;4&#x2F;5, SSD (部分)</td>
<td>✅</td>
</tr>
<tr>
<td>混合模型</td>
<td>CNN+Transformer（如 Conformer）</td>
<td>✅（CNN 部分）</td>
</tr>
<tr>
<td>生成模型</td>
<td>DCGAN &#x2F; StyleGAN</td>
<td>✅</td>
</tr>
<tr>
<td>强化学习</td>
<td>DQN &#x2F; PPO &#x2F; A3C（图像部分）</td>
<td>✅</td>
</tr>
</tbody></table>
<h2 id="3-hi-res-classifier（高分辨率分类器）"><a href="#3-hi-res-classifier（高分辨率分类器）" class="headerlink" title="3.hi-res classifier（高分辨率分类器）"></a>3.hi-res classifier（高分辨率分类器）</h2><h3 id="1-问题点"><a href="#1-问题点" class="headerlink" title="1.问题点"></a>1.问题点</h3><p>目标检测任务通常需要处理 <strong>高分辨率图像</strong>，而YOLOV1是先在 <strong>ImageNet（224×224）上预训练的</strong>，如果直接将这种低分辨率训练出的模型用于检测，会导致它对高分图像中的细节“视力不够”，表现不佳。就像一个人习惯了在小手机屏幕上看照片，突然换成大屏电视，他一开始会看不过来，需要重新适应，才能看清更多细节。所以，在分类预训练之后，YOLO 需要<strong>在更高分辨率（如 448×448）上再次训练这个分类器</strong>，以让网络更适应高分辨率输入。</p>
<h3 id="2-实现"><a href="#2-实现" class="headerlink" title="2.实现"></a>2.实现</h3><ol>
<li>使用 ImageNet 预训练的分类器（输入为 224×224）</li>
<li>把输入分辨率调整为 448×448</li>
<li>用 ImageNet 数据再微调一次网络（10次448×448 ）</li>
<li>得到适合高分辨率输入的分类器，作为目标检测的 backbone</li>
</ol>
<h3 id="3-为什么不直接用高分辨率训练？"><a href="#3-为什么不直接用高分辨率训练？" class="headerlink" title="3.为什么不直接用高分辨率训练？"></a>3.为什么不直接用高分辨率训练？</h3><p>既然目标检测最终就是用 448×448 的输入，为什么一开始不就用这个分辨率训练 ImageNet 分类器呢？</p>
<ol>
<li>计算成本太高<br> ImageNet 有 <strong>上百万张图片</strong>，训练一个分类模型已经很耗资源了：<ul>
<li>使用 224×224 的图像时，一次前向&#x2F;反向传播的内存和计算就已经很重</li>
<li>如果一开始就用 <strong>448×448 的输入尺寸</strong>，计算量将是原来的 <strong>4 倍</strong>（因为面积是×4）</li>
<li>在 2016 年 YOLOv2 发布的时候，硬件远没有现在这么强，训练成本很高<br> 所以，先在小图上训练，再在高分辨率上微调，是<strong>更划算、也更现实</strong>的做法。</li>
</ul>
</li>
<li>迁移学习的效率更高<br> 深度学习的常见套路是：<br> <strong>先在大数据上训练一个通用模型，然后在特定任务或分辨率上做微调（fine-tune）</strong><br>  好处是：<ul>
<li>低分辨率上已经能学到很多通用的图像特征（边缘、纹理、形状等）</li>
<li>只需要在高分辨率上再调整几轮，就可以适配检测任务</li>
<li>微调速度远快于从头训练，效果也不差</li>
</ul>
</li>
<li>ImageNet 本身就是低分辨率构建的<br> ImageNet 的原始训练流程和竞赛标准就是用 <strong>224×224 的裁剪图像</strong>：<ul>
<li>这是标准的数据输入格式，所有主流模型（ResNet、VGG、Darknet 等）都默认这个尺寸</li>
<li>如果用 448×448 去训练，等于是自己“重新做一版”ImageNet，很不现实</li>
<li>一些图像原本分辨率就没那么高，强行放大反而可能带来噪声</li>
</ul>
</li>
<li>分类和检测的侧重点不同<ul>
<li>分类任务关心的是 “图里有没有猫”</li>
<li>检测任务关心的是 “猫在哪”，更依赖<strong>空间信息</strong></li>
<li>所以在高分辨率上微调，更像是<strong>给模型增强空间感知能力</strong>，而不是重头学所有特征</li>
</ul>
</li>
</ol>
<h2 id="4-convolutional（全卷积）"><a href="#4-convolutional（全卷积）" class="headerlink" title="4.convolutional（全卷积）"></a>4.convolutional（全卷积）</h2><p>用卷积层代替全连接层，让网络可以接受任意大小的输入图像</p>
<h3 id="1-全连接层"><a href="#1-全连接层" class="headerlink" title="1.全连接层"></a>1.全连接层</h3><p>全连接层（FC）的本质是啥？<br>全连接层是一个<strong>矩阵乘法</strong>：输入是一个固定长度的向量 → 输出是另一个固定长度的向量。比如输入是 4096 个数，输出是 1000 类，那这个 FC 层的权重矩阵大小就是 1000×4096。所以你必须保证前一层的输出 <strong>是 4096 个数</strong>，否则就乘不了。</p>
<p>YOLO v1 的最后几层是这样的：</p>
<ul>
<li>前面是卷积层 → 提取特征</li>
<li>最后是 <strong>全连接层（FC）</strong> → 输出预测框（比如：7x7x30 的 tensor）</li>
<li>这就把整个网络“定死”只能接收某一种大小的图像，比如 448×448</li>
</ul>
<img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/202504102313.jpg" width="600"/>

<h3 id="2-全卷积层"><a href="#2-全卷积层" class="headerlink" title="2.全卷积层"></a>2.全卷积层</h3><p>而卷积层是<strong>滑动窗口操作</strong>，它并不关心图像多大：</p>
<ul>
<li>不管输入是 224×224，还是 448×448，卷积核都能滑动</li>
<li>输出特征图的尺寸会自动“缩放”，不会出错</li>
<li>所以卷积结构天然支持“可变大小输入”<br>这也是为什么 YOLOv2 改成全卷积后，就可以支持多尺度图像输入了。</li>
</ul>
<p>全卷积层的好处：</p>
<ol>
<li>输入尺寸灵活，可以输入任意分辨率的图片（比如：320×320、416×416、608×608）</li>
<li>更高效，少了大量 FC 层参数，网络更轻、更快</li>
<li>可以多尺度训练，后续的 multi-scale training（多尺度训练）正是建立在全卷积的基础之上</li>
<li>更强的空间感知能力，卷积层保留了图像的空间结构，FC 层则丢失了这些信息</li>
</ol>
<h2 id="5-anchor-boxes（锚框）"><a href="#5-anchor-boxes（锚框）" class="headerlink" title="5.anchor boxes（锚框）"></a>5.anchor boxes（锚框）</h2><h3 id="1-先验框"><a href="#1-先验框" class="headerlink" title="1.先验框"></a>1.先验框</h3><p>在目标检测中，模型要预测物体的 <strong>位置和大小</strong>，如果从零开始“随便猜一个框”，训练难度非常大。<br>为此，YOLOv2 引入了 <strong>Anchor Boxes</strong> ——一组预定义的“候选框模板”，<strong>帮助模型从一个合理的参考框出发进行微调</strong>。</p>
<p>假设图片中有一只狗，它大概是一个宽 100、高 50 的长方形。模型在这个位置预设了 5 个 anchor 框，比如：</p>
<ol>
<li>(120, 40) → 和狗最接近 ✅</li>
<li>(80, 80) → 太正方形 ❌</li>
<li>(50, 100) → 竖着的 ❌<br> ⋯⋯</li>
</ol>
<p>模型会选出最接近目标的 anchor（这里是第一个），然后在它的基础上稍作调整，预测出更加精确的边界框，这样比从头猜，更快、更准、收敛更稳定。</p>
<h3 id="2-框结构"><a href="#2-框结构" class="headerlink" title="2.框结构"></a>2.框结构</h3><p>每个 cell 会对应 <strong>K 个 anchor 框</strong>（如 K&#x3D;5），每个 anchor 都会预测：</p>
<ul>
<li>边界框位置（tx, ty, tw, th）</li>
<li>目标存在的置信度（objectness）</li>
<li>各类别的概率（class probs）</li>
</ul>
<h3 id="3-K-Means-聚类方法"><a href="#3-K-Means-聚类方法" class="headerlink" title="3.K-Means 聚类方法"></a>3.K-Means 聚类方法</h3><p>为了让 Anchor 框更贴近真实物体的尺寸分布，YOLOv2 不再手动设定，而是通过聚类方法自动学习得到。步骤如下：</p>
<ol>
<li><strong>收集所有真实框的宽高</strong>（w, h）作为聚类输入</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="number">40</span>, <span class="number">60</span>), (<span class="number">100</span>, <span class="number">120</span>), (<span class="number">30</span>, <span class="number">30</span>), (<span class="number">50</span>, <span class="number">80</span>), ...]</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>归一化</strong>，通常会除以图像尺寸，变成相对比例，比如 (0.25, 0.5)</li>
<li><strong>使用 K-Means 聚类</strong>得到 K 个代表框，作为 Anchor<ul>
<li>YOLOv2 用的是 <strong>IOU 距离</strong> 而不是欧几里得距离</li>
<li>IOU 距离定义为：distance &#x3D; 1 - IOU(真实框, anchor框)</li>
<li>IOU 越大，两个框越像，距离就越小，聚类效果更贴合目标检测的需求</li>
</ul>
</li>
</ol>
<h2 id="6-new-network（新网络结构）"><a href="#6-new-network（新网络结构）" class="headerlink" title="6.new network（新网络结构）"></a>6.new network（新网络结构）</h2><h3 id="1-Darknet-19"><a href="#1-Darknet-19" class="headerlink" title="1.Darknet-19"></a>1.Darknet-19</h3><p>Darknet-19在YOLOv2中充当主干网络（backbone），负责从输入图像中提取多层次的特征，能够捕捉物体的边缘、纹理、形状等关键信息，然后在此之上再集成检测组件，形成高效的目标检测模型。</p>
<p>YOLOv1 用的 GoogLeNet，但GoogLeNet有以下几个问题：</p>
<ol>
<li>太复杂，用的 Inception 模块，不好修改和扩展</li>
<li>准确率不高，在分类和检测上都被别的模型（如 Faster R-CNN）完爆</li>
<li>输入太小，只能处理 224×224 图，导致检测精度低（物体太小了）</li>
</ol>
<p>于是就有了Darknet-19，Darknet-19有如下特点：</p>
<ol>
<li>19 个卷积层 + 5 个 max pooling 层</li>
<li>统一用 3×3，小卷积核（类似 VGG）</li>
<li>全卷积设计，没用 Inception、ResNet 这些复杂模块</li>
<li>用了 Leaky ReLU（相比普通 ReLU 更稳定）</li>
<li>每层后面都加了 Batch Normalization，加速收敛</li>
</ol>
<h3 id="2-YOLOv2-中Darknet-19的网络结构"><a href="#2-YOLOv2-中Darknet-19的网络结构" class="headerlink" title="2.YOLOv2 中Darknet-19的网络结构"></a>2.YOLOv2 中Darknet-19的网络结构</h3><p>YOLOv2 中的 Darknet-19 是一个纯卷积网络，没有 FC 和 Softmax，结构共 19 个卷积层 + 5 个池化层，输出特征图给检测头做目标预测：</p>
<table>
<thead>
<tr>
<th><strong>层级</strong></th>
<th><strong>类型</strong></th>
<th><strong>卷积核</strong></th>
<th><strong>步长</strong></th>
<th><strong>输出通道数</strong></th>
<th><strong>输出尺寸（以输入 224×224 为例）</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>32</td>
<td>224×224</td>
</tr>
<tr>
<td>2</td>
<td>MaxPool</td>
<td>2×2</td>
<td>2</td>
<td>-</td>
<td>112×112</td>
</tr>
<tr>
<td>3</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>64</td>
<td>112×112</td>
</tr>
<tr>
<td>4</td>
<td>MaxPool</td>
<td>2×2</td>
<td>2</td>
<td>-</td>
<td>56×56</td>
</tr>
<tr>
<td>5</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>128</td>
<td>56×56</td>
</tr>
<tr>
<td>6</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>64</td>
<td>56×56</td>
</tr>
<tr>
<td>7</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>128</td>
<td>56×56</td>
</tr>
<tr>
<td>8</td>
<td>MaxPool</td>
<td>2×2</td>
<td>2</td>
<td>-</td>
<td>28×28</td>
</tr>
<tr>
<td>9</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>256</td>
<td>28×28</td>
</tr>
<tr>
<td>10</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>128</td>
<td>28×28</td>
</tr>
<tr>
<td>11</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>256</td>
<td>28×28</td>
</tr>
<tr>
<td>12</td>
<td>MaxPool</td>
<td>2×2</td>
<td>2</td>
<td>-</td>
<td>14×14</td>
</tr>
<tr>
<td>13</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>512</td>
<td>14×14</td>
</tr>
<tr>
<td>14</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>256</td>
<td>14×14</td>
</tr>
<tr>
<td>15</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>512</td>
<td>14×14</td>
</tr>
<tr>
<td>16</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>256</td>
<td>14×14</td>
</tr>
<tr>
<td>17</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>512</td>
<td>14×14</td>
</tr>
<tr>
<td>18</td>
<td>MaxPool</td>
<td>2×2</td>
<td>2</td>
<td>-</td>
<td>7×7</td>
</tr>
<tr>
<td>19</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>1024</td>
<td>7×7</td>
</tr>
<tr>
<td>20</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>512</td>
<td>7×7</td>
</tr>
<tr>
<td>21</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>1024</td>
<td>7×7</td>
</tr>
<tr>
<td>22</td>
<td>Conv</td>
<td>1×1</td>
<td>1</td>
<td>512</td>
<td>7×7</td>
</tr>
<tr>
<td>23</td>
<td>Conv</td>
<td>3×3</td>
<td>1</td>
<td>1024</td>
<td>7×7</td>
</tr>
</tbody></table>
<ul>
<li>卷积层（Conv）：是“特征提取器”，负责处理边缘、角落、图案等视觉元素</li>
<li>池化层（MaxPool）：是“特征压缩器”，帮助压缩尺寸、保留最重要信息</li>
</ul>
<h3 id="3-YOLOv2的检测头"><a href="#3-YOLOv2的检测头" class="headerlink" title="3.YOLOv2的检测头"></a>3.YOLOv2的检测头</h3><p>在 Darknet-19 后，YOLOv2 加了以下部分（检测头）：</p>
<table>
<thead>
<tr>
<th><strong>层级</strong></th>
<th><strong>类型</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td>24</td>
<td>Conv</td>
<td>3×3, 1024 filters</td>
</tr>
<tr>
<td>25</td>
<td>Conv</td>
<td>3×3, 1024 filters</td>
</tr>
<tr>
<td>26</td>
<td>Conv</td>
<td>3×3, 1024 filters</td>
</tr>
<tr>
<td>27</td>
<td>跳接（passthrough）</td>
<td>从 14×14 的中间层取特征图（高分辨率），拼接</td>
</tr>
<tr>
<td>28</td>
<td>Conv</td>
<td>3×3, 输出为 B × (5 + C) 的特征图，进行目标检测</td>
</tr>
</tbody></table>
<ul>
<li>跳接（passthrough）：把早期层（空间分辨率高）提取的细粒度信息拼接到后面层中，提升小目标检测效果</li>
</ul>
<h2 id="7-dimension-priors（维度先验）"><a href="#7-dimension-priors（维度先验）" class="headerlink" title="7.dimension priors（维度先验）"></a>7.dimension priors（维度先验）</h2><p>主要是解决了 YOLOv1 中 bounding box 回归不稳定的问题，大幅提升了检测效果。</p>
<h3 id="1-步骤"><a href="#1-步骤" class="headerlink" title="1.步骤"></a>1.步骤</h3><p>dimension priors 是一组在训练前从数据集中统计出来的、常见的 bounding box 宽高比例（w, h）模板，用来作为预测的起点（即 anchor boxes）。也就是不要让网络从零开始学会‘框’出物体的大小，而是给它几个常见的框，让它基于这些‘模板’微调。”<br>具体步骤：</p>
<ol>
<li>从训练集所有真实框中提取宽高（w, h）</li>
<li>对这些框做 <strong>K-means 聚类</strong>（使用 IOU 距离而不是欧式距离）</li>
<li>找到最能代表训练数据分布的 K 个宽高组合</li>
<li>将这些组合作为 <strong>anchor boxes（dimension priors）</strong></li>
</ol>
<h3 id="2-与anchor-boxes关系"><a href="#2-与anchor-boxes关系" class="headerlink" title="2. 与anchor boxes关系"></a>2. 与anchor boxes关系</h3><p>dimension priors VS anchor boxes:</p>
<table>
<thead>
<tr>
<th><strong>术语</strong></th>
<th><strong>定义</strong></th>
<th><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>dimension priors</strong></td>
<td>基于训练数据用 K-means 聚出来的常见框的宽高</td>
<td>是 “<strong>设计 anchor 的依据</strong>”</td>
</tr>
<tr>
<td><strong>anchor boxes</strong></td>
<td>模型中在每个 grid cell 固定放置的框模板，用于预测目标</td>
<td>是 “<strong>模型里实际使用的东西</strong>”</td>
</tr>
</tbody></table>
<p>关键步骤：<br>训练数据的框 → [K-means 聚类] → 得到 dimension priors → 用作 anchor boxes 尺寸</p>
<h2 id="8-location-prediction（位置预测优化）"><a href="#8-location-prediction（位置预测优化）" class="headerlink" title="8.location prediction（位置预测优化）"></a>8.location prediction（位置预测优化）</h2><h3 id="1-问题点-1"><a href="#1-问题点-1" class="headerlink" title="1.问题点"></a>1.问题点</h3><p>在 YOLOv1 中，网络直接预测目标框的位置（中心点 x, y 和宽高 w, h），但这样有两个大问题：</p>
<ol>
<li><strong>不稳定</strong> —— 网络一开始预测的位置可能离目标很远，训练效果差</li>
<li><strong>容易越界</strong> —— 容易预测到 cell 外面，导致损失很难收敛</li>
</ol>
<h3 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2.解决方法"></a>2.解决方法</h3><p>而YOLOv2 不再直接预测位置，而是直接预测一组偏移量（tx, ty, tw, th），然后用一个规则公式把它转换成真实框的位置。大致做法是将输入图像划分为 S×S 网格；</p>
<ul>
<li>每个 grid cell 预测多个 anchor boxes；</li>
<li>对于每个 anchor box，网络输出：$t_x,\ t_y,\ t_w,\ t_h,\ \text{objectness},\ \text{class scores}$<br>然后预测偏移量（tx, ty, tw, th），再通过 sigmoid + exp + anchor box 解码出预测框的真实位置</li>
</ul>
<h2 id="9-passthrough（通道连接）"><a href="#9-passthrough（通道连接）" class="headerlink" title="9.passthrough（通道连接）"></a>9.passthrough（通道连接）</h2><h3 id="1-问题点-2"><a href="#1-问题点-2" class="headerlink" title="1.问题点"></a>1.问题点</h3><p>为何需要通道连接？</p>
<ul>
<li>网络越深，特征越抽象，感受野大，但<strong>分辨率低</strong>，对小目标不友好</li>
<li>网络越浅，特征细节多，分辨率高，但语义弱</li>
</ul>
<h3 id="2-解决思路"><a href="#2-解决思路" class="headerlink" title="2.解决思路"></a>2.解决思路</h3><p>如果把前面网络层（分辨率高，细节多）的特征图拿回来用一下，和后面的粗特征图<strong>拼接</strong>在一起，就可以显著提升小目标的感知能力。YOLOv2 把 <strong>14×14 的中间层</strong>（细节丰富）和 <strong>7×7 的深层特征图</strong>（语义强）结合在一起！</p>
<p>Passthrough Layer实现细节：</p>
<ol>
<li>找到中间层某个较高分辨率的 feature map（比如 26×26×512）</li>
<li>把它做一个 reshape（空间变换）：<ul>
<li>把邻近的 2×2 像素拉平堆到通道维度</li>
<li>变成 13×13×2048（空间减小，通道增加）</li>
<li>类似 Pixel Shuffle 的反操作</li>
</ul>
</li>
<li>把 reshape 后的特征图和深层的 13×13×1024 的主特征图 <strong>在通道维度拼接</strong></li>
<li>得到一个更厚的 feature map（比如 13×13×3072），后面用来预测框</li>
</ol>
<h2 id="10-multi-scale（多尺度训练）"><a href="#10-multi-scale（多尺度训练）" class="headerlink" title="10.multi-scale（多尺度训练）"></a>10.multi-scale（多尺度训练）</h2><p>模型在训练过程中会每隔一段时间改变输入图像尺寸</p>
<h3 id="1-实现逻辑"><a href="#1-实现逻辑" class="headerlink" title="1.实现逻辑"></a>1.实现逻辑</h3><ul>
<li>每隔 <strong>10 个 batch</strong>，就 <strong>随机更换一次输入图片大小</strong></li>
<li>尺寸从 <strong>320 到 608</strong>，每次变化为 <strong>32 的倍数</strong></li>
<li>网络结构本身是完全卷积的（没有全连接），所以可以自适应输入大小</li>
<li>输出特征图大小会跟着输入变，但预测方式一样</li>
</ul>
<h3 id="2-优点"><a href="#2-优点" class="headerlink" title="2.优点"></a>2.优点</h3><ol>
<li>增强泛化能力，网络学会了适应不同分辨率的图，对实际应用（比如摄像头视频、各种输入源）更鲁棒</li>
<li><strong>小图（320×320）</strong> → 检测速度快，适合实时；<strong>大图（608×608）</strong> → 精度高，适合离线分析；一个模型搞定，不用单独训练多个</li>
<li>兼容不同尺寸输入</li>
</ol>
<h2 id="11-hi-res-detector（高分辨率检测器）"><a href="#11-hi-res-detector（高分辨率检测器）" class="headerlink" title="11.hi-res detector（高分辨率检测器）"></a>11.hi-res detector（高分辨率检测器）</h2><p>先用低分辨率训练模型，再切换高分辨率继续训练，提升最终检测精度，训练更高效。</p>
<h3 id="1-两段式流程"><a href="#1-两段式流程" class="headerlink" title="1.两段式流程"></a>1.两段式流程</h3><p>第一步：先用低分辨率训练</p>
<ul>
<li>比如 224×224 或 288×288</li>
<li>好处是：训练速度快，参数收敛快</li>
<li>网络学到了 <strong>粗结构、目标定位、分类特征</strong><br>第二步：切换成高分辨率继续训练</li>
<li>最后阶段，把输入图放大到 448×448 或 416×416（YOLOv2 默认是 416）</li>
<li>网络继续 fine-tune，学会适应更多细节</li>
</ul>
<h3 id="2-有效果"><a href="#2-有效果" class="headerlink" title="2.有效果"></a>2.有效果</h3><p>为什么后期再切换高分辨率很有效？</p>
<ul>
<li>之前低分辨率已经训练好了“看大概轮廓”的能力</li>
<li>高分辨率训练能学到 <strong>更准确的边界框和细节特征</strong></li>
<li>不需要从零开始，就能大幅提升检测效果，尤其是对 <strong>小目标</strong></li>
</ul>
<p>它和 <strong>multi-scale training</strong> 是互补的：</p>
<ul>
<li>hi-res 是训练流程中“先小后大”的策略</li>
<li>multi-scale 是在整个训练过程中“不断随机切换尺寸”的策略</li>
</ul>
<h2 id="12-总结"><a href="#12-总结" class="headerlink" title="12.总结"></a>12.总结</h2><h3 id="1-Batch-Normalization（批量归一化）​​"><a href="#1-Batch-Normalization（批量归一化）​​" class="headerlink" title="1.Batch Normalization（批量归一化）​​"></a>1.Batch Normalization（批量归一化）​​</h3><ul>
<li>​<strong>作用</strong>​：解决深度网络中的协变量偏移问题，稳定训练过程</li>
<li>​<strong>优势</strong>​：加速收敛、允许更大学习率、减少对初始化的依赖、轻微正则化效果</li>
<li>​<strong>应用</strong>​：在卷积层后普遍添加 BN，提升 mAP 约 2%</li>
</ul>
<h3 id="2-Hi-Res-Classifier（高分辨率分类器）​​"><a href="#2-Hi-Res-Classifier（高分辨率分类器）​​" class="headerlink" title="2.Hi-Res Classifier（高分辨率分类器）​​"></a>2.Hi-Res Classifier（高分辨率分类器）​​</h3><ul>
<li>​<strong>问题</strong>​：YOLOv1 使用低分辨率（224×224）预训练，导致检测时细节丢失</li>
<li>​<strong>改进</strong>​：先在 ImageNet（224×224）预训练，再微调至高分辨率（448×448），提升特征提取能力</li>
<li>​<strong>效果</strong>​：mAP 提升约 4%</li>
</ul>
<h3 id="3-Convolutional（全卷积结构）​​"><a href="#3-Convolutional（全卷积结构）​​" class="headerlink" title="3.Convolutional（全卷积结构）​​"></a>3.Convolutional（全卷积结构）​​</h3><ul>
<li>​<strong>替换全连接层</strong>​：改用卷积层支持任意输入尺寸，增强灵活性</li>
<li>​<strong>优势</strong>​：<ul>
<li>输入尺寸可变（如 320×320、416×416、608×608）</li>
<li>减少参数量，提升速度</li>
<li>为多尺度训练奠定基础</li>
</ul>
</li>
</ul>
<h3 id="4-Anchor-Boxes（锚框机制）​"><a href="#4-Anchor-Boxes（锚框机制）​" class="headerlink" title="4. Anchor Boxes（锚框机制）​"></a>4. Anchor Boxes（锚框机制）​</h3><ul>
<li>​<strong>改进点</strong>​：引入预定义的锚框模板（通过 K-Means 聚类得到），替代 YOLOv1 的随机预测</li>
<li>​<strong>优势</strong>​：<ul>
<li>提升边界框预测稳定性</li>
<li>通过聚类得到的锚框更贴合数据分布（如 VOC 数据集的常见物体宽高比）</li>
</ul>
</li>
<li>​<strong>效果</strong>​：召回率显著提升，mAP 小幅提高</li>
</ul>
<h3 id="5-New-Network-Darknet-19"><a href="#5-New-Network-Darknet-19" class="headerlink" title="5. New Network: Darknet-19"></a>5. New Network: Darknet-19</h3><ul>
<li>​<strong>结构</strong>​：19 层卷积 + 5 层池化，全卷积设计，无全连接层</li>
<li>​<strong>特点</strong>​：<ul>
<li>使用 3×3 小卷积核（类似 VGG）</li>
<li>每层后接 BN 和 Leaky ReLU</li>
<li>轻量高效，适合实时检测</li>
</ul>
</li>
<li>​<strong>对比</strong>​：比 YOLOv1 的 GoogLeNet 更简洁，精度更高</li>
</ul>
<h3 id="6-Dimension-Priors（维度先验）​​"><a href="#6-Dimension-Priors（维度先验）​​" class="headerlink" title="6. Dimension Priors（维度先验）​​"></a>6. Dimension Priors（维度先验）​​</h3><ul>
<li>​<strong>方法</strong>​：通过 K-Means 聚类统计训练集中真实框的宽高分布，作为锚框尺寸的初始化</li>
<li>​<strong>距离度量</strong>​：使用 IOU 而非欧式距离，更贴合检测任务需求</li>
<li>​<strong>效果</strong>​：提升边界框回归的准确性</li>
</ul>
<h3 id="7-Location-Prediction（位置预测优化）​"><a href="#7-Location-Prediction（位置预测优化）​" class="headerlink" title="7. Location Prediction（位置预测优化）​"></a>7. Location Prediction（位置预测优化）​</h3><ul>
<li>​<strong>问题</strong>​：YOLOv1 直接预测坐标，易导致训练不稳定</li>
<li>​<strong>改进</strong>​：<ul>
<li>预测相对于锚框的偏移量（tx, ty, tw, th）</li>
<li>通过 Sigmoid 限制中心点不超出当前网格，避免越界</li>
</ul>
</li>
<li>​<strong>效果</strong>​：训练更稳定，定位更精准</li>
</ul>
<h3 id="8-Passthrough-Layer（通道连接）​​"><a href="#8-Passthrough-Layer（通道连接）​​" class="headerlink" title="8. Passthrough Layer（通道连接）​​"></a>8. Passthrough Layer（通道连接）​​</h3><ul>
<li>​<strong>作用</strong>​：融合浅层高分辨率特征（细节）和深层语义特征，提升小目标检测能力</li>
<li>​<strong>实现</strong>​：将中间层（如 26×26）的特征图重塑后与深层（13×13）特征拼接</li>
<li>​<strong>效果</strong>​：mAP 提升约 1%</li>
</ul>
<h3 id="9-Multi-Scale-Training（多尺度训练）​​"><a href="#9-Multi-Scale-Training（多尺度训练）​​" class="headerlink" title="9. Multi-Scale Training（多尺度训练）​​"></a>9. Multi-Scale Training（多尺度训练）​​</h3><ul>
<li>​<strong>方法</strong>​：每 10 个 batch 随机切换输入尺寸（320×320 到 608×608，步长 32）</li>
<li>​<strong>优势</strong>​：<ul>
<li>增强模型对不同尺寸输入的鲁棒性</li>
<li>单一模型适配多种分辨率需求（小图快，大图准）</li>
</ul>
</li>
</ul>
<h3 id="10-Hi-Res-Detector（高分辨率检测器）​​"><a href="#10-Hi-Res-Detector（高分辨率检测器）​​" class="headerlink" title="10. Hi-Res Detector（高分辨率检测器）​​"></a>10. Hi-Res Detector（高分辨率检测器）​​</h3><ul>
<li>​<strong>策略</strong>​：先低分辨率（224×224）预训练，再切换至高分辨率（如 416×416）微调</li>
<li>​<strong>效果</strong>​：平衡训练效率与最终精度，尤其提升小目标检测能力</li>
</ul>
<h3 id="11-整体改进效果​"><a href="#11-整体改进效果​" class="headerlink" title="11. 整体改进效果​"></a>11. 整体改进效果​</h3><ul>
<li>​<strong>精度</strong>​：mAP 从 YOLOv1 的 63.4% 提升至 78.6%（VOC 2007）</li>
<li>​<strong>速度</strong>​：保持实时性（67 FPS on Titan X）</li>
<li>​<strong>灵活性</strong>​：支持多尺度输入，适应不同应用场景</li>
</ul>
<p>YOLOv2 的改进围绕 ​<strong>稳定性</strong>​（BN、位置预测）、<strong>灵活性</strong>​（全卷积、多尺度）、<strong>数据驱动</strong>​（锚框聚类）展开，同时通过结构优化（Darknet-19、Passthrough）平衡速度与精度，奠定了后续 YOLO 系列的基础设计理念。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://keychankc.github.io/2025/04/21/012-yolo-yolov2/" title="[YOLO系列②] YOLOv2十大改进点解析">https://keychankc.github.io/2025/04/21/012-yolo-yolov2/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
              <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag"># 目标检测</a>
              <a href="/tags/YOLO/" rel="tag"># YOLO</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/04/17/011-yolo-eval-metrics-yolov1/" rel="prev" title="[YOLO系列①] 物体检测评估指标和YOLO-v1实现思路">
                  <i class="fa fa-angle-left"></i> [YOLO系列①] 物体检测评估指标和YOLO-v1实现思路
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/04/29/013-yolo-yolov3-yolov4/" rel="next" title="[YOLO系列③] YOLOv3和YOLOv4优化策略">
                  [YOLO系列③] YOLOv3和YOLOv4优化策略 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">206k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/04/21/012-yolo-yolov2/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
