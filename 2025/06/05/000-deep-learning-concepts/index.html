<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"keychankc.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="深度学习因为涉及大量的专业术语和复杂概念，系统性地整理这些内容非常有必要。这不仅有助于构建清晰的知识框架，还能避免理解偏差，让沟通更顺畅。同时，随着技术的快速发展，定期梳理这些概念也能帮助我们及时跟上领域前沿。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习的概念们">
<meta property="og:url" content="https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="深度学习因为涉及大量的专业术语和复杂概念，系统性地整理这些内容非常有必要。这不仅有助于构建清晰的知识框架，还能避免理解偏差，让沟通更顺畅。同时，随着技术的快速发展，定期梳理这些概念也能帮助我们及时跟上领域前沿。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-06-05T09:03:25.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.536Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="概念">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/","path":"2025/06/05/000-deep-learning-concepts/","title":"深度学习的概念们"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习的概念们 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="nav-text">张量（Tensor）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Neural-Network%EF%BC%89"><span class="nav-text">神经网络（Neural Network）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%EF%BC%88Weight%EF%BC%89"><span class="nav-text">权重（Weight）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E7%BD%AE%EF%BC%88Bias%EF%BC%89"><span class="nav-text">偏置（Bias）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Forward-Pass%EF%BC%89"><span class="nav-text">前向传播（Forward Pass）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Backward-Pass%EF%BC%89"><span class="nav-text">反向传播（Backward Pass）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89"><span class="nav-text">损失函数（Loss Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%EF%BC%88Gradient%EF%BC%89"><span class="nav-text">梯度（Gradient）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="nav-text">梯度下降（Gradient Descent）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Learning-Rate%EF%BC%89"><span class="nav-text">学习率（Learning Rate）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="nav-text">优化器（Optimizer）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="nav-text">激活函数（Activation Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Normalization%EF%BC%89"><span class="nav-text">归一化（Normalization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="nav-text">正则化（Regularization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%EF%BC%88Momentum%EF%BC%89"><span class="nav-text">动量（Momentum）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%88One-Hot-Encoding%EF%BC%89"><span class="nav-text">独热编码（One-Hot Encoding）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%EF%BC%88Pooling%EF%BC%89"><span class="nav-text">池化（Pooling）</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/keychankc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习的概念们 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习的概念们
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-05 17:03:25" itemprop="dateCreated datePublished" datetime="2025-06-05T17:03:25+08:00">2025-06-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/06/05/000-deep-learning-concepts/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/06/05/000-deep-learning-concepts/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>38 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><p>张量是深度学习中用于存储和计算数据的基本数据结构，可以把它理解为高维数组的泛化。</p>
<p>定义和结构</p>
<p>张量是一个带有形状（shape）和数值（data）的多维数组结构，0 维张量是标量，1 维张量是向量，2 维张量是矩阵，高阶张量就是三维及以上。</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>数学维度</strong></th>
<th><strong>举例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>标量</td>
<td>0D</td>
<td>5</td>
</tr>
<tr>
<td>向量</td>
<td>1D</td>
<td>[1, 2, 3]</td>
</tr>
<tr>
<td>矩阵</td>
<td>2D</td>
<td><code>[[1, 2], [3, 4]]</code></td>
</tr>
<tr>
<td>三维张量</td>
<td>3D</td>
<td><code>[[[...], [...]], [...]]</code></td>
</tr>
<tr>
<td>n 维张量</td>
<td>nD</td>
<td>形如 shape&#x3D;(2, 3, 28, 28)</td>
</tr>
</tbody></table>
<p>用法</p>
<table>
<thead>
<tr>
<th><strong>用法</strong></th>
<th><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td>表示数据</td>
<td>各种类型的输入数据（图像、文本、音频、视频等），支持多维结构</td>
</tr>
<tr>
<td>表示模型参数</td>
<td>网络中的权重、偏置、卷积核等参数</td>
</tr>
<tr>
<td>数值计算</td>
<td>所有计算（矩阵乘法、卷积、激活、归一化等）的基本单位</td>
</tr>
<tr>
<td>自动求导</td>
<td>追踪计算图，实现反向传播和梯度更新</td>
</tr>
<tr>
<td>GPU 加速</td>
<td>适配并行计算，可高效运行在 GPU 上</td>
</tr>
<tr>
<td>高维数据操作</td>
<td>提供强大的维度变换、索引、切片、广播等操作，方便处理复杂结构</td>
</tr>
</tbody></table>
<hr>
<h2 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h2><p>神经网络是一个由大量简单计算单元（称为“人工神经元“或“节点”）​​分层互联而成的计算模型。</p>
<p>基本组成单元（神经元&#x2F;节点）</p>
<ul>
<li><strong>输入：</strong> 接收来自前一层或其他源的数据（<strong>特征</strong>）和<strong>权重</strong></li>
<li><strong>计算：</strong> 对输入进行<strong>加权求和</strong>，并加上一个<strong>偏置（bias）</strong>，这个值代表该节点接收到的“总刺激”</li>
<li><strong>激活：</strong> 将加权求和的结果传入一个<strong>非线性激活函数</strong>（如 ReLU、Sigmoid、Tanh）。激活函数是神经网络具有强大表达能力、避免退化为线性模型的关键，它决定该节点是否“激活”以及激活的强度</li>
<li><strong>输出：</strong> 将激活函数处理后的值传递给下一层神经元</li>
</ul>
<p>层级结构</p>
<ul>
<li><strong>输入层：</strong> 接收原始数据（如图像像素值、文本编码、传感器数据等），每个节点对应一个输入特征</li>
<li><strong>隐藏层：</strong> 位于输入层和输出层之间，可以有一层或多层，用于对特征进行层层变换和抽象提取，隐藏层的节点数量及其连接方式决定模型的容量和复杂度，是信息处理的核心区域，特征在此逐步被抽象化</li>
<li><strong>输出层：</strong> 产生最终的预测结果（如分类类别、数值、概率分布等）。激活函数通常根据任务而定，例如：Sigmoid 用于二分类概率，Softmax 用于多分类概率，线性回归则可能不使用激活函数<br>层与层之间的节点连接可以是<strong>全连接</strong>（即每个节点与下一层所有节点相连），也可以采用其他连接方式（如卷积）。每条连接都对应一个<strong>权重（Weight）</strong>，它是神经网络的核心参数，表示某个输入特征对当前节点的重要性。每个节点还会有一个<strong>偏置（Bias）</strong>，这是一个独立的可学习参数，相当于一个“激活门槛”，即便输入总和为零也能激活节点。</li>
</ul>
<p>神经网络的运行原理<br>神经网络的运行机制围绕两个核心过程展开：<strong>前向传播（Forward Propagation）</strong> 和 <strong>反向传播（Back Propagation）</strong>。</p>
<ol>
<li><strong>前向传播：</strong> 原始输入数据从输入层开始，逐层传递。在每一层，每个节点对输入进行加权求和、加偏置并通过激活函数处理，最终到达输出层，得出模型的预测结果。其目的是计算给定输入下的输出，即预测值。</li>
<li><strong>学习（训练）机制：</strong><br> 首先定义一个<strong>损失函数</strong>（如均方误差 MSE、交叉熵 Cross-Entropy），用于衡量预测值与真实目标之间的差距（损失）。<br> 接着，通过<strong>反向传播</strong>算法计算损失函数对网络中每一个权重和偏置的<strong>梯度（Gradient）</strong>，这个梯度表示为了减小损失，该参数应如何调整。<br> 然后利用这些梯度，通过优化算法（如随机梯度下降 SGD、Adam、RMSprop 等）更新所有参数。简单地说，就是：<code>新权重 = 旧权重 - 学习率 × 损失对该权重的梯度。</code><br> 这个过程会在整个训练数据集上反复执行，通常以多个小批量进行，每次完整遍历一轮称为一个 Epoch。通过不断迭代，网络的预测将越来越接近目标。</li>
</ol>
<p>核心流程：​</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入数据 -&gt; 前向传播 -&gt; 计算损失 -&gt; 反向传播求梯度 -&gt; 优化算法更新参数 -&gt; （重复前向传播...）-&gt; 模型收敛（损失不再显著下降或达到预定条件）</span><br></pre></td></tr></table></figure>

<p>用法</p>
<table>
<thead>
<tr>
<th><strong>应用类型</strong></th>
<th><strong>说明</strong></th>
<th><strong>常用网络</strong></th>
</tr>
</thead>
<tbody><tr>
<td>图像分类</td>
<td>输入图片，输出类别</td>
<td>CNN（卷积神经网络）</td>
</tr>
<tr>
<td>语音识别</td>
<td>音频转文字</td>
<td>RNN、Transformer</td>
</tr>
<tr>
<td>文本情感分析</td>
<td>判断情感极性</td>
<td>RNN、BERT</td>
</tr>
<tr>
<td>机器翻译</td>
<td>英文转中文等</td>
<td>Transformer</td>
</tr>
<tr>
<td>图像生成</td>
<td>生成照片&#x2F;艺术图</td>
<td>GAN</td>
</tr>
<tr>
<td>时间序列预测</td>
<td>股票、气温预测</td>
<td>RNN、LSTM</td>
</tr>
<tr>
<td>特征提取</td>
<td>提取中间层特征</td>
<td>CNN、预训练模型</td>
</tr>
</tbody></table>
<hr>
<h2 id="权重（Weight）"><a href="#权重（Weight）" class="headerlink" title="权重（Weight）"></a>权重（Weight）</h2><p>权重是连接神经元之间的系数，表示一个输入对输出的重要程度。换句话说，<strong>权重是模型学习的参数</strong>，它决定了每个输入特征对预测结果的影响。</p>
<p>权重本质上是一个张量，存储在神经元之间（输入层→隐藏层、隐藏层→隐藏层、隐藏层→输出层）的连线上，量化上游神经元信号对下游神经元的<strong>影响程度</strong>。其初始值常随机设置（如正态分布），训练过程通过优化算法（如SGD&#x2F;Adam）基于损失函数反馈动态调整。</p>
<p>核心机制</p>
<ol>
<li>​<strong>线性变换</strong>​：在神经元激活前执行 加权求和 操作（即输入特征向量 $X$ 与权重向量 $W$ 的点积）：  输出 &#x3D; $w1x1 + w2x2 + … + wnxn$+ 偏置项</li>
<li>​<strong>特征重要性学习</strong>​：网络通过反向传播调整权重：<ul>
<li>正权重​ → 增强信号 → 正向关联特征</li>
<li>​负权重​ → 抑制信号 → 负向关联特征</li>
<li>​近似零权重​ → 屏蔽无效特征</li>
</ul>
</li>
<li>​<strong>知识载体</strong>​：训练后稳定的权重矩阵编码了模型从数据中学习到的核心内在逻辑（如边缘检测器、语义特征提取器）。</li>
</ol>
<p>用法场景</p>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td>前向传播</td>
<td>决定输入特征对输出的贡献</td>
</tr>
<tr>
<td>模型训练</td>
<td>通过梯度下降不断更新</td>
</tr>
<tr>
<td>模型存储</td>
<td>训练完成后模型参数即为权重和偏置集合</td>
</tr>
<tr>
<td>迁移学习</td>
<td>已训练模型的权重可用于新任务，作为预训练参数</td>
</tr>
</tbody></table>
<hr>
<h2 id="偏置（Bias）"><a href="#偏置（Bias）" class="headerlink" title="偏置（Bias）"></a>偏置（Bias）</h2><p>偏置（Bias）​​ 是神经网络的另一类核心可学习参数，与权重协同工作，为模型提供<strong>灵活性</strong>和<strong>表达能力</strong>。<br>偏置本质是一个张量，​是独立于输入特征的存在，它对神经元的加权求和结果 （$z &#x3D; W·X$）进行<strong>平移操作</strong>​：  $输出 &#x3D; z + b &#x3D; W·X + b$，一般与权重同步优化，初始值常设为0或较小值（如0.01）。</p>
<p>核心机制</p>
<ol>
<li><strong>打破原点对称性</strong>​：若无偏置，所有神经元的计算起点强制为原点（$X&#x3D;0 → z&#x3D;0$），这就限制了模型的表达能力。偏置允许决策边界（或特征空间划分面）​<strong>自由平移</strong>，突破坐标原点的束缚，使得模型拟合能力提高。</li>
<li>​<strong>阈值调节器</strong>​：可以在激活函数前调整信号的基准位置（以Sigmoid为例）：<ul>
<li>$b &gt; 0$ ➔ 加权输出 $z$ 整体右移，这样更易激活神经元</li>
<li>$b &lt; 0$ ➔ $z$ 整体左移，这样使得神经元更难激活</li>
</ul>
</li>
<li><strong>补偿数据偏差</strong>​：学习数据中的系统偏移（如所有样本的标签存在固定偏移量）。比如预测房价时，即便所有输入特征为0（如零面积房屋），房价也不应为0，偏置可学习该基准值。</li>
</ol>
<p>用法场景</p>
<table>
<thead>
<tr>
<th><strong>应用场景</strong></th>
<th><strong>是否使用偏置</strong></th>
<th><strong>作用与说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>全连接层（Dense &#x2F; Linear）</strong></td>
<td>默认使用</td>
<td>提供输出的可调“起点”，允许模型学习数据的平移特性</td>
</tr>
<tr>
<td><strong>卷积层（Conv2d &#x2F; Conv1d &#x2F; Conv3d）</strong></td>
<td>默认使用</td>
<td>给每个卷积核增加偏置，有助于处理背景亮度或全局偏移等问题</td>
</tr>
<tr>
<td><strong>循环神经网络（RNN、LSTM、GRU）</strong></td>
<td>使用多个偏置项</td>
<td>控制不同门（输入门、遗忘门、输出门等）的激活偏移，提高时序建模能力</td>
</tr>
<tr>
<td><strong>Transformer 中的注意力机制（Attention）</strong></td>
<td>一般在线性变换时使用</td>
<td>在生成 Query、Key、Value 向量时常通过含偏置的线性层增强表达力</td>
</tr>
<tr>
<td><strong>批归一化（BatchNorm）之后</strong></td>
<td>常关闭偏置</td>
<td>因为 BatchNorm 自带偏移与缩放参数（γ 与 β），原始层的 bias 可省略</td>
</tr>
<tr>
<td><strong>激活函数之前（如 ReLU、Sigmoid）</strong></td>
<td>使用偏置</td>
<td>通过加偏置调整输入激活值范围，使得激活函数在有效区间工作</td>
</tr>
<tr>
<td><strong>AutoEncoder（自编码器）</strong></td>
<td>编码器与解码器中的线性层都使用</td>
<td>提高压缩与重构能力</td>
</tr>
<tr>
<td><strong>生成对抗网络（GAN）</strong></td>
<td>生成器和判别器都常含偏置</td>
<td>增强网络表达能力</td>
</tr>
<tr>
<td><strong>图神经网络（GNN）</strong></td>
<td>在图卷积层中常包含</td>
<td>保证节点在无邻居输入时仍有输出可能</td>
</tr>
<tr>
<td><strong>深度残差网络（ResNet）</strong></td>
<td>通常保留偏置</td>
<td>残差块内的卷积层默认使用偏置，提升表示能力</td>
</tr>
<tr>
<td><strong>迁移学习（Fine-tune）</strong></td>
<td>或冻结偏置</td>
<td>在微调阶段可能保留或冻结偏置，看任务需求而定</td>
</tr>
</tbody></table>
<hr>
<h2 id="前向传播（Forward-Pass）"><a href="#前向传播（Forward-Pass）" class="headerlink" title="前向传播（Forward Pass）"></a>前向传播（Forward Pass）</h2><p>前向传播是神经网络的<strong>数据加工流水线</strong>​，将原始输入数据从输入层逐层向前流动，通过加权计算与非线性变换，最终在输出层生成预测结果。这也是神经网络进行<strong>特征抽象</strong>的核心过程。</p>
<p>运行原理<br>神经元视角：<code>输出 = 激活函数(权重x输入 + 偏置)</code>，就像流水线上的加工站，接收原料（输入），用特定工具（权重）加工，质检员（激活函数）决定是否放行。<br>网络层视角：<code>输入层 → 隐藏层₁ → ... → 隐藏层ₙ → 输出层</code>  ，类似工厂装配线：</p>
<ol>
<li>初级车间：提取边缘&#x2F;颜色特征（浅层网络）</li>
<li>中级车间：组装成纹理&#x2F;部件（中层网络）</li>
<li>高级车间：合成完整物体（深层网络）<br>在代码中的典型用法（PyTorch为例）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">128</span>)   <span class="comment"># 第一层：全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)    <span class="comment"># 第二层：输出层（10类）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)         <span class="comment"># 线性变换</span></span><br><span class="line">        x = F.relu(x)           <span class="comment"># 非线性激活</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)         <span class="comment"># 输出层线性变换</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>作用</p>
<ol>
<li><strong>预测输出</strong>：输入样本经过网络计算预测结果</li>
<li><strong>计算损失</strong>：前向传播的结果与真实标签比较，得到损失值</li>
<li><strong>反向传播的基础</strong>：只有先完成前向传播，才能基于损失计算梯度，执行反向传播和参数更新</li>
</ol>
<hr>
<h2 id="反向传播（Backward-Pass）"><a href="#反向传播（Backward-Pass）" class="headerlink" title="反向传播（Backward Pass）"></a>反向传播（Backward Pass）</h2><p>反向传播（Back Propagation）是训练神经网络的核心算法，它通过<strong>从输出层向输入层反向传递误差信号</strong>来计算网络中每个参数的梯度，从而用梯度下降法优化参数。</p>
<p>工作流程：</p>
<ol>
<li><strong>前向传播（Forward Pass）</strong>：输入数据经过网络一层层传播，得到预测结果。</li>
<li><strong>计算损失</strong>：使用损失函数评估预测值与真实值之间的误差。</li>
<li><strong>反向传播（Backward Pass）</strong>：<ul>
<li>从输出层开始，计算损失函数对输出的梯度</li>
<li>利用链式法则，逐层向前计算每个参数的梯度（即损失函数对各层参数的导数）</li>
</ul>
</li>
<li><strong>参数更新</strong>：使用梯度下降等优化算法，根据计算出的梯度对每个参数进行更新。</li>
</ol>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>模型训练过程中的参数更新</strong></td>
<td>与优化器配合，自动计算并更新网络中每一层的权重和偏置</td>
</tr>
<tr>
<td><strong>计算梯度信息</strong></td>
<td>使模型能够感知“如何调整”才能减小误差</td>
</tr>
<tr>
<td><strong>实现自动微分（autograd）机制</strong></td>
<td>如 PyTorch &#x2F; TensorFlow 内部都基于反向传播自动求导</td>
</tr>
<tr>
<td><strong>调试模型收敛问题</strong></td>
<td>通过监控梯度是否为0、是否爆炸&#x2F;消失，判断训练是否正常</td>
</tr>
</tbody></table>
<hr>
<h2 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h2><p>损失函数是训练过程中的“评价器”，它衡量模型预测结果与真实值之间的差距，核心组成如下：</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>预测值（Output &#x2F; Logits）</strong></td>
<td>模型输出的结果（可能是分数、概率或回归值）</td>
</tr>
<tr>
<td><strong>真实标签（Ground Truth）</strong></td>
<td>数据集中真实的目标值，用于监督学习</td>
</tr>
<tr>
<td><strong>误差度量方式（Metric）</strong></td>
<td>衡量“预测值”和“真实值”之间距离的方法，如L1、L2、交叉熵等</td>
</tr>
<tr>
<td><strong>可微性设计</strong></td>
<td>损失函数需具备可导性，以便参与反向传播计算梯度</td>
</tr>
</tbody></table>
<p>工作原理<br>损失函数的核心机制是<strong>将模型的“预测结果”转化为“可以优化的标量反馈”</strong>，从而驱动神经网络更新参数。</p>
<ol>
<li><strong>前向传播阶段</strong>：<ul>
<li>模型对输入数据做出预测，输出结果如 $\hat{y}$</li>
<li>损失函数将 $\hat{y}$ 与真实值 $y$ 比较，计算出一个损失值 $L(\hat{y}, y)$</li>
</ul>
</li>
<li><strong>反向传播阶段</strong>：<ul>
<li>利用损失值对网络参数反向求导，生成梯度</li>
<li>梯度引导优化器调整模型参数以最小化损失</li>
</ul>
</li>
</ol>
<blockquote>
<p>损失值越大 → 模型越差，损失值越小 → 模型越接近理想预测</p>
</blockquote>
<p>用法：</p>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>指导模型学习方向</strong></td>
<td>通过损失函数提供反馈，让模型知道哪些预测是错误的、需要纠正的</td>
</tr>
<tr>
<td><strong>计算梯度的起点</strong></td>
<td>所有参数的梯度都基于损失函数的导数进行计算</td>
</tr>
<tr>
<td><strong>衡量模型性能变化趋势</strong></td>
<td>训练过程中，监控 loss 能帮助判断是否收敛、是否过拟合等</td>
</tr>
<tr>
<td><strong>任务定制</strong></td>
<td>分类、回归、分割等任务使用不同类型的损失函数（如交叉熵、MSE、Dice Loss）</td>
</tr>
<tr>
<td><strong>多目标训练</strong></td>
<td>可组合多个损失函数进行加权，用于多任务学习或辅助任务引导训练</td>
</tr>
</tbody></table>
<p>常见的损失函数：</p>
<table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>常见损失函数</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>分类</strong></td>
<td>交叉熵损失（Cross Entropy）</td>
</tr>
<tr>
<td><strong>回归</strong></td>
<td>均方误差（MSE）、平均绝对误差（MAE）</td>
</tr>
<tr>
<td><strong>分割 &#x2F; 检测</strong></td>
<td>Dice Loss、IoU Loss、Focal Loss</td>
</tr>
<tr>
<td><strong>对比学习</strong></td>
<td>Triplet Loss、Contrastive Loss</td>
</tr>
</tbody></table>
<hr>
<h2 id="梯度（Gradient）"><a href="#梯度（Gradient）" class="headerlink" title="梯度（Gradient）"></a>梯度（Gradient）</h2><p>梯度是模型学习方向的“指南针”，是告诉每个参数“往哪走才能更好”的关键信息。梯度的本质是一个向量，它反映了<strong>损失函数对模型参数的偏导数</strong>，从组成上看，梯度涉及以下要素：</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>损失函数（Loss）</strong></td>
<td>衡量预测值与真实值之间的误差，是梯度的“源头”</td>
</tr>
<tr>
<td><strong>模型参数（权重、偏置）</strong></td>
<td>需要被优化的目标，是梯度的“作用对象”</td>
</tr>
<tr>
<td><strong>偏导数（Partial Derivative）</strong></td>
<td>描述损失在每个参数维度上的变化率</td>
</tr>
<tr>
<td><strong>梯度向量</strong></td>
<td>多维参数对应多个偏导，整体形成一个方向性的向量</td>
</tr>
</tbody></table>
<p>工作原理<br>梯度是<strong>指明函数在当前点上最陡峭上升（或下降）的方向和速率</strong>。<br>在深度学习中，我们关心的是如何<strong>减小损失函数的值</strong>，因此会反向使用梯度信息：</p>
<ol>
<li><strong>梯度的方向</strong>：告诉我们如果参数朝哪个方向微调，损失会变大。</li>
<li><strong>负梯度方向</strong>：我们采用负梯度方向作为更新方向，以减少损失。</li>
<li><strong>梯度大小</strong>：决定了更新步长的大小（配合学习率使用）。</li>
</ol>
<blockquote>
<p>举个例子：若某个参数的梯度是 0.5，就表示该参数若增加一点，损失会相应增加（损失函数对该参数是“上升”趋势），所以我们要朝“反方向”调整它。</p>
</blockquote>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>参数更新</strong></td>
<td>梯度用于指导优化器（如 SGD、Adam）更新每一层的参数</td>
</tr>
<tr>
<td><strong>误差传播（反向传播）</strong></td>
<td>反向传播过程的核心计算就是梯度链式相乘</td>
</tr>
<tr>
<td><strong>调试训练过程</strong></td>
<td>梯度消失&#x2F;爆炸等问题是训练失败的重要信号</td>
</tr>
<tr>
<td><strong>自动求导</strong></td>
<td>框架（如 PyTorch）通过自动微分机制计算每个参数的梯度</td>
</tr>
<tr>
<td><strong>神经网络学习机制的本质体现</strong></td>
<td>训练的全部过程，其实就是围绕着“计算并利用梯度”展开的</td>
</tr>
</tbody></table>
<hr>
<h2 id="梯度下降（Gradient-Descent）"><a href="#梯度下降（Gradient-Descent）" class="headerlink" title="梯度下降（Gradient Descent）"></a>梯度下降（Gradient Descent）</h2><p>梯度下降是一种“聪明的试错”方法，通过沿着误差减少的方向不断微调参数，使模型逐步逼近最优性能。它由以下几个核心要素构成：</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>模型参数</strong>（如权重 W、偏置 b）</td>
<td>需要被优化的变量</td>
</tr>
<tr>
<td><strong>损失函数</strong> $\mathcal{L}$</td>
<td>衡量预测结果与真实值之间的误差</td>
</tr>
<tr>
<td><strong>梯度</strong> $\nabla \mathcal{L}$</td>
<td>损失函数对每个参数的导数，表示误差随参数变化的趋势</td>
</tr>
<tr>
<td><strong>学习率</strong> $\eta$</td>
<td>控制每次参数更新的步长，决定收敛速度和稳定性</td>
</tr>
</tbody></table>
<p>工作原理<br>在当前点上，计算损失函数对参数的梯度，朝梯度反方向移动参数，减小损失。<br>具体步骤如下：</p>
<ol>
<li><strong>前向传播</strong>：模型对输入数据生成预测结果；</li>
<li><strong>计算损失</strong>：通过损失函数得到预测误差；</li>
<li><strong>反向传播</strong>：计算损失函数对每个参数的梯度；</li>
<li><strong>参数更新</strong>：按以下公式更新每个参数：<br> $\theta \leftarrow \theta - \eta \cdot \nabla_\theta \mathcal{L}$    $\theta$：待更新的参数  $\eta$：学习率  $\nabla_\theta \mathcal{L}$：该参数的梯度<br>这个过程持续进行，直到损失函数收敛或达到设定轮数。</li>
</ol>
<p>梯度下降广泛应用于深度学习模型训练，是模型“学习”的基础：</p>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>神经网络训练的核心优化方法</strong></td>
<td>所有模型参数的更新都依赖梯度下降</td>
</tr>
<tr>
<td><strong>与反向传播结合</strong></td>
<td>梯度由反向传播计算，梯度下降据此更新参数</td>
</tr>
<tr>
<td><strong>支持多种变体</strong></td>
<td>包括批量（Batch）、小批量（Mini-batch）、随机（SGD）三种方式</td>
</tr>
<tr>
<td><strong>可与高级优化器结合</strong></td>
<td>如 Adam、RMSProp、Momentum 等都基于梯度下降改进</td>
</tr>
<tr>
<td><strong>调节学习率以控制训练过程</strong></td>
<td>过小 → 学得慢；过大 → 震荡或发散；可配合调度策略如余弦退火、Warmup等</td>
</tr>
</tbody></table>
<p>梯度下降三种常见形式</p>
<table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>特点</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>批量梯度下降</strong></td>
<td>每轮使用全部数据，计算精确，但慢</td>
<td>小数据集</td>
</tr>
<tr>
<td><strong>随机梯度下降（SGD）</strong></td>
<td>每个样本单独更新，波动大但快</td>
<td>大数据训练、在线学习</td>
</tr>
<tr>
<td><strong>小批量梯度下降</strong></td>
<td>每次使用部分样本，速度与稳定性兼顾</td>
<td>实际最常用</td>
</tr>
</tbody></table>
<hr>
<h2 id="学习率（Learning-Rate）"><a href="#学习率（Learning-Rate）" class="headerlink" title="学习率（Learning Rate）"></a>学习率（Learning Rate）</h2><p>学习率（Learning Rate）是梯度下降等优化算法中的一个关键超参数，控制着每次模型参数更新的“步长”</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>学习率值（η 或 lr）</strong></td>
<td>一个标量，表示每一步更新沿梯度方向移动的幅度</td>
</tr>
<tr>
<td><strong>优化器</strong></td>
<td>学习率是优化器（如SGD、Adam等）的输入</td>
</tr>
<tr>
<td><strong>梯度信息</strong></td>
<td>与学习率配合决定参数的实际更新量</td>
</tr>
<tr>
<td><strong>动态调整策略（可选）</strong></td>
<td>可以采用学习率调度器根据训练进度动态变化</td>
</tr>
</tbody></table>
<p>工作原理：</p>
<blockquote>
<p><strong>决定梯度更新的“步长”大小，直接影响模型收敛速度和稳定性。</strong></p>
</blockquote>
<p>在参数更新中，学习率控制了“走多远”：<br>$\theta \leftarrow \theta - \eta \cdot \nabla_\theta \mathcal{L}$</p>
<table>
<thead>
<tr>
<th><strong>情况</strong></th>
<th><strong>影响</strong></th>
</tr>
</thead>
<tbody><tr>
<td>学习率太大</td>
<td>更新过猛，可能震荡或发散，无法收敛</td>
</tr>
<tr>
<td>学习率太小</td>
<td>收敛速度慢，训练效率低，容易陷入局部最优或提前停止</td>
</tr>
</tbody></table>
<p>用法<br>学习率是训练过程中必须设置的重要超参数，使用方式包括：</p>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>固定学习率训练</strong></td>
<td>设置一个固定值，适用于简单问题或短时间训练</td>
</tr>
<tr>
<td><strong>逐步衰减（Step Decay）</strong></td>
<td>每隔一定epoch将学习率缩小，比如除以10</td>
</tr>
<tr>
<td><strong>余弦退火（Cosine Annealing）</strong></td>
<td>学习率按余弦函数周期变化，有利于跳出局部最优</td>
</tr>
<tr>
<td><strong>Warm-up</strong></td>
<td>在训练开始阶段先用较小学习率“热身”，避免初始梯度爆炸</td>
</tr>
<tr>
<td><strong>自适应学习率（如Adam）</strong></td>
<td>优化器内部根据梯度历史动态调整每个参数的学习率</td>
</tr>
<tr>
<td><strong>手动调参</strong></td>
<td>可视化 loss 曲线和验证指标，手动调整学习率以优化训练表现</td>
</tr>
</tbody></table>
<p>学习率常见设置技巧</p>
<table>
<thead>
<tr>
<th><strong>训练目标</strong></th>
<th><strong>建议策略</strong></th>
</tr>
</thead>
<tbody><tr>
<td>模型初期不稳定</td>
<td>使用 warm-up 或较小初始学习率</td>
</tr>
<tr>
<td>模型收敛缓慢</td>
<td>适当调大学习率或改用带动量的优化器</td>
</tr>
<tr>
<td>模型震荡或 loss 上下跳动</td>
<td>适当降低学习率</td>
</tr>
<tr>
<td>训练中后期精调</td>
<td>使用调度器降低学习率以精细收敛</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>学习率就像“油门”，踩得太猛容易翻车，踩得太轻走不动，只有控制得当，模型训练才能又快又稳。</strong></p>
</blockquote>
<hr>
<h2 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h2><p>优化器是神经网络训练中用于<strong>更新模型参数</strong>的模块，它基于<strong>梯度信息</strong>，决定每次如何调整参数以减小损失函数。</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>模型参数</strong>（weights &#x2F; biases）</td>
<td>需要被优化的变量</td>
</tr>
<tr>
<td><strong>损失函数的梯度</strong> $\nabla \mathcal{L}$</td>
<td>通过反向传播计算得到</td>
</tr>
<tr>
<td><strong>学习率（Learning Rate）</strong></td>
<td>决定每次更新的步长</td>
</tr>
<tr>
<td><strong>动量 &#x2F; 累积项</strong>（某些优化器有）</td>
<td>如 SGD with Momentum、Adam 等，引入历史信息增强稳定性</td>
</tr>
<tr>
<td><strong>优化规则</strong></td>
<td>每种优化器都有自己独特的参数更新公式和机制</td>
</tr>
</tbody></table>
<p>工作原理</p>
<blockquote>
<p><strong>利用损失函数的梯度信息，按一定规则更新参数，以使损失函数逐步下降，模型性能不断提升。</strong></p>
</blockquote>
<p>以最基础的梯度下降法为例：<br>$\theta \leftarrow \theta - \eta \cdot \nabla_\theta \mathcal{L}$</p>
<p>不同的优化器在此基础上会加入各种机制来提高训练效率与稳定性：</p>
<table>
<thead>
<tr>
<th><strong>优化器</strong></th>
<th><strong>关键机制</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>SGD（随机梯度下降）</strong></td>
<td>使用当前样本或小批量样本的梯度</td>
</tr>
<tr>
<td><strong>SGD + Momentum</strong></td>
<td>引入“惯性”，缓解震荡，增强方向感</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>自动调整每个参数的学习率，抑制震荡</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>综合 Momentum 和 RMSProp，自适应调整每个参数的更新步长</td>
</tr>
</tbody></table>
<p>用法：</p>
<table>
<thead>
<tr>
<th><strong>用法场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>训练神经网络</strong></td>
<td>反向传播后由优化器根据梯度更新模型参数</td>
</tr>
<tr>
<td><strong>选择合适优化器</strong></td>
<td>小模型可用 SGD，大模型推荐 Adam；分类任务多用 Adam 或 AdamW</td>
</tr>
<tr>
<td><strong>调整学习率</strong></td>
<td>大多数优化器需手动设置初始学习率，必要时配合调度器</td>
</tr>
<tr>
<td><strong>支持参数分组</strong></td>
<td>可为不同层设置不同学习率、权重衰减等超参数（如 param_groups）</td>
</tr>
<tr>
<td><strong>结合正则化</strong></td>
<td>优化器常配合 L2 正则（weight decay）防止过拟合</td>
</tr>
</tbody></table>
<p>常用优化器对比表</p>
<table>
<thead>
<tr>
<th><strong>优化器</strong></th>
<th><strong>是否使用动量</strong></th>
<th><strong>是否自适应学习率</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody><tr>
<td>SGD</td>
<td>否</td>
<td>否</td>
<td>简单、高效，适合小规模数据</td>
</tr>
<tr>
<td>SGD + Momentum</td>
<td>是</td>
<td>否</td>
<td>更平稳、更快收敛</td>
</tr>
<tr>
<td>RMSProp</td>
<td>否</td>
<td>是</td>
<td>更适合非平稳目标或循环网络</td>
</tr>
<tr>
<td>Adam</td>
<td>是</td>
<td>是</td>
<td>默认首选，适用于大多数任务</td>
</tr>
<tr>
<td>AdamW</td>
<td>是</td>
<td>是</td>
<td>Adam 的改进版，更适合 Transformer 类模型（如 BERT、ViT）</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>优化器是训练的“驾驶员”，它读取梯度信息，控制模型参数沿着“更优方向”前进，让模型不断逼近理想状态。</strong></p>
</blockquote>
<hr>
<h2 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h2><p>激活函数是<strong>神经网络中每个神经元内部的非线性变换函数</strong>，它决定了每一层输出的形式，控制信息的传播和模型的表达能力。</p>
<table>
<thead>
<tr>
<th><strong>组成部分</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>输入信号</strong>（一般为线性加权和）</td>
<td>来自前一层的输出，即 $z &#x3D; w^\top x + b$</td>
</tr>
<tr>
<td><strong>激活函数本体</strong></td>
<td>将输入信号变换为非线性输出，如 ReLU、Sigmoid、Tanh 等</td>
</tr>
<tr>
<td><strong>输出结果</strong></td>
<td>作为当前神经元的输出传递给下一层</td>
</tr>
</tbody></table>
<p>工作原理</p>
<blockquote>
<p><strong>对神经元的线性输出进行非线性映射，从而打破神经网络的线性性，使模型能学习复杂、高阶的表示。</strong></p>
</blockquote>
<p>没有激活函数的神经网络，哪怕有多层，整体仍是线性函数，无法解决实际问题中的非线性关系。</p>
<p>常见激活函数及机制特点：</p>
<table>
<thead>
<tr>
<th><strong>函数</strong></th>
<th><strong>表达式</strong></th>
<th><strong>机制说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>ReLU</strong></td>
<td>$f(x) &#x3D; \max(0, x)$</td>
<td>保留正值，抑制负值，收敛快，简洁高效</td>
</tr>
<tr>
<td><strong>Sigmoid</strong></td>
<td>$f(x) &#x3D; \frac{1}{1 + e^{-x}}$</td>
<td>输出范围(0, 1)，适用于概率建模，但梯度容易消失</td>
</tr>
<tr>
<td><strong>Tanh</strong></td>
<td>$f(x) &#x3D; \tanh(x)$</td>
<td>输出范围(-1, 1)，比Sigmoid居中，但仍可能梯度消失</td>
</tr>
<tr>
<td><strong>Leaky ReLU</strong></td>
<td>$f(x) &#x3D; \max(0.01x, x)$</td>
<td>改进ReLU，允许负值通过，避免神经元“死亡”</td>
</tr>
<tr>
<td><strong>GELU &#x2F; Swish</strong></td>
<td>更复杂</td>
<td>新型激活函数，用于BERT、Vision Transformer等</td>
</tr>
</tbody></table>
<p>用法<br>激活函数广泛用于神经网络的各层中，具体用法包括：</p>
<table>
<thead>
<tr>
<th><strong>应用场景</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>隐藏层激活函数</strong></td>
<td>通常使用 ReLU 或其变体，提升训练效率和非线性表达能力</td>
</tr>
<tr>
<td><strong>输出层激活函数</strong></td>
<td>根据任务类型选择：回归 → 无激活 &#x2F; ReLU，二分类 → Sigmoid，多分类 → Softmax</td>
</tr>
<tr>
<td><strong>防止梯度问题</strong></td>
<td>Tanh&#x2F;Sigmoid 早期常用，但容易梯度消失；ReLU 更常用于深层网络</td>
</tr>
<tr>
<td><strong>搭配正则化使用</strong></td>
<td>常与 BatchNorm、Dropout 一起增强训练稳定性和泛化能力</td>
</tr>
<tr>
<td><strong>注意选择位置</strong></td>
<td>激活函数通常在每层线性操作（Linear &#x2F; Conv）之后使用：</td>
</tr>
</tbody></table>
<p>激活函数的对比简表：</p>
<table>
<thead>
<tr>
<th><strong>函数</strong></th>
<th><strong>非线性</strong></th>
<th><strong>导数计算</strong></th>
<th><strong>是否零均值</strong></th>
<th><strong>是否抗梯度消失</strong></th>
<th><strong>常用位置</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ReLU</td>
<td>✅</td>
<td>简单</td>
<td>否</td>
<td>✅</td>
<td>隐藏层</td>
</tr>
<tr>
<td>Tanh</td>
<td>✅</td>
<td>复杂</td>
<td>✅</td>
<td>一定程度✅</td>
<td>RNN 中常用</td>
</tr>
<tr>
<td>Sigmoid</td>
<td>✅</td>
<td>复杂</td>
<td>否</td>
<td>❌</td>
<td>输出层（二分类）</td>
</tr>
<tr>
<td>Softmax</td>
<td>✅</td>
<td>稍复杂</td>
<td>-</td>
<td>-</td>
<td>输出层（多分类）</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>激活函数是神经网络的“灵魂”，它赋予网络理解非线性世界的能力，使模型不仅能加减乘除，更能思考弯弯绕绕。</strong></p>
</blockquote>
<hr>
<h2 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h2><p>用于<strong>标准化神经网络层输入</strong>，它通过<strong>调整数据的均值和方差</strong>，来解决深度学习模型训练中常见的问题，比如内部协变量偏移、梯度消失&#x2F;爆炸，从而<strong>加速模型收敛、提高训练稳定性、改善模型泛化能力</strong>。</p>
<p>主要归一化方法</p>
<ol>
<li>​<strong>批归一化</strong>：​在一个训练批次内，对同一个神经元​（特征通道）在所有样本上的激活值进行归一化。</li>
<li>​<strong>层归一化</strong>： 对单个样本，在一个层（所有神经元）的激活值上进行归一化（计算均值和方差）。</li>
<li>​<strong>实例归一化</strong>：对单个样本，在其空间维度上（对卷积层而言，是高度H和宽度W），对每个特征通道单独进行归一化。</li>
<li>​<strong>组归一化</strong>​：主要是对<strong>单个样本</strong>，将其特征通道划分为 ​<strong>G 个组</strong>，然后对<strong>每个组内的所有通道上的激活值</strong>​（跨越通道维度）进行归一化。</li>
</ol>
<p>原理</p>
<ol>
<li><strong>缓解内部协变量偏移（Internal Covariate Shift）：</strong> 在训练时，每层输入的分布不断变化，会导致训练不稳定，而归一化强制每层输入保持稳定分布（均值为 0，方差为 1），从而提升模型收敛效率。</li>
<li><strong>改善梯度传播：</strong> 归一化使激活值落入激活函数的线性敏感区间（如 Sigmoid&#x2F;Tanh 的中段），从而避免梯度消失或爆炸。</li>
<li><strong>轻度正则化作用（主要对 BN 而言）：</strong> Batch 内统计带来的噪声有助于防止过拟合，提升泛化能力。</li>
<li><strong>允许更大学习率：</strong> 训练过程更稳定，能使用更大的学习率，进一步提升训练效率。</li>
</ol>
<p>用法</p>
<table>
<thead>
<tr>
<th><strong>场景</strong></th>
<th><strong>推荐归一化方法</strong></th>
<th><strong>原因说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>卷积网络 + 大Batch（≥32）</strong></td>
<td>BatchNorm（BN）</td>
<td>训练快、性能好，是标准选择</td>
</tr>
<tr>
<td><strong>卷积网络 + 小Batch</strong></td>
<td>GroupNorm（GN）</td>
<td>不依赖 batch，适合目标检测、分割等任务</td>
</tr>
<tr>
<td><strong>RNN &#x2F; Transformer</strong></td>
<td>LayerNorm（LN）</td>
<td>不受序列长度与 batch 大小影响，是 NLP&#x2F;时序模型标准配置</td>
</tr>
<tr>
<td><strong>图像生成 &#x2F; 风格迁移任务</strong></td>
<td>InstanceNorm（IN）</td>
<td>去除图像对比度等风格信息，更关注结构与纹理生成</td>
</tr>
</tbody></table>
<hr>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><p>是通过在训练过程中<strong>引入额外的约束或惩罚</strong>，限制模型的复杂性，使其更倾向于学习数据中<strong>泛化的模式（General Pattern）​</strong>，而不是对训练数据中的<strong>噪声或特定细节（Specific Detail&#x2F;Noise）​</strong>进行精确记忆。</p>
<p>核心目标：解决过拟合</p>
<ul>
<li>​<strong>过拟合：​</strong>​ 模型在训练集上表现极好，但在未见过的测试集或真实数据上表现很差。这表明模型记住了训练数据的“个性”，而非掌握了数据的“共性”。</li>
<li>​<strong>欠拟合：​</strong>​ 模型在训练集上都表现不佳，说明模型能力不够或训练不足。正则化主要解决的是过拟合问题。</li>
</ul>
<p>常见正则化技术及其机制</p>
<ol>
<li>​<strong>L1 正则化（Lasso Regression）​</strong>： 鼓励模型学习<strong>稀疏（Sparse）权重</strong>。它倾向于将不重要的特征权重<strong>精确地压缩到0</strong>，实现<strong>特征选择（Feature Selection）​</strong>​ 。被压缩到零的特征对预测基本没有贡献。​</li>
<li>​<strong>L2 正则化（Ridge Regression &#x2F; Weight Decay - 权重衰减）​</strong>：鼓励模型学习<strong>较小、分布分散（Diffuse）的权重</strong>。它惩罚过大的权重值，防止模型过度依赖少数几个特征。使权重趋向于零但不精确为零（除非λ极大）。能有效<strong>缓解多重共线性问题</strong>，提高模型的<strong>稳定性和泛化能力</strong>。是深度学习中<strong>最常用</strong>的正则化方式之一（常直接称为 Weight Decay）。​</li>
<li>​<strong>Dropout</strong>：​在<strong>训练阶段</strong>的<strong>每次前向传播（Forward Pass）​</strong>​ 过程中，以预先设定的概率 <code>p</code>（Dropout Rate）​<strong>随机暂时丢弃（置零）​</strong>​ 网络中的神经元（通常是隐藏层节点）。一方面<strong>打破协同适应性（Co-adaptation）​</strong>，​ 迫使网络不能过度依赖少数几个特定神经元或特征。每个神经元都必须学会在随机的、缺失部分信息的子集（“残破的网络”）中有效工作。另一方面<strong>隐式的模型集成（Ensembling）​</strong>，​不同的 Dropout 模式相当于在训练多个“瘦身”后的子网络。在测试（推理）阶段，所有神经元被保留，但它们的输出值要乘以 <code>(1 - p)</code>（近似平均集成效果）。这大大增强了泛化能力。</li>
<li>​<strong>数据增强（Data Augmentation）​</strong>：在<strong>不改变数据标签（Label）​</strong>​ 的前提下，对原始训练数据进行<strong>合理、多样化的变换或生成新的合成数据</strong>。通过生成大量不同的训练样本变体，​<strong>模拟数据的自然变异</strong>，让模型学习到<strong>更鲁棒、更具不变性的特征</strong>，减少对特定数据点和噪声的敏感性。本质上是在<strong>增加训练集的有效大小和多样性</strong>。</li>
<li>​<strong>早停法（Early Stopping）​</strong>：<strong>监测模型在验证集（Validation Set）上的性能</strong>​（如loss或accuracy）。当验证集性能在一定迭代次数（epoch）或迭代步数（steps）后不再提升，甚至开始下降时，​<strong>停止训练</strong>。</li>
<li>​<strong>模型复杂度约束</strong>： 直接限制了模型拟合复杂函数（包括噪声）的能力，使其更倾向于学习更简单、更泛化的模式。这样直观有效，但在设计时需要根据数据复杂度进行经验性选择（容量太小会导致欠拟合）。通常结合其他正则化技术（如 L2 正则化&#x2F;Dropout）以在保持一定容量的同时控制过拟合。</li>
</ol>
<p>用法<br><strong>深度学习：​</strong>​</p>
<ul>
<li>​<strong>L2 正则化 (Weight Decay)​</strong>​ 一般常用方式</li>
<li>​<strong>Dropout</strong>​ 在 FC 层、CNN 中被广泛应用（常放在 FC 层之后）</li>
<li>​<strong>数据增强</strong>​ 是图像任务的必备项，文本、语音任务应用也越来越多</li>
<li>​<strong>Batch Normalization</strong>​ 被广泛用于稳定训练和加速收敛，还可以顺带正则化。 </li>
<li>​<strong>早停法</strong>​ 使用方便有效</li>
<li>结合起来使用，如 <code>Dropout + L2 + Data Augmentation</code><br><strong>选择依据：​</strong>​</li>
<li>​<strong>数据量：​</strong>​ 小数据时更依赖正则化（数据增强、Dropout、L1&#x2F;L2、强约束）</li>
<li>​<strong>数据噪声：​</strong>​ 噪声大时更需要正则化（Dropout、数据清洗）</li>
<li>​<strong>任务&#x2F;模型：​</strong>​ 不同模型有其偏好（CNN常用Dropout+BN+数据增强；RNN常用L2&#x2F;梯度裁剪）</li>
<li>​<strong>计算资源：​</strong>​ 数据增强、早停法等需额外计算&#x2F;验证</li>
<li>​<strong>问题目标：​</strong>​ 是否需要特征选择（L1）或可解释性（模型约束）</li>
<li>​<strong>超参数调节：​</strong>​ 正则化强度（如 λ, Dropout Rate, 数据增强强度）需要通过验证集仔细调整</li>
</ul>
<hr>
<h2 id="动量（Momentum）"><a href="#动量（Momentum）" class="headerlink" title="动量（Momentum）"></a>动量（Momentum）</h2><p><strong>动量（Momentum）​</strong>​ 是一种优化算法，用于加速梯度下降过程并减少振荡。其核心思想是引入历史梯度信息的加权平均，模拟物理中的“动量”概念，帮助参数更新在正确方向上积累速度，从而更快收敛并逃离局部极小值或鞍点。</p>
<p>核心机制​</p>
<ol>
<li>​<strong>历史梯度的累积</strong>​<br> 每次更新时，不仅考虑当前梯度，还加入之前更新方向的加权值，形成“惯性”：<br> $$v_t &#x3D; \beta v_{t-1} + (1 - \beta) \nabla_\theta J(\theta_t)$$<ul>
<li>$v_t$：当前动量（更新方向）</li>
<li>$\beta$：动量系数（通常取0.9），控制历史信息的权重</li>
<li>$\nabla_\theta J(\theta_t)$：当前梯度</li>
</ul>
</li>
<li>​<strong>参数更新</strong>​<br> 使用动量项替代原始梯度更新参数：<br> $$ \theta_{t+1} &#x3D; \theta_t - \alpha v_t $$<ul>
<li>$\alpha$：学习率</li>
</ul>
</li>
</ol>
<p>解决的问题</p>
<ol>
<li>​<strong>梯度振荡</strong>​<br> 在损失函数的狭窄山谷中，传统SGD会因梯度方向反复变化而振荡。动量通过累积同方向梯度，平滑更新路径，加速穿过平坦区域。<blockquote>
<p>​<strong>示例</strong>​：山谷中，梯度在两侧来回震荡，动量使其沿谷底方向加速。</p>
</blockquote>
</li>
<li>​<strong>局部极小值与鞍点</strong>​<br> 动量提供的“惯性”帮助跳出局部极小值或缓慢穿越鞍点（梯度接近零的区域）。</li>
<li>​<strong>噪声鲁棒性</strong>​<br> 对小批量（mini-batch）的梯度噪声具有平滑作用，提升稳定性。</li>
</ol>
<p>物理类比​<br>想象一个球从山坡滚下：</p>
<ul>
<li>​<strong>传统SGD</strong>​：球每一步只根据当前坡度调整方向，容易卡在局部凹坑</li>
<li>​<strong>动量法</strong>​：球拥有“速度”，下坡时加速，遇到反向坡度时速度逐渐衰减而非立刻转向，更易越过小障碍</li>
</ul>
<p>超参数选择​</p>
<ul>
<li>​<strong>动量系数</strong> $\beta$：<ul>
<li>常用值：​<strong>0.9</strong>​（保留90%历史梯度方向）</li>
<li>$\beta \uparrow$：对历史依赖更强，更新更平滑，但可能过度惯性</li>
<li>$\beta \downarrow$：更依赖当前梯度，振荡可能增加</li>
</ul>
</li>
<li>​<strong>学习率</strong> $\alpha$​​：<br>  通常需略高于标准SGD（因动量加速收敛），但需调优避免发散</li>
</ul>
<p>使用场景</p>
<ul>
<li>​<strong>训练加速</strong>​：在卷积网络（CNN）、循环网络（RNN）中常见，收敛速度显著提升</li>
<li>​<strong>振荡抑制</strong>​：在病态条件（如不同特征尺度差异大）的问题中表现更稳定</li>
<li>​<strong>与自适应方法结合</strong>​：如Adam（融合动量与RMSProp），成为主流优化器之一</li>
</ul>
<hr>
<h2 id="独热编码（One-Hot-Encoding）"><a href="#独热编码（One-Hot-Encoding）" class="headerlink" title="独热编码（One-Hot Encoding）"></a>独热编码（One-Hot Encoding）</h2><p><strong>独热编码（One-Hot Encoding）​</strong>​ 是将分类变量转换为数值形式的标准化技术，使算法能够正确处理非数值特征（如颜色、国家、类别）。其核心思想是为每个类别生成一个<strong>二元向量</strong>​（仅包含0和1），长度等于类别总数，且仅在对应类别位置为1。</p>
<p>核心机制<br><strong>编码过程</strong>​ ：假设一个特征有 <code>k</code> 个类别，创建一个全零向量 <code>[0, 0, ..., 0]</code>，长度为 <code>k</code>。将该特征对应的类别位置设为 <code>1</code>。<br>​<strong>示例</strong>​：颜色特征包含 <code>[红, 绿, 蓝]</code> 三个类别：    </p>
<ul>
<li>红 → <code>[1, 0, 0]</code></li>
<li>绿 → <code>[0, 1, 0]</code></li>
<li>蓝 → <code>[0, 0, 1]</code></li>
</ul>
<p>解决的问题</p>
<ol>
<li>​<strong>分类特征的数值化</strong>​：机器学习模型（如神经网络、SVM）只能处理数值输入，无法直接处理“颜色&#x3D;红”等文本信息。</li>
<li>​<strong>避免数值陷阱</strong>​：若直接赋值（如红&#x3D;1、绿&#x3D;2、蓝&#x3D;3），模型可能错误认为：数值大小反映重要性（如蓝&gt;红），类别间存在线性关系（如（红+蓝）&#x2F;2&#x3D;绿）。独热编码通过<strong>等距表示</strong>消除此类偏差。</li>
</ol>
<p>应用场景​</p>
<ul>
<li>​<strong>结构化数据</strong>​：处理表格中的分类列（如性别、产品类型）</li>
<li>​<strong>自然语言处理（NLP）​</strong>​：将词转换为向量（词袋模型的基础）</li>
<li>​<strong>推荐系统</strong>​：用户或物品的类别特征编码</li>
</ul>
<p>局限性​</p>
<table>
<thead>
<tr>
<th>​<strong>问题</strong>​</th>
<th>​<strong>原因</strong>​</th>
<th>​<strong>解决方案</strong>​</th>
</tr>
</thead>
<tbody><tr>
<td>​<strong>维度爆炸</strong>​</td>
<td>类别数 <code>k</code> 过大时，编码后特征维度急剧增加（如国家有200类 → 200列）</td>
<td>嵌入层（Embedding）、特征哈希（Hashing）</td>
</tr>
<tr>
<td>​<strong>数据稀疏性</strong>​</td>
<td>矩阵中大量0值，存储计算效率低</td>
<td>使用稀疏矩阵格式存储（如<code>scipy.sparse</code>）</td>
</tr>
<tr>
<td>​<strong>类别信息丢失</strong>​</td>
<td>忽略类别间潜在关系（如“动物”与“植物”的距离）</td>
<td>嵌入表示（如Word2Vec）、目标编码</td>
</tr>
<tr>
<td>​<strong>新类别处理</strong>​</td>
<td>测试集出现训练时未见的类别（如训练集无“紫”，测试出现“紫”）</td>
<td>统一预留“未知”类别位或舍弃该特征</td>
</tr>
</tbody></table>
<p>替代方案​</p>
<ol>
<li>​<strong>标签编码（Label Encoding）​</strong>：​直接分配数值标签（如红→0, 绿→1, 蓝→2），​<strong>仅适用于有序类别</strong>​（如学历：小学&lt;中学&lt;大学）。  <strong>风险</strong>​：无序类别引入虚假顺序关系（如“红&lt;绿”无意义）。  </li>
<li>​<strong>嵌入（Embedding）​</strong>​  ：用低维稠密向量表示类别（如将1000个国家映射为16维向量），可学习类别间语义关系。</li>
<li>​<strong>特征哈希（Hashing Trick）​</strong>​：用哈希函数压缩维度（固定输出维度），牺牲少量精度以换取效率。</li>
</ol>
<hr>
<h2 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h2><p><strong>池化（Pooling）​</strong>​ 是卷积神经网络（CNN）中的核心操作，通过对局部区域进行<strong>降采样</strong>​（downsampling），逐步减少特征图的空间尺寸，从而压缩信息并增强特征的鲁棒性。其本质是<strong>保留显著特征，抑制冗余细节</strong>，类似于人眼忽略细节、关注整体轮廓的认知机制。</p>
<p>核心目的​</p>
<ol>
<li>​<strong>降低维度</strong>​：减少特征图尺寸（如将224×224变为112×112），​<strong>显著降低计算量和参数量</strong>。</li>
<li>​<strong>平移不变性（Invariance）​</strong>：​使模型对微小位置变化不敏感（如猫的耳朵在图像中移动几个像素仍能被识别）。</li>
<li>​<strong>特征层次化</strong>​：逐层抽象，浅层保留边缘等细节，深层提取语义特征（如“猫脸”）。</li>
<li>​<strong>防止过拟合</strong>：​压缩信息减少模型容量，抑制噪声干扰。</li>
</ol>
<p>主要类型与操作方式​</p>
<ol>
<li><strong>最大池化（Max Pooling）​</strong> ：保留最显著特征（如纹理、边缘），适用于特征识别任务（如图像分类）<br> ​<strong>操作</strong>​：取局部窗口内的最大值  $\text{Output} &#x3D; \max(\text{Region})$</li>
<li><strong>平均池化（Average Pooling）</strong>  ：平滑区域特征，抑制极端值，适用于需要保留整体信息的任务（如生成模型）<br> <strong>操作</strong>​：取局部窗口内的平均值  $\text{Output} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} x_i$</li>
<li><strong>全局池化（Global Pooling）</strong>：​​代替全连接层，直接输出分类概率，减少参数量（如ResNet、Inception中使用）。<br> <strong>操作</strong>​：对整个特征图进行池化（取全局最大值或平均值）</li>
</ol>
<p>关键参数与计算</p>
<ul>
<li>​<strong>池化窗口（Kernel Size）​</strong>​：一般为2×2或3×3</li>
<li>​<strong>步长（Stride）​</strong>​：通常等于窗口大小（如2×2窗口步长为2）</li>
<li>​<strong>填充（Padding）​</strong>​：通常设为0（<code>valid</code>模式）</li>
</ul>
<p>​输出尺寸公式​：<br>$$<br>\text{Output Size} &#x3D; \left\lfloor \frac{\text{Input Size} - \text{Kernel Size}}{\text{Stride}} + 1 \right\rfloor<br>$$</p>
<p>与卷积层的区别​</p>
<table>
<thead>
<tr>
<th>​<strong>特性</strong>​</th>
<th>​<strong>卷积层</strong>​</th>
<th>​<strong>池化层</strong>​</th>
</tr>
</thead>
<tbody><tr>
<td>​<strong>参数</strong>​</td>
<td>有权重（可学习）</td>
<td>无权重（固定操作）</td>
</tr>
<tr>
<td>​<strong>作用</strong>​</td>
<td>特征提取（主动学习）</td>
<td>特征压缩（被动降维）</td>
</tr>
<tr>
<td>​<strong>通道处理</strong>​</td>
<td>跨通道融合</td>
<td>各通道独立操作</td>
</tr>
<tr>
<td>​<strong>信息保留</strong>​</td>
<td>保留细节</td>
<td>保留最显著特征</td>
</tr>
</tbody></table>
<p>局限性与现代替代方案​</p>
<ol>
<li>​<strong>信息丢失问题</strong>​：池化会丢弃非最大值信息，可能导致细节损失（如小物体检测）。<br> ​<strong>解决方案</strong>​：<ul>
<li>​<strong>步长卷积（Strided Convolution）​</strong>​：用步长&gt;1的卷积代替池化（如VGG16）</li>
<li>​<strong>空洞卷积（Dilated Convolution）​</strong>​：扩大感受野且不降采样</li>
</ul>
</li>
<li>​<strong>位置信息破坏</strong>：​平移不变性可能损害需要位置的任务（如目标检测）。<br> ​<strong>解决方案</strong>​：<ul>
<li>减少池化层数量（如YOLO仅用1次最大池化）</li>
<li>使用可学习池化（如<strong>LPPool</strong>、<strong>SoftPool</strong>）</li>
</ul>
</li>
<li>​<strong>全局信息缺失</strong>​：浅层池化丢失长距离依赖。<br> ​<strong>解决方案</strong>​：​<strong>自注意力机制</strong>​（如Transformer）</li>
</ol>
<p>用法​</p>
<ol>
<li>​<strong>基础架构</strong>​：CNN浅层优先使用<strong>最大池化</strong>​（保留强特征），深层可尝试<strong>全局池化</strong>替代全连接层。</li>
<li>​<strong>尺寸控制</strong>​：避免频繁池化导致特征图过小（如小于4×4时停止池化）。</li>
<li>​<strong>任务适配</strong>​：<ul>
<li>​<strong>分类任务</strong>​：池化提升平移不变性</li>
<li>​<strong>检测&#x2F;分割任务</strong>​：减少池化层或使用步长卷积保留位置信息</li>
</ul>
</li>
</ol>
<hr>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/" title="深度学习的概念们">https://keychankc.github.io/2025/06/05/000-deep-learning-concepts/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%A6%82%E5%BF%B5/" rel="tag"># 概念</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/06/04/018-u2net-saliency-detection/" rel="prev" title="U²-Net显著性目标检测">
                  <i class="fa fa-angle-left"></i> U²-Net显著性目标检测
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/06/10/019-deeplab-series/" rel="next" title="图像分割DeepLab系列算法思路分析">
                  图像分割DeepLab系列算法思路分析 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">180k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/06/05/000-deep-learning-concepts/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
