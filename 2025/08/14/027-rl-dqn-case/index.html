<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. 任务与背景介绍在 Gym&#x2F;Gymnasium 的 MountainCar-v0 环境中，有这样一个场景：一辆小车被困在两个山坡之间，目标是到达右侧山坡顶端的红旗位置。 乍一看，这似乎只需要踩油门往右冲就行，但现实并非如此，小车的发动机动力不足，单次加速无法直接登顶，它会在半途滑落回谷底。正确的策略是先向左加速爬上左坡，然后顺势向右冲下去，再反复摆动、积累动能，最终才能冲上右侧山顶。">
<meta property="og:type" content="article">
<meta property="og:title" content="DQN(Deep Q-Network)系列算法解析与实践">
<meta property="og:url" content="https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. 任务与背景介绍在 Gym&#x2F;Gymnasium 的 MountainCar-v0 环境中，有这样一个场景：一辆小车被困在两个山坡之间，目标是到达右侧山坡顶端的红旗位置。 乍一看，这似乎只需要踩油门往右冲就行，但现实并非如此，小车的发动机动力不足，单次加速无法直接登顶，它会在半途滑落回谷底。正确的策略是先向左加速爬上左坡，然后顺势向右冲下去，再反复摆动、积累动能，最终才能冲上右侧山顶。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/mountaincar_ep3.gif">
<meta property="article:published_time" content="2025-08-14T05:40:12.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.541Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="目标网络">
<meta property="article:tag" content="经验回放">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/mountaincar_ep3.gif">


<link rel="canonical" href="https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/","path":"2025/08/14/027-rl-dqn-case/","title":"DQN(Deep Q-Network)系列算法解析与实践"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DQN(Deep Q-Network)系列算法解析与实践 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-系列"><a href="/series/" rel="section"><i class="fa fa-list-ol fa-fw"></i>系列</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%BB%E5%8A%A1%E4%B8%8E%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="nav-text">1. 任务与背景介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E7%8E%AF%E5%A2%83%E5%8F%82%E6%95%B0"><span class="nav-text">1.1 环境参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E4%BB%BB%E5%8A%A1%E6%80%A7%E8%B4%A8"><span class="nav-text">1.2 任务性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E9%97%AE%E9%A2%98%E9%9A%BE%E7%82%B9"><span class="nav-text">1.3 问题难点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8-DQN"><span class="nav-text">1.4 为什么用 DQN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-DQN-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-text">2. DQN 基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Q-Learning"><span class="nav-text">2.1 Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-Q-Learning%E5%9B%9E%E9%A1%BE"><span class="nav-text">2.1.1 Q-Learning回顾</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-%E5%8D%B3%E6%97%B6%E5%A5%96%E5%8A%B1-vs-%E6%9C%AA%E6%9D%A5%E5%9B%9E%E6%8A%A5"><span class="nav-text">2.1.2 即时奖励 vs. 未来回报</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-DQN"><span class="nav-text">2.2 DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E6%9B%BF%E4%BB%A3%E8%A1%A8%E6%A0%BC%EF%BC%9F"><span class="nav-text">2.2.1 为什么要用深度网络替代表格？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-%E4%B8%BB%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C"><span class="nav-text">2.2.2 主网络 &amp; 目标网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-DQN-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-text">3. DQN 核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E4%B8%BB%E7%BD%91%E7%BB%9C%EF%BC%88Online-Q-Network%EF%BC%89"><span class="nav-text">3.1 主网络（Online Q-Network）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%EF%BC%88Target-Q-Network%EF%BC%89"><span class="nav-text">3.2 目标网络（Target Q-Network）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0%EF%BC%88Replay-Buffer%EF%BC%89"><span class="nav-text">3.3 经验回放池（Replay Buffer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%CE%B5-%E8%B4%AA%E5%A9%AA%E7%AD%96%E7%95%A5"><span class="nav-text">3.4 ε-贪婪策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90"><span class="nav-text">4.训练流程解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-DQN%E7%BD%91%E7%BB%9C%E5%AE%9A%E4%B9%89"><span class="nav-text">4.1 DQN网络定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0"><span class="nav-text">4.2 模型训练实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E6%97%A5%E5%BF%97%E8%BE%93%E5%87%BA"><span class="nav-text">4.3 日志输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C"><span class="nav-text">4.4 验证模型效果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%8E%E5%85%AC%E5%BC%8F%E8%A7%A3%E6%9E%90"><span class="nav-text">5.目标函数与公式解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E7%9B%AE%E6%A0%87%EF%BC%9A%E8%AE%A9%E9%A2%84%E6%B5%8B-Q-%E5%80%BC%E6%9B%B4%E6%8E%A5%E8%BF%91%E2%80%9C%E6%AD%A3%E7%A1%AE%E7%AD%94%E6%A1%88%E2%80%9D"><span class="nav-text">5.1 目标：让预测 Q 值更接近“正确答案”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E7%9B%AE%E6%A0%87-Q-%E5%80%BC%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">5.2 目标 Q 值的计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%EF%BC%88Target-Q-Network%EF%BC%89"><span class="nav-text">5.3 为什么要用目标网络（Target Q-Network）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81-detach-%E7%9B%AE%E6%A0%87%E5%80%BC"><span class="nav-text">5.4 为什么要 detach() 目标值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-%E6%95%B4%E4%B8%AA%E8%BF%87%E7%A8%8B%E7%B1%BB%E6%AF%94"><span class="nav-text">5.5 整个过程类比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Double-DQN%EF%BC%88%E8%A7%A3%E5%86%B3-Q-%E5%80%BC%E8%BF%87%E9%AB%98%E4%BC%B0%E8%AE%A1%EF%BC%89"><span class="nav-text">6.Double DQN（解决 Q 值过高估计）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E9%97%AE%E9%A2%98%E7%82%B9"><span class="nav-text">6.1 问题点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E7%AD%96%E7%95%A5"><span class="nav-text">6.2 策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-text">6.3 完整实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-%E6%88%90%E5%8A%9F%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="nav-text">6.4 成功率对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Dueling-DQN%EF%BC%88%E5%88%86%E7%A6%BB%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E4%B8%8E%E5%8A%A8%E4%BD%9C%E4%BC%98%E5%8A%BF%EF%BC%89"><span class="nav-text">7.Dueling DQN（分离状态价值与动作优势）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%99%E6%A0%B7%E6%8B%86%E5%88%86"><span class="nav-text">7.1 为什么要这样拆分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Dueling-DQN-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-text">7.2 Dueling DQN 的优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.3 完整实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E6%88%90%E5%8A%9F%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="nav-text">7.4 成功率对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Prioritized-Experience-Replay%EF%BC%88%E4%BC%98%E5%85%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%EF%BC%89"><span class="nav-text">8. Prioritized Experience Replay（优先经验回放）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-PER-%E5%92%8C-DQN-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">8.1 PER 和 DQN 的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-PER-%E7%9A%84%E5%8E%9F%E7%90%86"><span class="nav-text">8.2 PER 的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-PER-%E5%9C%A8-MountainCar-v0-%E4%B8%AD%E7%9A%84%E6%84%8F%E4%B9%89"><span class="nav-text">8.3 PER 在 MountainCar-v0 中的意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="nav-text">8.4 完整实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-%E6%88%90%E5%8A%9F%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="nav-text">8.5 成功率对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-%E6%80%BB%E7%BB%93%E4%B8%8E%E5%BB%B6%E4%BC%B8"><span class="nav-text">9.总结与延伸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-%E5%A4%87%E6%B3%A8"><span class="nav-text">10. 备注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/keychankc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DQN(Deep Q-Network)系列算法解析与实践 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DQN(Deep Q-Network)系列算法解析与实践
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-14 13:40:12" itemprop="dateCreated datePublished" datetime="2025-08-14T13:40:12+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/08/14/027-rl-dqn-case/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/08/14/027-rl-dqn-case/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-任务与背景介绍"><a href="#1-任务与背景介绍" class="headerlink" title="1. 任务与背景介绍"></a>1. 任务与背景介绍</h2><p>在 Gym&#x2F;Gymnasium 的 <strong>MountainCar-v0</strong> 环境中，有这样一个场景：一辆小车被困在两个山坡之间，目标是到达右侧山坡顶端的红旗位置。</p>
<p>乍一看，这似乎只需要踩油门往右冲就行，但现实并非如此，小车的发动机动力不足，单次加速无法直接登顶，它会在半途滑落回谷底。正确的策略是先向左加速爬上左坡，然后顺势向右冲下去，再反复摆动、积累动能，最终才能冲上右侧山顶。</p>
<span id="more"></span>
<p><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/mountaincar_ep3.gif"></p>
<h3 id="1-1-环境参数"><a href="#1-1-环境参数" class="headerlink" title="1.1 环境参数"></a>1.1 环境参数</h3><p>在 <strong>MountainCar-v0</strong> 中，环境由以下元素构成：</p>
<ul>
<li><strong>状态（State）</strong>：小车的水平位置与速度（连续值）</li>
<li><strong>动作（Action）</strong>：三个离散动作<ol>
<li>向左加速（0）</li>
<li>向右加速（2）</li>
<li>不加速（no push）</li>
</ol>
</li>
<li><strong>奖励（Reward）</strong>：<ul>
<li>每执行一步都会收到 <strong>-1</strong> 惩罚（鼓励尽快完成任务）</li>
<li>到达山顶时获得额外奖励并结束回合</li>
</ul>
</li>
</ul>
<h3 id="1-2-任务性质"><a href="#1-2-任务性质" class="headerlink" title="1.2 任务性质"></a>1.2 任务性质</h3><p>这是一个<strong>离散动作空间的决策问题</strong>，适合用 DQN 解决，原因如下：</p>
<ol>
<li><strong>动作空间小且离散</strong>（3 个动作），可直接用 Q 值表示各动作价值</li>
<li><strong>状态空间连续</strong>（位置、速度），传统 Q 表难以应用，需要神经网络来近似 Q 值函数</li>
<li><strong>延迟回报明显</strong>：到达山顶的奖励需要经过一系列操作才能获得，算法必须学会权衡眼前损失与未来收益</li>
</ol>
<h3 id="1-3-问题难点"><a href="#1-3-问题难点" class="headerlink" title="1.3 问题难点"></a>1.3 问题难点</h3><ul>
<li><strong>动力不足</strong>：无法直接登顶，必须借助坡道助跑</li>
<li><strong>积累动能</strong>：需要多次反向加速形成足够速度</li>
<li><strong>奖励稀疏</strong>：过程几乎全是负奖励，只有成功登顶才有正反馈</li>
<li><strong>探索必要性</strong>：如果只向右加速，几乎不可能完成任务，必须探索反向助跑策略</li>
</ul>
<h3 id="1-4-为什么用-DQN"><a href="#1-4-为什么用-DQN" class="headerlink" title="1.4 为什么用 DQN"></a>1.4 为什么用 DQN</h3><p>DQN 在此类任务中有天然优势：</p>
<ul>
<li>能学习<strong>长期回报</strong>，避免只追求即时收益</li>
<li><strong>ε-贪婪策略</strong>让智能体有机会探索看似“错误”的操作（如向左加速），发现更优路径</li>
<li>结合<strong>经验回放</strong>与<strong>目标网络</strong>，在连续状态空间中稳定学习高价值动作序列</li>
</ul>
<h2 id="2-DQN-基本原理"><a href="#2-DQN-基本原理" class="headerlink" title="2. DQN 基本原理"></a>2. DQN 基本原理</h2><h3 id="2-1-Q-Learning"><a href="#2-1-Q-Learning" class="headerlink" title="2.1 Q-Learning"></a>2.1 Q-Learning</h3><h4 id="2-1-1-Q-Learning回顾"><a href="#2-1-1-Q-Learning回顾" class="headerlink" title="2.1.1 Q-Learning回顾"></a>2.1.1 Q-Learning回顾</h4><p>在讲 DQN 之前，我们先回顾一下 <strong>Q-Learning</strong>，因为 DQN 其实就是 Q-Learning 在<strong>高维连续状态空间</strong>下的一种扩展。</p>
<p>Q-Learning 是一种 <strong>值迭代算法</strong>，核心思想是学习一个状态–动作价值函数 $Q(s,a)$，这个函数表示在状态 $s$ 下执行动作 $a$ 并按最优策略继续下去所能获得的期望回报。<br>它的更新公式为：<br>$$Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max{a’} Q(s’,a’) - Q(s,a) \big]$$<br>在离散、低维的状态空间里，Q 值可以直接用一个表（Q 表）存储，并在不断交互中更新。</p>
<p>如果省略学习率 α 并直接用目标替换原值，就得到核心形式：<br>$$Q(s, a) &#x3D; r + \gamma max_{a’} Q(s’, a’) $$<br>这里：</p>
<ul>
<li>$r$：当前动作的即时奖励</li>
<li>$\gamma$：折扣因子（0~1 之间），衡量未来奖励的重要性</li>
<li>$\max_{a’} Q(s’, a’)$表示：假设在下一个状态选择最优动作时的未来价值</li>
</ul>
<h4 id="2-1-2-即时奖励-vs-未来回报"><a href="#2-1-2-即时奖励-vs-未来回报" class="headerlink" title="2.1.2 即时奖励 vs. 未来回报"></a>2.1.2 即时奖励 vs. 未来回报</h4><p>在小车登山任务中：</p>
<ul>
<li><strong>即时奖励</strong>：每一步都是 -1（让你尽快到达山顶）</li>
<li><strong>未来回报</strong>：一旦到达山顶，获得额外奖励</li>
</ul>
<p>Q-Learning 的优势在于，它会同时考虑当前奖励和未来回报。<br>比如，如果现在向左加速会让你离山顶更远（即时奖励差），但能积累动能从而之后登顶（未来回报高），Q 值会判断这是值得的。<br>这正是强化学习解决延迟回报问题的关键。</p>
<h3 id="2-2-DQN"><a href="#2-2-DQN" class="headerlink" title="2.2 DQN"></a>2.2 DQN</h3><h4 id="2-2-1-为什么要用深度网络替代表格？"><a href="#2-2-1-为什么要用深度网络替代表格？" class="headerlink" title="2.2.1 为什么要用深度网络替代表格？"></a>2.2.1 为什么要用深度网络替代表格？</h4><p>在经典 Q-Learning 中，Q 值通常存储在一个<strong>状态-动作表</strong>（Q 表）里。但在小车登山任务中，<strong>状态是连续值</strong>（位置、速度），可能有无数个组合。如果用表格存储，每个状态都需要一行，不好存储。<br>于是，<strong>DQN（Deep Q-Network）</strong> 用一个神经网络来逼近 Q 值函数：</p>
<ul>
<li>输入：状态向量（位置、速度）</li>
<li>输出：该状态下每个动作的 Q 值（3 个数）</li>
<li>优势：不需要穷举所有状态，能在相似状态之间<strong>泛化</strong>学习成果</li>
</ul>
<h4 id="2-2-2-主网络-目标网络"><a href="#2-2-2-主网络-目标网络" class="headerlink" title="2.2.2 主网络 &amp; 目标网络"></a>2.2.2 主网络 &amp; 目标网络</h4><p>DQN 有两个几乎一模一样的网络：</p>
<ol>
<li><strong>主网络（Online Q-Network）</strong>：每次更新时使用它来预测当前状态的 Q 值，并进行梯度下降优化。</li>
<li><strong>目标网络（Target Q-Network）</strong>：在计算目标值时使用，参数更新得更慢（例如每隔 N 步从主网络复制一次）。</li>
</ol>
<p><strong>为什么要两个网络？</strong><br>如果目标值直接由主网络实时计算，会导致更新过程不稳定，因为目标和预测值来自同一个会不断变化的网络。引入目标网络可以让目标在一段时间内保持相对稳定，从而减少训练振荡。</p>
<p><strong>类比解释：“现在的自己” vs. “过去的自己”</strong><br>可以把两个网络类比成：</p>
<ul>
<li><strong>主网络</strong> &#x3D; 现在的自己（每天学习新知识，变化很快）</li>
<li><strong>目标网络</strong> &#x3D; 过去的自己（定期拍个快照，参考当时的想法）</li>
</ul>
<p>学习的时候：</p>
<ul>
<li>主网络负责提出<strong>当前的理解</strong>（预测 Q 值）</li>
<li>目标网络提供一个<strong>相对稳定的参考答案</strong>（计算目标值）</li>
<li>过一段时间，过去的自己会更新为现在的自己（同步参数）</li>
</ul>
<p>这种设计，让 DQN 既能快速学习，又不至于因为目标值不停抖动而陷入不稳定。</p>
<h2 id="3-DQN-核心组件"><a href="#3-DQN-核心组件" class="headerlink" title="3. DQN 核心组件"></a>3. DQN 核心组件</h2><h3 id="3-1-主网络（Online-Q-Network）"><a href="#3-1-主网络（Online-Q-Network）" class="headerlink" title="3.1 主网络（Online Q-Network）"></a>3.1 主网络（Online Q-Network）</h3><p><strong>主网络（Online Q-Network）</strong> 的作用是接收当前状态 $s$（位置、速度），输出该状态下每个可能动作的 Q 值。在训练中，主网络是实时更新的，利用梯度下降不断调整参数，让预测的 Q 值更接近目标 Q 值。</p>
<p>在小车登山这个例子中，当小车处于“靠近左坡顶、速度向右”的状态时，主网络会输出三个数：</p>
<ul>
<li>向左加速 Q 值</li>
<li>向右加速 Q 值</li>
<li>不动 Q 值</li>
</ul>
<p>算法会选择其中 Q 值最大的动作（除非在探索阶段）。</p>
<h3 id="3-2-目标网络（Target-Q-Network）"><a href="#3-2-目标网络（Target-Q-Network）" class="headerlink" title="3.2 目标网络（Target Q-Network）"></a>3.2 目标网络（Target Q-Network）</h3><p>目标网络主要是计算目标 Q 值时使用，目的是保持一段时间内不变，减少训练振荡。更新方式其实不是每次训练都更新，而是每隔固定步数，将主网络的参数复制给目标网络。</p>
<p><strong>小车登山类比</strong>：</p>
<ul>
<li>主网络 &#x3D; “现在的自己”，每次都在调整思路</li>
<li>目标网络 &#x3D; “过去的自己”，一段时间才更新一次观点</li>
</ul>
<p>这样，主网络在学习时总是参考一个相对稳定的“过去自己”，不至于因为目标值不断变化而陷入混乱。</p>
<h3 id="3-3-经验回放池（Replay-Buffer）"><a href="#3-3-经验回放池（Replay-Buffer）" class="headerlink" title="3.3 经验回放池（Replay Buffer）"></a>3.3 经验回放池（Replay Buffer）</h3><p>经验回放池的主要作用是存储过去的状态、动作、奖励、下一状态等（即 transition），并在训练时随机抽取一批样本进行学习。<br> 这样做的好处有：</p>
<ol>
<li><strong>打乱数据相关性</strong>：环境中的数据是连续的（比如小车一直在左坡上下摆动），直接用会导致模型过拟合特定轨迹</li>
<li><strong>提升样本利用率</strong>：同一条经验可以多次用于训练，而不是用一次就丢掉</li>
</ol>
<p><strong>还是以小车登山举例</strong>：<br>如果没有经验回放，小车可能连续 100 步都在左坡附近，这些相似数据会让模型短时间内“只记得左坡的事”。而有了 Replay Buffer，我们可以把过去不同位置的经验混合，让网络更全面地学习。</p>
<h3 id="3-4-ε-贪婪策略"><a href="#3-4-ε-贪婪策略" class="headerlink" title="3.4 ε-贪婪策略"></a>3.4 ε-贪婪策略</h3><p>ε-贪婪策略的主要作用是在训练过程中平衡 探索（<strong>Exploration</strong>）与利用（<strong>Exploitation</strong>）。<br><strong>规则</strong>：    </p>
<ul>
<li>以概率 $\varepsilon$ 随机选择一个动作（探索）        </li>
<li>以概率 $1 - \varepsilon$ 选择当前 Q 值最高的动作（利用）</li>
</ul>
<p><strong>动态调整</strong>：通常从较大的 $\varepsilon$ 开始（更多探索），然后逐渐减小（更多利用）。</p>
<p><strong>还是以小车登山例子</strong>：<br>刚开始，小车可能会随机向左冲、向右冲甚至不动——这有助于发现“先向左加速再向右冲顶”的策略。随着训练进行，ε 会逐渐降低，小车会更多地按照学到的最优策略去冲山顶。</p>
<p>这四个组件在 DQN 中缺一不可：</p>
<ol>
<li><strong>主网络</strong>：负责当前 Q 值预测    </li>
<li><strong>目标网络</strong>：提供稳定的学习目标</li>
<li><strong>经验回放池</strong>：打乱相关性、提升数据利用率</li>
<li><strong>ε-贪婪策略</strong>：保证探索与利用的平衡</li>
</ol>
<p>结合起来，DQN 能够在像小车登山这种<strong>连续状态 + 离散动作 + 延迟回报</strong>的任务中稳定、高效地学出策略。</p>
<h2 id="4-训练流程解析"><a href="#4-训练流程解析" class="headerlink" title="4.训练流程解析"></a>4.训练流程解析</h2><h3 id="4-1-DQN网络定义"><a href="#4-1-DQN网络定义" class="headerlink" title="4.1 DQN网络定义"></a>4.1 DQN网络定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DQN</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_dim, action_dim</span>):  </span><br><span class="line">        <span class="built_in">super</span>(DQN, <span class="variable language_">self</span>).__init__()  </span><br><span class="line">        <span class="comment"># 两层隐层 MLP，将连续状态映射到每个离散动作的 Q 值  </span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(state_dim, <span class="number">128</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">128</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">128</span>, action_dim)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))  </span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc2(x))  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc3(x) <span class="comment"># 输出形状：[batch, action_dim]，即 Q(s, ·)  </span></span><br></pre></td></tr></table></figure>

<h3 id="4-2-模型训练实现"><a href="#4-2-模型训练实现" class="headerlink" title="4.2 模型训练实现"></a>4.2 模型训练实现</h3><p>DQN 的大致训练流程：</p>
<ol>
<li>初始化网络、经验池、优化器</li>
<li>重置环境，得到初始状态</li>
<li>按 ε-贪婪策略选择动作</li>
<li>执行动作，获取下一状态、奖励、结束标记</li>
<li>存储 (s, a, r, s′) 到经验池</li>
<li>如果经验池样本数达阈值：采样 batch，计算目标，反向传播</li>
<li>每隔固定步数同步目标网络参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():  </span><br><span class="line">    <span class="comment"># ===== (1) 初始化网络、经验池、优化器 =====</span></span><br><span class="line">    env = gym.make(<span class="string">&quot;MountainCar-v0&quot;</span>)  </span><br><span class="line">    <span class="comment"># state: 2维 [position, velocity]</span></span><br><span class="line">    state_dim = env.observation_space.shape[<span class="number">0</span>]  </span><br><span class="line">    <span class="comment"># action: 3个离散动作：左推、无操作、右推</span></span><br><span class="line">    action_dim = env.action_space.n  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 超参数</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># 折扣因子，衡量未来奖励在当前决策中的重要程度</span></span><br><span class="line">    epsilon = <span class="number">1.0</span>  <span class="comment"># ε-贪婪策略初始探索率，ε1.0意味着第一回合几乎全随机选动作</span></span><br><span class="line">    epsilon_min = <span class="number">0.01</span>  <span class="comment"># 最小探索率，保留一小部分随机动作（1% 概率），防止策略僵化</span></span><br><span class="line">    epsilon_decay = <span class="number">0.995</span>  <span class="comment"># 每回合结束后对 ε 做指数衰减（更快收敛）</span></span><br><span class="line">    lr = <span class="number">1e-3</span>  <span class="comment"># 学习率，控制每次参数更新的步幅大小</span></span><br><span class="line">    batch_size = <span class="number">64</span>  <span class="comment"># 批量大小，每次从经验池里采多少条样本来更新网络</span></span><br><span class="line">    target_update_freq = <span class="number">500</span>  <span class="comment"># 目标网络更新频率，每隔多少环境步同步一次目标网络</span></span><br><span class="line">    max_episodes = <span class="number">600</span>  <span class="comment"># 最大训练回合数</span></span><br><span class="line">    memory_size = <span class="number">50000</span>  <span class="comment"># 经验回放池容量</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 构建主网络 &amp; 目标网络（结构相同）  </span></span><br><span class="line">    online_net = DQN(state_dim, action_dim)  </span><br><span class="line">    target_net = DQN(state_dim, action_dim)  </span><br><span class="line">    target_net.load_state_dict(online_net.state_dict())  <span class="comment"># 初始先对齐</span></span><br><span class="line">    optimizer = optim.Adam(online_net.parameters(), lr=lr)  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 经验回放池：存储 (s, a, r, s′, done)</span></span><br><span class="line">    memory = deque(maxlen=memory_size)  </span><br><span class="line">  </span><br><span class="line">    total_steps = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(max_episodes):  </span><br><span class="line">        <span class="comment"># ===== (2) 重置环境，得到初始状态 =====</span></span><br><span class="line">        state, _ = env.reset()  </span><br><span class="line">        total_reward = <span class="number">0</span>  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一个回合最多 200 步（MountainCar的默认上限）</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):  </span><br><span class="line">            total_steps += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ===== (3) 按ε-贪婪策略选择动作 =====</span></span><br><span class="line">            <span class="comment"># 以ε的概率做随机动作（探索），否则选Q值最大的动作</span></span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; epsilon:  </span><br><span class="line">                <span class="comment"># 探索：从动作空间随机采一个动作，0（左加速）、1（不动）、2（右加速）</span></span><br><span class="line">                action = env.action_space.sample()  </span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">                <span class="comment"># 利用</span></span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">                    <span class="comment"># [position, velocity] -&gt; tensor</span></span><br><span class="line">                    state_tensor = torch.FloatTensor(state).unsqueeze(<span class="number">0</span>) </span><br><span class="line">                    <span class="comment"># 把当前状态输入DQN主网络，得到每个可能动作的Q值 -&gt; [1, action_dim]</span></span><br><span class="line">                    q_values = online_net(state_tensor)  </span><br><span class="line">                    <span class="comment"># 找出Q值最大的动作的索引(最优动作)</span></span><br><span class="line">                    action = q_values.argmax().item()  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ===== (4) 执行动作，获取下一状态、奖励、结束标记 =====</span></span><br><span class="line">            next_state, reward, terminated, truncated, _ = env.step(action)  </span><br><span class="line">            done = terminated <span class="keyword">or</span> truncated  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ---- 奖励塑形（可选）----</span></span><br><span class="line">            <span class="comment"># 这里额外加了 abs(position - (-0.5))，鼓励远离谷底（-0.5）向两侧移动，</span></span><br><span class="line">            <span class="comment"># 使学到“先左后右”的策略更容易出现（尤其在稀疏奖励场景）。 </span></span><br><span class="line">            position, velocity = next_state  </span><br><span class="line">            reward += <span class="built_in">abs</span>(position - (-<span class="number">0.5</span>))  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ===== (5) 存储 (s, a, r, s′, done) 到经验池 =====</span></span><br><span class="line">            memory.append((state, action, reward, next_state, done))  </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 状态推进到下一步</span></span><br><span class="line">            state = next_state  </span><br><span class="line">            total_reward += reward  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ===== (6) 如果经验池样本数达阈值：开始学习 =====</span></span><br><span class="line">            <span class="comment"># 这里用“&gt; 1000”作为启动学习的阈值，避免一开始样本过少导致过拟合/发散</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(memory) &gt; <span class="number">1000</span>:  </span><br><span class="line">                batch = random.sample(memory, batch_size)  </span><br><span class="line">                states, actions, rewards, next_states, dones = <span class="built_in">zip</span>(*batch)  </span><br><span class="line"></span><br><span class="line">                <span class="comment"># 转张量</span></span><br><span class="line">                states = torch.FloatTensor(states) <span class="comment"># [B, state_dim]</span></span><br><span class="line">                actions = torch.LongTensor(actions).unsqueeze(<span class="number">1</span>) <span class="comment"># [B, 1]</span></span><br><span class="line">                rewards = torch.FloatTensor(rewards).unsqueeze(<span class="number">1</span>) <span class="comment"># [B, 1]</span></span><br><span class="line">                next_states = torch.FloatTensor(next_states) <span class="comment"># [B, state_dim]</span></span><br><span class="line">                dones = torch.FloatTensor(dones).unsqueeze(<span class="number">1</span>) <span class="comment"># [B, 1] (1.0/0.0)</span></span><br><span class="line">  </span><br><span class="line">                <span class="comment"># (6-2) 主网络预测当前 Q 值：Q(s,a)</span></span><br><span class="line">                <span class="comment"># online_net(states) -&gt; [B, A]，gather挑出实际执行的动作a的Q值</span></span><br><span class="line">                q_values = online_net(states).gather(<span class="number">1</span>, actions)  <span class="comment"># [B, 1]</span></span><br><span class="line">  </span><br><span class="line">                <span class="comment"># (6-3) 目标网络计算下一个状态 s′ 的最大Q值：max_a&#x27; Q_target(s′, a′)</span></span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">                    next_q_values = target_net(next_states).<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>].unsqueeze(<span class="number">1</span>)  <span class="comment"># [B, 1]</span></span><br><span class="line">                    <span class="comment"># 对终止状态不进行 bootstrap：*(1 - done)</span></span><br><span class="line">                    <span class="comment"># (6-4) 构造目标：y = r + γ * max Q_target(s′, a′)</span></span><br><span class="line">                    target_q = rewards + gamma * next_q_values * (<span class="number">1</span> - dones)  </span><br><span class="line">  </span><br><span class="line">                <span class="comment"># (6-5) 计算损失并反向传播（这里用 MSELoss）</span></span><br><span class="line">                loss = nn.MSELoss()(q_values, target_q)  </span><br><span class="line">                optimizer.zero_grad()  </span><br><span class="line">                loss.backward()  </span><br><span class="line">                optimizer.step()  </span><br><span class="line">  </span><br><span class="line">            <span class="comment"># ===== (7) 每隔固定步数同步一次目标网络参数 =====</span></span><br><span class="line">            <span class="keyword">if</span> total_steps % target_update_freq == <span class="number">0</span>:  </span><br><span class="line">                <span class="comment"># 将主网络的最新参数拷贝给目标网络，稳定目标值的估计</span></span><br><span class="line">                target_net.load_state_dict(online_net.state_dict())  </span><br><span class="line">  </span><br><span class="line">            <span class="keyword">if</span> done:  </span><br><span class="line">                <span class="keyword">break</span>  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># ε 衰减：每回合结束后降低探索比例，最终不低于 epsilon_min</span></span><br><span class="line">        epsilon = <span class="built_in">max</span>(epsilon_min, epsilon * epsilon_decay)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 训练日志：Steps 是本回合步数；Reward 是“原始奖励 + 塑形奖励”的总和</span></span><br><span class="line">        <span class="built_in">print</span>(  </span><br><span class="line">            <span class="string">f&quot;Episode <span class="subst">&#123;episode + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;max_episodes&#125;</span> | Steps: <span class="subst">&#123;t + <span class="number">1</span>&#125;</span> | Reward: <span class="subst">&#123;total_reward:<span class="number">.2</span>f&#125;</span> | Epsilon: <span class="subst">&#123;epsilon:<span class="number">.3</span>f&#125;</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 训练完成，保存主网络（即推理时使用的在线网络）参数</span></span><br><span class="line">    torch.save(online_net.state_dict(), <span class="string">&quot;dqn_mountaincar_fast.pth&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存到 dqn_mountaincar_fast.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-3-日志输出"><a href="#4-3-日志输出" class="headerlink" title="4.3 日志输出"></a>4.3 日志输出</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Episode 1/600 | Steps: 200 | Reward: -191.07 | Epsilon: 0.995</span><br><span class="line">Episode 2/600 | Steps: 200 | Reward: -181.50 | Epsilon: 0.990</span><br><span class="line">Episode 3/600 | Steps: 200 | Reward: -192.44 | Epsilon: 0.985</span><br><span class="line">...</span><br><span class="line">Episode 178/600 | Steps: 200 | Reward: -146.89 | Epsilon: 0.410</span><br><span class="line">Episode 179/600 | Steps: 163 | Reward: -112.88 | Epsilon: 0.408</span><br><span class="line">Episode 180/600 | Steps: 200 | Reward: -127.69 | Epsilon: 0.406</span><br><span class="line">...</span><br><span class="line">Episode 597/600 | Steps: 97 | Reward: -57.98 | Epsilon: 0.050</span><br><span class="line">Episode 598/600 | Steps: 134 | Reward: -93.57 | Epsilon: 0.050</span><br><span class="line">Episode 599/600 | Steps: 88 | Reward: -56.04 | Epsilon: 0.050</span><br><span class="line">Episode 600/600 | Steps: 112 | Reward: -74.69 | Epsilon: 0.049</span><br><span class="line">模型已保存到 dqn_mountaincar_fast.pth</span><br></pre></td></tr></table></figure>

<p>日志说明：</p>
<ul>
<li><strong>Episode X&#x2F;Y</strong>：第 X 回合，总共 Y 回合训练</li>
<li><strong>Steps: S</strong>：本回合用的步数（最多 200 步，MountainCar-v0 是默认 200 步终止）</li>
<li><strong>Reward: R</strong>：本回合的总奖励（这里是<strong>原始奖励 + 奖励塑形</strong>后的结果），MountainCar 原始奖励：每步 -1，到达终点额外奖励 0，所以当没有奖励塑形时，每步都会让总奖励更负。因为代码中加了奖励塑形，所以奖励值不是纯整数，而是会出现 -191.07、-57.98 这种小数。</li>
<li><strong>Epsilon: E</strong>：当前 ε-贪婪策略的探索率，随着回合增加而衰减，代表模型更倾向于利用而不是探索。</li>
</ul>
<h3 id="4-4-验证模型效果"><a href="#4-4-验证模型效果" class="headerlink" title="4.4 验证模型效果"></a>4.4 验证模型效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>():</span><br><span class="line">    <span class="comment"># === 1. 加载训练好的 DQN 模型 ===</span></span><br><span class="line">    <span class="comment"># MountainCar 状态 2 维(位置、速度)，动作 3 种(左加速、右加速、不加速)</span></span><br><span class="line">    model = DQN(state_dim=<span class="number">2</span>, action_dim=<span class="number">3</span>)  </span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;dqn_mountaincar_fast.pth&quot;</span>))</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 切换到评估模式（关闭 Dropout / BN 等训练特性）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 2. 创建环境（渲染模式） ===</span></span><br><span class="line">    env = gym.make(<span class="string">&quot;MountainCar-v0&quot;</span>, render_mode=<span class="string">&quot;human&quot;</span>)</span><br><span class="line"></span><br><span class="line">    num_episodes = <span class="number">10</span>      <span class="comment"># 测试回合数</span></span><br><span class="line">    success_count = <span class="number">0</span>      <span class="comment"># 成功登顶的次数计数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 3. 循环执行多个评估回合 ===</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        state, _ = env.reset()  <span class="comment"># 重置环境，获取初始状态</span></span><br><span class="line">        done = <span class="literal">False</span>            <span class="comment"># 回合是否结束</span></span><br><span class="line">        step = <span class="number">0</span>                <span class="comment"># 当前回合步数计数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># === 4. 单个回合循环 ===</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            env.render()        <span class="comment"># 渲染画面（显示小车运动）</span></span><br><span class="line">            time.sleep(<span class="number">0.02</span>)    <span class="comment"># 控制动画播放速度（不然会很快看不清）</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将状态转换为张量，并加 batch 维度 [1, state_dim]</span></span><br><span class="line">            state_tensor = torch.FloatTensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 用训练好的模型计算 Q 值，并选择 Q 值最大的动作</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不计算梯度（节省内存和加速）</span></span><br><span class="line">                q_values = model(state_tensor)</span><br><span class="line">            action = q_values.argmax().item()  <span class="comment"># 取最大 Q 值对应的动作编号</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在环境中执行这个动作</span></span><br><span class="line">            state, reward, terminated, truncated, _ = env.step(action)</span><br><span class="line">            done = terminated <span class="keyword">or</span> truncated  <span class="comment"># 终止或超时都算回合结束</span></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># === 5. 回合结束后，判断是否成功到达右侧山顶 ===</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">0</span>] &gt;= <span class="number">0.5</span>:  <span class="comment"># MountainCar 目标位置是 0.5</span></span><br><span class="line">            success_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印该回合的执行结果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Episode <span class="subst">&#123;episode + <span class="number">1</span>&#125;</span> | Steps: <span class="subst">&#123;step&#125;</span> | Final Pos: <span class="subst">&#123;state[<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 6. 计算总的成功率 ===</span></span><br><span class="line">    success_rate = success_count / num_episodes * <span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;成功率: <span class="subst">&#123;success_rate:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.close()  <span class="comment"># 关闭环境窗口</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Episode 1 | Steps: 173 | Final Pos: 0.54</span><br><span class="line">Episode 2 | Steps: 152 | Final Pos: 0.54</span><br><span class="line">Episode 3 | Steps: 154 | Final Pos: 0.54</span><br><span class="line">Episode 4 | Steps: 87 | Final Pos: 0.51</span><br><span class="line">Episode 5 | Steps: 86 | Final Pos: 0.52</span><br><span class="line">Episode 6 | Steps: 86 | Final Pos: 0.51</span><br><span class="line">Episode 7 | Steps: 161 | Final Pos: 0.54</span><br><span class="line">Episode 8 | Steps: 179 | Final Pos: 0.54</span><br><span class="line">Episode 9 | Steps: 167 | Final Pos: 0.54</span><br><span class="line">Episode 10 | Steps: 156 | Final Pos: 0.54</span><br><span class="line">成功率: 100.00%</span><br></pre></td></tr></table></figure>
<p>日志说明：<br><strong>1. 成功率100%</strong> 表示模型已经学会稳定地冲上右侧山顶（终点位置 ≥ 0.5），每回合都完成任务，训练效果非常好，没有偶发失败的情况。</p>
<p><strong>2. 步数差异</strong></p>
<ul>
<li>步数范围 <strong>86 ~ 179</strong> 步 → 虽然都能成功，但到达速度不完全一致</li>
<li>较短的回合（86~87 步）说明模型走的是比较高效的冲顶路线</li>
<li>较长的回合（170+ 步）可能是因为初期动作探索了更多，不是最短路径，但仍能完成任务</li>
</ul>
<p><strong>3. 最终位置</strong></p>
<ul>
<li><strong>Final Pos 0.51~0.54</strong> → 每次登顶都刚好过终点线，MountainCar 到达 0.5 就算成功，所以模型倾向于“够用就好”，到达目标就停止</li>
<li>没有出现明显的“冲太远”或“差一点没到”的情况，策略很稳定</li>
</ul>
<p>模型已经完全学会 MountainCar 任务的关键策略（先反向加速积累动能，再加速冲顶），并且稳定性极高，几乎没有失败风险。不同回合的步数差异主要来自动作选择的细微差异，而非策略不稳定。</p>
<h2 id="5-目标函数与公式解析"><a href="#5-目标函数与公式解析" class="headerlink" title="5.目标函数与公式解析"></a>5.目标函数与公式解析</h2><h3 id="5-1-目标：让预测-Q-值更接近“正确答案”"><a href="#5-1-目标：让预测-Q-值更接近“正确答案”" class="headerlink" title="5.1 目标：让预测 Q 值更接近“正确答案”"></a>5.1 目标：让预测 Q 值更接近“正确答案”</h3><p>在 DQN 中，神经网络的任务是去拟合 $Q(s,a)$，也就是，当前状态 s 下执行动作 a，未来能获得多少回报。而我们希望 <strong>预测的 Q 值</strong> 和 <strong>目标 Q 值</strong> 越接近越好。<br>因此，用<strong>均方误差（MSE）</strong> 来衡量两者差距：<br>$$L(\theta) &#x3D; \big( Q(s,a;\theta) - y \big)^2$$<br>其中：</p>
<ul>
<li>$Q(s,a; \theta)$：当前 <strong>主网络</strong>（Online Q-Network）的预测值，即对未来回报的估计<ul>
<li>$s$：当前状态（比如小车的位置和速度）</li>
<li>$a$：当前动作（向左、向右、不动）</li>
<li>$\theta$：神经网络的参数（权重和偏置等）</li>
</ul>
</li>
<li>$y$：目标值（Target Q）</li>
</ul>
<h3 id="5-2-目标-Q-值的计算"><a href="#5-2-目标-Q-值的计算" class="headerlink" title="5.2 目标 Q 值的计算"></a>5.2 目标 Q 值的计算</h3><p>目标值 y 来源于 <strong>贝尔曼方程（Bellman Equation）</strong>：<br>$$y &#x3D; r + \gamma \cdot max_{a’} Q_{\text{target}}(s’, a’)$$<br>含义：</p>
<ol>
<li>$r$ ：当前动作获得的<strong>即时奖励</strong></li>
<li>$\gamma$ ：折扣因子（0~1），控制<strong>未来奖励的重要性</strong></li>
<li>$\max_{a’} Q_{\text{target}}(s’, a’)$：在下一个状态 s’ 中，选择最优动作 a’ 能获得的最大价值（由<strong>目标网络</strong>计算）<br>换句话说：目标 Q &#x3D; 即时奖励 + 未来可能的最大奖励（折扣后）</li>
</ol>
<h3 id="5-3-为什么要用目标网络（Target-Q-Network）"><a href="#5-3-为什么要用目标网络（Target-Q-Network）" class="headerlink" title="5.3 为什么要用目标网络（Target Q-Network）"></a>5.3 为什么要用目标网络（Target Q-Network）</h3><p>如果目标值也用当前网络计算，就会出现，当前网络的参数在更新，目标值也跟着变，这会导致训练过程不稳定（Q 值像踩着自己影子）。</p>
<p>解决办法是引入一个 <strong>延迟更新</strong> 的目标网络 $Q_{\text{target}}$，每隔 <strong>N 步</strong>，把主网络参数在复制到目标网络中。</p>
<h3 id="5-4-为什么要-detach-目标值"><a href="#5-4-为什么要-detach-目标值" class="headerlink" title="5.4 为什么要 detach() 目标值"></a>5.4 为什么要 detach() 目标值</h3><p>在 PyTorch 里，如果不加 .detach()，$y$ 会被认为是计算图的一部分，误差回传时，梯度会<strong>一路回传到目标网络</strong>，这样目标网络也会被更新（这不是我们想要的）。</p>
<p>而我们希望<strong>只更新主网络参数</strong> $\theta$，目标网络只是一个<strong>固定参考</strong>，不会被梯度影响。</p>
<p>所以在代码中是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 或 .detach()</span></span><br><span class="line">    next_q_values = target_net(next_states).<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    target_q = rewards + gamma * next_q_values * (<span class="number">1</span> - dones)</span><br></pre></td></tr></table></figure>
<p>这样，target_q 不会产生梯度。</p>
<h3 id="5-5-整个过程类比"><a href="#5-5-整个过程类比" class="headerlink" title="5.5 整个过程类比"></a>5.5 整个过程类比</h3><p>可以把主网络和目标网络想象成：</p>
<ul>
<li><strong>主网络（现在的自己）</strong>：正在学习，更新很频繁</li>
<li><strong>目标网络（过去的自己）</strong>：阶段性保存下来的“笔记”，在一段时间内不变，作为稳定的参考</li>
<li>训练目标：现在的自己尽量学得跟过去的“稳定版本”一致，但未来会更新过去的版本</li>
</ul>
<h2 id="6-Double-DQN（解决-Q-值过高估计）"><a href="#6-Double-DQN（解决-Q-值过高估计）" class="headerlink" title="6.Double DQN（解决 Q 值过高估计）"></a>6.Double DQN（解决 Q 值过高估计）</h2><h3 id="6-1-问题点"><a href="#6-1-问题点" class="headerlink" title="6.1 问题点"></a>6.1 问题点</h3><p><strong>DQN 目标值计算存在高估风险</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	<span class="comment"># target_net(next_states) 是用 目标网络 预测所有可能动作的Q值</span></span><br><span class="line">	<span class="comment"># .max(1)[0] 直接选最大值，这个操作既选择动作又用这个最大值作为评估值</span></span><br><span class="line">	<span class="comment"># 如果预测里有噪声（预测值有偏差），max会偏向取偏高的那个，从而产生乐观偏差</span></span><br><span class="line">    next_q_values = target_net(next_states).<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    target_q = rewards + gamma * next_q_values * (<span class="number">1</span> - dones)</span><br></pre></td></tr></table></figure>
<p>假设目标网络预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">动作 <span class="number">0</span>: <span class="number">5.0</span>  (真实应是 <span class="number">4.8</span>)</span><br><span class="line">动作 <span class="number">1</span>: <span class="number">4.6</span>  (真实应是 <span class="number">4.6</span>)</span><br><span class="line">动作 <span class="number">2</span>: <span class="number">4.9</span>  (真实应是 <span class="number">4.9</span>)</span><br></pre></td></tr></table></figure>
<p>因为预测的噪声，动作 0 被预测得高了一点（5.0），max 会选它，并直接把 <strong>5.0 当作真实价值</strong>。<br>一次两次还好，但每轮更新都会<strong>不断把最大值往高的方向推</strong>这就会导致<strong>Q 值虚高</strong>。</p>
<h3 id="6-2-策略"><a href="#6-2-策略" class="headerlink" title="6.2 策略"></a>6.2 策略</h3><p>为了避免这个问题，Double DQN 分两步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 用主网络选择下一个状态的最优动作</span></span><br><span class="line">next_actions = online_net(next_states).argmax(<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 2. 用目标网络评估这个动作的 Q 值</span></span><br><span class="line">next_q_values = target_net(next_states).gather(<span class="number">1</span>, next_actions)</span><br><span class="line">target_q = rewards + gamma * next_q_values * (<span class="number">1</span> - dones)</span><br></pre></td></tr></table></figure>
<p>这样<strong>动作选择</strong>（argmax）用的是主网络，而<strong>动作评估</strong>（gather）用的是目标网络。两个网络参数不同，噪声就不会同步放大，从而可以降低过高估计，提高稳定性，在MountainCar-v0 case中， Double DQN可以让 Q 值估计更平稳，不会盲目认为某个方向的冲刺能立刻成功，从而减少无效尝试。</p>
<h3 id="6-3-完整实现"><a href="#6-3-完整实现" class="headerlink" title="6.3 完整实现"></a>6.3 完整实现</h3><p><a target="_blank" rel="noopener" href="https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_double_DQN.py">https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_double_DQN.py</a></p>
<h3 id="6-4-成功率对比"><a href="#6-4-成功率对比" class="headerlink" title="6.4 成功率对比"></a>6.4 成功率对比</h3><p>DQN vs Double DQN 实现的成功率对比：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">===== 成功率对比（100 回合） =====</span><br><span class="line">DQN         | 成功率: 99.00%  | 平均步数: 125.60 | 平均终点位置: 0.52</span><br><span class="line">Double DQN  | 成功率: 100.00% | 平均步数: 115.60 | 平均终点位置: 0.51</span><br></pre></td></tr></table></figure>
<h2 id="7-Dueling-DQN（分离状态价值与动作优势）"><a href="#7-Dueling-DQN（分离状态价值与动作优势）" class="headerlink" title="7.Dueling DQN（分离状态价值与动作优势）"></a>7.Dueling DQN（分离状态价值与动作优势）</h2><p><strong>DQN</strong>是直接用一个神经网络输入状态 $s$，输出每个动作 $a$ 对应的 Q 值，所有 Q 值都是一层网络直接回归出来的，没有显式区分<strong>状态价值</strong>和<strong>动作优势</strong>。</p>
<p><strong>Dueling DQN</strong>则是改了最后几层的结构，把 Q 值拆分成：    </p>
<ol>
<li><strong>状态价值</strong> $V(s)$ ：当前状态本身的好坏，不依赖于具体动作        </li>
<li><strong>动作优势</strong> $A(s,a)$ ：在当前状态下，不同动作之间的相对优劣</li>
</ol>
<p>最后再合成：$Q(s,a) &#x3D; V(s) + A(s,a) - \frac{1}{|\mathcal{A}|} \sum_{a’} A(s,a’)$，减去平均优势是为了<strong>去中心化</strong>，避免 $V(s)$ 和 $A(s,a)$ 的数值相互冲突。<br>所以：</p>
<ul>
<li><strong>DQN</strong> “一个脑子同时管状态和动作的价值”</li>
<li><strong>Dueling DQN</strong> “一个脑子负责状态价值，另一个脑子负责动作差异”</li>
</ul>
<h3 id="7-1-为什么要这样拆分"><a href="#7-1-为什么要这样拆分" class="headerlink" title="7.1 为什么要这样拆分"></a>7.1 为什么要这样拆分</h3><p>在很多状态下，<strong>动作选择不敏感</strong>。<br>举个 MountainCar 例子：</p>
<ul>
<li>当小车已经在山顶或者刚出发速度很慢时，左右加速的效果差别不大，这时动作优势 $A(s,a)$ 接近 0，主要由 $V(s)$ 决定状态价值</li>
<li>DQN 在这些状态下还是会为每个动作都单独学 Q 值，效率低</li>
<li>Dueling DQN 可以直接先学 $V(s)$，不用反复去学相似的 Q 值，从而<strong>收敛更快</strong></li>
</ul>
<h3 id="7-2-Dueling-DQN-的优势"><a href="#7-2-Dueling-DQN-的优势" class="headerlink" title="7.2 Dueling DQN 的优势"></a>7.2 Dueling DQN 的优势</h3><ol>
<li><strong>泛化性更好</strong>：类似状态下的价值能被快速共享，减少重复学习</li>
<li><strong>学习更稳定</strong>：在动作选择不重要的状态里，仍能准确评估状态价值</li>
<li><strong>适用于在稀疏奖励任务</strong>：因为它能先学会哪些状态接近目标，再去细化动作优势</li>
</ol>
<p>MountainCar-v0 中的任务目标是先积累动能再冲顶。在上坡的中段，<strong>左右加速的效果差别很大</strong> → 依赖 $A(s,a)$ 来学优势。在坡底加速积能时，<strong>左右加速的差别没那么大</strong> → 依赖 $V(s)$ 来学状态价值。<br>结果就是，<strong>DQN</strong> 可能在不重要的状态浪费学习资源，而<strong>Dueling DQN</strong> 能更快聚焦在真正关键的动作选择时刻（比如冲顶前一两秒）。Dueling 架构能更快学到“山谷是低价值区域，山顶是高价值区域”。</p>
<h3 id="7-3-完整实现"><a href="#7-3-完整实现" class="headerlink" title="7.3 完整实现"></a>7.3 完整实现</h3><p><a target="_blank" rel="noopener" href="https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_dueling_DQN.py">https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_dueling_DQN.py</a></p>
<h3 id="7-4-成功率对比"><a href="#7-4-成功率对比" class="headerlink" title="7.4 成功率对比"></a>7.4 成功率对比</h3><p>DQN vs Double DQN vs Dueling DQN 实现的成功率对比：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">===== 成功率对比（100 回合） =====</span><br><span class="line">DQN          | 成功率: 99.00% | 平均步数: 123.66 | 平均终点位置: 0.51</span><br><span class="line">Double DQN   | 成功率: 100.00% | 平均步数: 122.57 | 平均终点位置: 0.51</span><br><span class="line">Dueling DQN  | 成功率: 100.00% | 平均步数: 126.50 | 平均终点位置: 0.53</span><br></pre></td></tr></table></figure>
<h2 id="8-Prioritized-Experience-Replay（优先经验回放）"><a href="#8-Prioritized-Experience-Replay（优先经验回放）" class="headerlink" title="8. Prioritized Experience Replay（优先经验回放）"></a>8. Prioritized Experience Replay（优先经验回放）</h2><h3 id="8-1-PER-和-DQN-的关系"><a href="#8-1-PER-和-DQN-的关系" class="headerlink" title="8.1 PER 和 DQN 的关系"></a>8.1 PER 和 DQN 的关系</h3><p><strong>DQN</strong>的核心思想是用<strong>经验回放（Replay Buffer）</strong> 打破样本之间的时间相关性，提升样本利用率。但 DQN 默认 <strong>均匀随机采样</strong>（Uniform Sampling）——每个样本被选中的概率一样，不管它对学习有多重要。<br><strong>PER（Prioritized Experience Replay）</strong> 是 DQN 的<strong>一个改进策略</strong>，它只改“采样”这一步，把经验池里的样本按照<strong>学习价值高低</strong>来调整被采样的概率。</p>
<ul>
<li>学习价值高 → 采样概率高（重点训练难学的、错误大的样本）     </li>
<li>学习价值低 → 采样概率低（减少浪费在早就学会的简单样本上）</li>
</ul>
<p>所以，<strong>PER 是 DQN 的一个升级版本</strong>，DQN 的其他部分（Q 网络、目标网络、损失函数等）都可以保持不变，只要改“怎么从经验池抽样”。</p>
<h3 id="8-2-PER-的原理"><a href="#8-2-PER-的原理" class="headerlink" title="8.2 PER 的原理"></a>8.2 PER 的原理</h3><p>在 DQN 中，每个训练样本的 TD 误差（Temporal-Difference Error）是：<br>$$\delta_i &#x3D; y_i - Q(s_i, a_i)$$</p>
<ul>
<li>$Q(s_i, a_i)$：当前网络预测的 Q 值</li>
<li>$y_i &#x3D; r_i + \gamma \max_{a’} Q_{\text{target}}(s’_i, a’)$：目标 Q 值</li>
<li>$|\delta_i|$ 表示当前预测和真实目标的差距。</li>
</ul>
<p>PER 认为：</p>
<ul>
<li><strong>差距大（|δ| 大）</strong> → 当前模型预测不准，这个样本对学习的帮助大 → 采样概率应该高</li>
<li><strong>差距小（|δ| 小）</strong> → 模型已经学得差不多，这个样本对学习帮助有限 → 采样概率可以低一些</li>
</ul>
<p>采样概率公式：<br>$P(i) &#x3D; \frac{|\delta_i|^\alpha}{\sum_k |\delta_k|^\alpha}$</p>
<ul>
<li><strong>α</strong> 控制优先程度（α&#x3D;0 就退化为均匀采样）</li>
<li>TD 误差大 → 概率高</li>
</ul>
<p>另外，为了防止高概率样本被重复学习过多，PER 会配合<strong>重要性采样权重（IS weight）</strong> 做修正，保证训练无偏性。</p>
<h3 id="8-3-PER-在-MountainCar-v0-中的意义"><a href="#8-3-PER-在-MountainCar-v0-中的意义" class="headerlink" title="8.3 PER 在 MountainCar-v0 中的意义"></a>8.3 PER 在 MountainCar-v0 中的意义</h3><p>MountainCar 是一个<strong>稀疏奖励 + 延迟奖励</strong>任务，普通 DQN 学得慢的原因之一是：</p>
<ul>
<li>大部分时间，小车动作对最终奖励影响很小（例如卡在坡中间的微调动作）</li>
<li>真正关键的状态（比如冲顶前的几个动作、助跑的反向加速）出现概率很低，如果采样不够多，网络很难学到</li>
</ul>
<p>PER 的作用：</p>
<ul>
<li><strong>放大关键瞬间的学习频率</strong>：当网络第一次成功冲顶时，这些状态的 TD 误差会很大，PER 会让它们在后续训练中反复出现，强化记忆</li>
<li><strong>减少浪费时间在无关状态</strong>：对已经学会的坡底慢速动作，TD 误差会变小，采样概率降低</li>
</ul>
<p>这样，<strong>MountainCar 用 PER 收敛速度会比普通 DQN 快很多</strong>，尤其是在早期探索到关键路径时。</p>
<h3 id="8-4-完整实现"><a href="#8-4-完整实现" class="headerlink" title="8.4 完整实现"></a>8.4 完整实现</h3><p><a target="_blank" rel="noopener" href="https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_PER_DQN.py">https://github.com/keychankc/dl_code_for_blog/blob/main/027_DQN_code/mountainCar_PER_DQN.py</a></p>
<h3 id="8-5-成功率对比"><a href="#8-5-成功率对比" class="headerlink" title="8.5 成功率对比"></a>8.5 成功率对比</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">===== 成功率对比（100 回合） =====</span><br><span class="line">DQN         | 成功率: 100.00% | 平均步数: 122.94 | 平均终点位置: 0.52</span><br><span class="line">Double DQN  | 成功率: 100.00% | 平均步数: 120.32 | 平均终点位置: 0.51</span><br><span class="line">Dueling DQN  | 成功率: 100.00% | 平均步数: 121.35 | 平均终点位置: 0.53</span><br><span class="line">PER DQN      | 成功率: 100.00% | 平均步数: 119.92 | 平均终点位置: 0.52</span><br></pre></td></tr></table></figure>
<p><strong>MountainCar-v0</strong> 这个任务太简单了，状态空间低维（2维），而且目标非常明确，DQN 本身就足够解决得很好， <strong>Double DQN、Dueling DQN、PER DQN</strong> 三个改进点在这里的性能提升几乎看不出来。</p>
<h2 id="9-总结与延伸"><a href="#9-总结与延伸" class="headerlink" title="9.总结与延伸"></a>9.总结与延伸</h2><table>
<thead>
<tr>
<th>​<strong>方法</strong>​</th>
<th>​<strong>核心改进</strong>​</th>
<th>​<strong>任务优势</strong>​</th>
<th>​<strong>适用场景</strong>​</th>
</tr>
</thead>
<tbody><tr>
<td>​<strong>原始DQN</strong>​</td>
<td>神经网络拟合Q值函数</td>
<td>基础解决方案，适合简单任务</td>
<td>低维状态空间，动作选择明确的任务</td>
</tr>
<tr>
<td>​<strong>Double DQN</strong>​</td>
<td>解耦动作选择与价值评估</td>
<td>减少Q值高估，策略更稳定</td>
<td>需长期策略的任务（如延迟回报明显）</td>
</tr>
<tr>
<td>​<strong>Dueling DQN</strong>​</td>
<td>分离状态价值(V)和动作优势(A)</td>
<td>更快识别关键状态，减少冗余学习</td>
<td>动作影响差异大的任务（如冲刺关键期）</td>
</tr>
<tr>
<td>​<strong>优先经验回放(PER)​</strong>​</td>
<td>按TD误差优先级采样</td>
<td>加速收敛，聚焦关键经验</td>
<td>稀疏奖励或关键样本稀少的任务</td>
</tr>
<tr>
<td><strong>补充说明</strong>​：</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol>
<li>在MountainCar-v0中，因任务简单（状态仅2维），所有方法均能100%成功，改进版优势不明显</li>
<li>若环境更复杂（如高维状态&#x2F;稀疏奖励），改进方法（尤其是Dueling+PER组合）的收敛速度和稳定性优势会更显著</li>
</ol>
<h2 id="10-备注"><a href="#10-备注" class="headerlink" title="10. 备注"></a>10. 备注</h2><p>环境：</p>
<ul>
<li>mac: 15.2</li>
<li>python: 3.12.4</li>
<li>pytorch: 2.5.1</li>
<li>numpy: 1.26.4</li>
<li>gymnasium: 1.2.0</li>
<li>box2d-py: 2.3.8</li>
</ul>
<p>完整代码：<br>	<a target="_blank" rel="noopener" href="https://github.com/keychankc/dl_code_for_blog/tree/main/027_DQN_code">https://github.com/keychankc/dl_code_for_blog/tree/main/027_DQN_code</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/" title="DQN(Deep Q-Network)系列算法解析与实践">https://www.keychan.xyz/2025/08/14/027-rl-dqn-case/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C/" rel="tag"># 目标网络</a>
              <a href="/tags/%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE/" rel="tag"># 经验回放</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/30/026-rl-ppo-discrete-continuous-case/" rel="prev" title="PPO算法在连续与离散动作空间中的案例实践">
                  <i class="fa fa-angle-left"></i> PPO算法在连续与离散动作空间中的案例实践
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/08/22/028-rl-ac-a3c-mario-case-1/" rel="next" title="A3C 算法原理与超级马里奥实践（上）">
                  A3C 算法原理与超级马里奥实践（上） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">224k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/08/14/027-rl-dqn-case/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
