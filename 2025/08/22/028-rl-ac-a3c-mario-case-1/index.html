<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"keychankc.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1. AC 算法1.1 策略梯度在强化学习中，如果我们想让智能体学会这样一个 策略（在不同状态下选什么动作）:  一个动作能带来高奖励，就要让它以后更可能被选上 一个动作只能带来低回报，就要减少使用它的频率  而策略梯度就是一个这样的工具，“根据奖励信号，调整策略参数，让好动作更可能被选中，坏动作少被选上。”">
<meta property="og:type" content="article">
<meta property="og:title" content="A3C 算法原理与超级马里奥实践（上）">
<meta property="og:url" content="https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1. AC 算法1.1 策略梯度在强化学习中，如果我们想让智能体学会这样一个 策略（在不同状态下选什么动作）:  一个动作能带来高奖励，就要让它以后更可能被选上 一个动作只能带来低回报，就要减少使用它的频率  而策略梯度就是一个这样的工具，“根据奖励信号，调整策略参数，让好动作更可能被选中，坏动作少被选上。”">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-22T05:40:12.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.542Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="策略梯度">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/","path":"2025/08/22/028-rl-ac-a3c-mario-case-1/","title":"A3C 算法原理与超级马里奥实践（上）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>A3C 算法原理与超级马里奥实践（上） | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-AC-%E7%AE%97%E6%B3%95"><span class="nav-text">1. AC 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">1.1 策略梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%A5%96%E5%8A%B1%E6%9C%9F%E6%9C%9B"><span class="nav-text">1.2 奖励期望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Baseline-%E7%9A%84%E5%BC%95%E5%85%A5%E4%B8%8E-Critic-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-text">1.3 Baseline 的引入与 Critic 的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Q-%E5%80%BC%E4%B8%8E%E5%9B%9E%E6%8A%A5%E7%9A%84%E8%BF%91%E4%BC%BC%E5%85%B3%E7%B3%BB"><span class="nav-text">1.4 Q 值与回报的近似关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%84%8F%E4%B9%89"><span class="nav-text">1.5 优势函数的定义与意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-Actor-%E4%B8%8E-Critic-%E7%9A%84%E5%88%86%E5%B7%A5"><span class="nav-text">1.6 Actor 与 Critic 的分工</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90"><span class="nav-text">2. 优势函数深入解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-text">2.1 优势函数的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Q-%E7%9A%84%E8%BF%91%E4%BC%BC"><span class="nav-text">2.2 Q 的近似</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E7%9A%84%E8%BF%91%E4%BC%BC"><span class="nav-text">2.3 优势函数的近似</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-A3C-%E4%B8%AD%E7%9A%84%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B"><span class="nav-text">3. A3C 中的计算流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E8%BD%A8%E8%BF%B9%E7%94%9F%E6%88%90"><span class="nav-text">3.1 数据采集与轨迹生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%EF%BC%88%E8%BF%91%E4%BC%BC%E5%8C%96%E5%8E%9F%E5%9B%A0%EF%BC%89"><span class="nav-text">3.2 优势函数计算（近似化原因）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%8F%8C%E7%BD%91%E7%BB%9C%E6%9B%B4%E6%96%B0%EF%BC%88Actor-Critic%EF%BC%89"><span class="nav-text">3.3 双网络更新（Actor + Critic）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-Actor-%E6%9B%B4%E6%96%B0%EF%BC%9A%E5%AD%A6%E4%BC%9A%E9%80%89%E5%A5%BD%E5%8A%A8%E4%BD%9C"><span class="nav-text">3.3.1 Actor 更新：学会选好动作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-Critic-%E6%9B%B4%E6%96%B0%EF%BC%9A%E5%AD%A6%E4%BC%9A%E8%AF%84%E4%BC%B0%E7%8A%B6%E6%80%81"><span class="nav-text">3.3.2 Critic 更新：学会评估状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-Actor-Critic-%E5%85%B1%E4%BA%AB%E4%B8%BB%E5%B9%B2%E7%BD%91%E7%BB%9C"><span class="nav-text">3.3.3 Actor + Critic 共享主干网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-text">3.4 稳定性问题与解决方案</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-A3C-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="nav-text">4. A3C 整体架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%B8%8E-AC-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">4.1 与 AC 的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%85%A8%E5%B1%80%E7%BD%91%E7%BB%9C-vs-%E6%9C%AC%E5%9C%B0%E7%BD%91%E7%BB%9C"><span class="nav-text">4.2 全局网络 vs 本地网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E5%BC%82%E6%AD%A5%E6%9B%B4%E6%96%B0%E6%B5%81%E7%A8%8B"><span class="nav-text">4.3 异步更新流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%B9%B6%E8%A1%8C%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="nav-text">4.4 多智能体并行的好处</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-A3C-%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">5. A3C 的损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E7%AD%96%E7%95%A5%E6%8D%9F%E5%A4%B1%EF%BC%88Policy-Loss%EF%BC%89"><span class="nav-text">5.1 策略损失（Policy Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E4%BB%B7%E5%80%BC%E6%8D%9F%E5%A4%B1%EF%BC%88Value-Loss%EF%BC%89"><span class="nav-text">5.2 价值损失（Value Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Entropy-Loss%EF%BC%89"><span class="nav-text">5.3 熵损失（Entropy Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E6%80%BB%E4%BD%93%E6%8D%9F%E5%A4%B1%E5%85%AC%E5%BC%8F"><span class="nav-text">5.4 总体损失公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E6%80%BB%E7%BB%93"><span class="nav-text">6. 总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E2%80%8B%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E4%B8%8E%E5%9F%BA%E7%BA%BF%EF%BC%88Baseline%EF%BC%89%E7%9A%84%E5%BC%95%E5%85%A5%E2%80%8B"><span class="nav-text">6.1 ​策略梯度与基线（Baseline）的引入​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E2%80%8BActor-Critic-%E6%9E%B6%E6%9E%84%E7%9A%84%E5%88%86%E5%B7%A5%E2%80%8B"><span class="nav-text">6.2 ​Actor-Critic 架构的分工​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E2%80%8B%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%88Advantage-Function%EF%BC%89%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BD%9C%E7%94%A8%E2%80%8B"><span class="nav-text">6.3 ​优势函数（Advantage Function）的核心作用​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-A3C-%E7%AE%97%E6%B3%95%E7%9A%84%E5%BC%82%E6%AD%A5%E5%B9%B6%E8%A1%8C%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%88%9B%E6%96%B0%E2%80%8B"><span class="nav-text">6.4 A3C 算法的异步并行架构与创新​</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-A3C-%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A8%B3%E5%AE%9A%E5%8C%96%E6%8A%80%E6%9C%AF%E2%80%8B"><span class="nav-text">6.5 A3C 的损失函数与稳定化技术​</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/keychankc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="A3C 算法原理与超级马里奥实践（上） | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A3C 算法原理与超级马里奥实践（上）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-22 13:40:12" itemprop="dateCreated datePublished" datetime="2025-08-22T13:40:12+08:00">2025-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/08/22/028-rl-ac-a3c-mario-case-1/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/08/22/028-rl-ac-a3c-mario-case-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-AC-算法"><a href="#1-AC-算法" class="headerlink" title="1. AC 算法"></a>1. AC 算法</h2><h3 id="1-1-策略梯度"><a href="#1-1-策略梯度" class="headerlink" title="1.1 策略梯度"></a>1.1 策略梯度</h3><p>在强化学习中，如果我们想让智能体学会这样一个 <strong>策略</strong>（在不同状态下选什么动作）:</p>
<ul>
<li>一个动作能带来高奖励，就要让它以后更可能被选上</li>
<li>一个动作只能带来低回报，就要减少使用它的频率</li>
</ul>
<p>而策略梯度就是一个这样的工具，<strong>“根据奖励信号，调整策略参数，让好动作更可能被选中，坏动作少被选上。”</strong></p>
<span id="more"></span>
<p>策略梯度，为什么要加上“梯度”？<br>因为我们的目标是如何最大化期望奖励：$J(\theta) &#x3D; \mathbb{E}{\pi\theta}[R]$。这就像爬山——目标是爬到“最大奖励”的山顶，梯度就是指引方向的指南针：告诉我们“往哪个方向改策略，奖励会变大”。</p>
<p>策略梯度的公式：<br>$$\nabla_\theta J(\theta) &#x3D; \mathbb{E}{\pi\theta}\Big[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^\pi(s,a)\Big]$$</p>
<ul>
<li>$\nabla_\theta \log \pi_\theta(a|s)$：表示如果我要让动作 $a$ 在状态 $s$ 下更常出现，参数该怎么调整？简单来说就是一个方向提示</li>
<li>$Q^\pi(s,a)$：这个动作到底值不值得？也就是动作的分数</li>
</ul>
<p>整个公式的意思是：<strong>调整参数，让高价值动作出现概率上升，低价值动作出现概率下降。</strong></p>
<h3 id="1-2-奖励期望"><a href="#1-2-奖励期望" class="headerlink" title="1.2 奖励期望"></a>1.2 奖励期望</h3><p>在强化学习中，每次智能体执行动作，环境会给出一个奖励 $r$</p>
<ul>
<li><strong>累积奖励</strong> $R$ ：一条轨迹（episode）里所有奖励的总和，或者折扣累积总和</li>
<li><strong>奖励期望</strong>：按照当前策略 $\pi$ 执行动作，智能体在长期平均下来可以获得的奖励的期望值</li>
</ul>
<p>用公式表示为：$J(\theta) &#x3D; \mathbb{E}{\pi\theta}[R]$<br>意思就是：<strong>如果按照策略</strong> $\pi_\theta$ <strong>玩很多次游戏，平均每次能得到多少奖励</strong>。<br>举个例子，就像某人每天做菜会选择不同食谱，而奖励就是吃的满意度，如果后面一周一直按某种做菜策略选择食谱，<strong>奖励期望</strong>就是这一周的总满意度 &#x2F; 7。</p>
<h3 id="1-3-Baseline-的引入与-Critic-的作用"><a href="#1-3-Baseline-的引入与-Critic-的作用" class="headerlink" title="1.3 Baseline 的引入与 Critic 的作用"></a>1.3 Baseline 的引入与 Critic 的作用</h3><p>在<strong>策略梯度</strong>里，如果动作 $a$ 的回报 $Q^\pi(s,a)$ 高，就增加它的概率，如果回报低，就减少它的概率。<br>但问题是如果$Q^\pi(s,a)$ 的数值波动大（高方差），训练就会变得很不稳定。比如某人今天心情好奖励高，明天环境差奖励低，这样就会导致梯度的更新方向乱跳。</p>
<p>为了让更新更稳定，我们引入一个 <strong>基准线（baseline）</strong>，公式变为：<br>$$\nabla_\theta J(\theta) &#x3D; \mathbb{E}{\pi\theta}\big[ \nabla_\theta \log \pi_\theta(a|s) \cdot (Q^\pi(s,a) - V^\pi(s)) \big]$$<br>这里的 baseline 最常用就是 <strong>状态值函数</strong> $V^\pi(s)$。</p>
<ul>
<li>$Q^\pi(s,a)$：<strong>在状态</strong> $s$ <strong>下，选择动作</strong> $a$ <strong>的概率分布</strong>，然后按照策略 $\pi$ 继续执行，能得到的 <strong>长期累计奖励的期望</strong>，Q 值并不是在算概率，而是在算 <strong>某个具体动作一旦发生后的表现</strong></li>
<li>$V^\pi(s)$：在当前状态下，不指定动作，<strong>平均水平</strong>的表现</li>
<li>两者相减 $(Q^\pi(s,a) - V^\pi(s))$：就是“动作 $a$ 相比平均水平，到底好多少或差多少”</li>
</ul>
<p>这样一来，如果动作比平均水平好 → 增加概率，如果动作比平均水平差 → 降低概率，如果差不多 → 不用大幅调整。这样的更新更有针对性，方差也大大降低。</p>
<p> 以上策略也是<strong>Actor-Critic</strong> 架构的由来：</p>
<ul>
<li><strong>Actor（演员）</strong>：负责学策略 $\pi_\theta(a|s)$，输出“我下一步该怎么做”，像一个舞台演员，他想尝试各种表演动作（策略）</li>
<li><strong>Critic（评论家）</strong>：负责学状态值函数 $V^\pi(s)$，用作 baseline，给 Actor 一个“基准参考”，像观众&#x2F;评论家，给出“这个动作比一般水平好&#x2F;差多少”的评价</li>
</ul>
<p>有了 Critic 的反馈，Actor 就能更稳定地提升自己的表现。</p>
<p>原始策略梯度只看“绝对奖励” → 学习抖动大，而引入 baseline 让更新看“相对奖励” → 更平稳。Critic 的任务就是提供这个 baseline（通常是 $V^\pi(s)$）。<br>所以 Actor-Critic 的本质就是：<strong>Actor 负责决策，Critic 负责评价，二者协作使得学习更快更稳。</strong></p>
<h3 id="1-4-Q-值与回报的近似关系"><a href="#1-4-Q-值与回报的近似关系" class="headerlink" title="1.4 Q 值与回报的近似关系"></a>1.4 Q 值与回报的近似关系</h3><p>在强化学习里，<strong>Q 值</strong> $Q^\pi(s,a)$ 的意思是在状态 $s$ 下执行动作 $a$，然后继续按照策略 $\pi$ 行动，能得到的 <strong>长期累计奖励的期望</strong>。但问题是，这个“长期累计奖励”通常需要跑完整条轨迹（可能很长）才能算出来，在实际应用中（比如玩游戏、机器人控制），这样做代价太大。</p>
<p>于是，有人想了个聪明的办法：<strong>不用看那么远，先近似一下</strong>。<br>公式：<br>$$Q^\pi(s,a) \approx r(s,a) + \gamma V^\pi(s’)$$<br>你在 $s$ 状态下做了动作 $a$，立刻得到了一个 <strong>即时奖励</strong> $r(s,a)$，然后环境跳转到新状态 $s’$，在这个新状态下，你还能继续获得未来的奖励，未来的奖励我们就用 Critic 学到的 <strong>状态值函数</strong> $V^\pi(s’)$ 来估计，折扣因子 $\gamma$ 表示“未来的奖励要打点折扣，因为不确定性更大”。<br>换句话说：<strong>Q 值 ≈ 当前得到的奖励 + 未来可能奖励的估计</strong>。</p>
<p>打个比方，想象你投资一个项目：今天投进去 100 块（即时奖励），明天这个项目的价值取决于市场行情（下一个状态的价值），你关心的不是仅仅今天的收益，而是 <strong>今天赚多少 + 明天可能的未来价值</strong>，但未来越远越不确定，所以要打折（$\gamma$）。</p>
<p>这样做的好处是不用等完整一局游戏打完再更新，可以 <strong>在线学习</strong>，这样更新速度会更快，更符合实际应用（比如实时决策）。这其实就是 <strong>时序差分（Temporal Difference, TD）方法</strong> 的核心思想。</p>
<h3 id="1-5-优势函数的定义与意义"><a href="#1-5-优势函数的定义与意义" class="headerlink" title="1.5 优势函数的定义与意义"></a>1.5 优势函数的定义与意义</h3><p>优势函数定义：<br>$$A^\pi(s,a) &#x3D; Q^\pi(s,a) - V^\pi(s)$$</p>
<ul>
<li>$Q^\pi(s,a)$：表示“在状态 $s$ 下，做动作 $a$，然后继续执行策略 $\pi$，能得到的总回报”</li>
<li>$V^\pi(s)$：表示“在状态 $s$ 下，按照策略 $\pi$ 的平均水平，能得到的总回报”</li>
</ul>
<p>所以，$A^\pi(s,a)$ 就是 <strong>动作</strong> $a$ <strong>相比平均水平，到底好多少或差多少</strong>。</p>
<p>想象一个同学在学校里考试，考了 90 分（对应 $Q^\pi(s,a)$），班级平均分是 85 分（对应 $V^\pi(s)$），那么他的优势就是 90 - 85 &#x3D; +5。如果另一个同学考了 70 分，相比平均分 85，他的优势就是 70 - 85 &#x3D; -15。所以 <strong>优势函数本质上就是一个“相对好坏的评价标准”</strong>。</p>
<p>在策略梯度更新里，原始版本用的是 $Q^\pi(s,a)$，但有问题，它只告诉你动作的“绝对表现”，没告诉你“相对表现”。<br>举个例子：假设某状态下所有动作回报都很高（环境奖励普遍高），那你更新时可能会把所有动作概率都加大，其实没有意义。<br>引入优势函数后，就变成了只关心某个动作比“平均水平”好多少。好就增加概率，差就减少概率，如果差不多，就别乱调。这样更新方向更精准，学习更高效。</p>
<p>策略梯度的最终公式：<br>$$\nabla_\theta J(\theta) &#x3D; \mathbb{E}{\pi\theta}\big[ \nabla_\theta \log \pi_\theta(a|s) \cdot A^\pi(s,a) \big]$$</p>
<ul>
<li>$\nabla_\theta \log \pi_\theta(a|s)$：如果要多做这个动作，应该怎样调整参数</li>
<li>$A^\pi(s,a)$：这个动作值得多做还是少做</li>
</ul>
<p>合起来就是：<strong>往“有优势”的动作方向调整策略</strong>。</p>
<p>优势函数衡量了“一个动作相对于平均水平的好坏”，它让策略更新更精准、更稳定，是 Actor-Critic 系列算法的核心工具。</p>
<h3 id="1-6-Actor-与-Critic-的分工"><a href="#1-6-Actor-与-Critic-的分工" class="headerlink" title="1.6 Actor 与 Critic 的分工"></a>1.6 Actor 与 Critic 的分工</h3><ul>
<li><strong>Actor</strong>：根据策略 $\pi_\theta$ 选择动作，并通过梯度更新策略。</li>
<li><strong>Critic</strong>：估计状态值函数 $V^\pi(s)$，帮助构造 baseline，从而降低方差。</li>
<li><strong>优势函数 $A(s,a)$</strong>：把 Actor 的学习信号调节成“比平均水平好多少”，让训练更加高效。</li>
</ul>
<p>我们想最大化回报 $\mathbb{E}[R]$，于是引出 <strong>策略梯度方法</strong>，但带来了一个方差太大的问题。为了解决方差太大、训练不稳定的问题，自然引出了 <strong>Critic</strong> 的角色。Critic 通过学习状态值函数 $V^\pi(s)$，作为一个 <strong>baseline</strong>，帮助我们在更新策略时不直接依赖高方差的 $Q^\pi(s,a)$。进一步地，借助近似关系 $Q^\pi(s,a) \approx r + \gamma V^\pi(s’)$，Critic 估计的 $V$ 还能被用来间接构造 $Q$，从而让 Actor 的更新既可计算、又更稳定。</p>
<h2 id="2-优势函数深入解析"><a href="#2-优势函数深入解析" class="headerlink" title="2. 优势函数深入解析"></a>2. 优势函数深入解析</h2><p>在前面我们已经聊过策略梯度、baseline 以及 Actor-Critic，现在再看看 <strong>优势函数（Advantage Function）</strong> 。它其实是 Actor-Critic 中的一个关键工具，帮助我们判断 <strong>某个动作到底值不值得选</strong>。</p>
<h3 id="2-1-优势函数的定义"><a href="#2-1-优势函数的定义" class="headerlink" title="2.1 优势函数的定义"></a>2.1 优势函数的定义</h3><p>$$A^\pi(s,a) &#x3D; Q^\pi(s,a) - V^\pi(s)$$</p>
<ul>
<li><strong>$Q^\pi(s,a)$</strong>：在状态 $s$ 下执行动作 $a$，然后继续按照策略 $\pi$ 行动，能拿到的<strong>长期累计奖励的期望</strong>。可以理解为：<strong>“这个动作的绝对表现”</strong>。</li>
<li><strong>$V^\pi(s)$</strong>：在状态 $s$ 下，不管选什么动作，按照策略 $\pi$ 的整体水平，能拿到的平均回报。可以理解为：<strong>“这个状态下的平均水平”</strong>。</li>
</ul>
<p>于是，优势函数就是：“某个动作的表现” - “当前状态的平均表现”。</p>
<h3 id="2-2-Q-的近似"><a href="#2-2-Q-的近似" class="headerlink" title="2.2 Q 的近似"></a>2.2 Q 的近似</h3><p>在实际计算中，Q 值不好直接估。怎么办？<br>我们可以用 <strong>一步回报加上后续状态的价值</strong> 来近似：<br>$$Q^\pi(s,a) \approx r(s,a) + \gamma V^\pi(s’)$$<br>意思是：<strong>这次动作拿到的即时奖励 + 后续状态的价值折扣</strong>。</p>
<h3 id="2-3-优势函数的近似"><a href="#2-3-优势函数的近似" class="headerlink" title="2.3 优势函数的近似"></a>2.3 优势函数的近似</h3><p>把上面这个近似放进优势函数里：<br>$$A^\pi(s,a) \approx r(s,a) + \gamma V^\pi(s’) - V^\pi(s)$$<br>看出来没？我们只需要知道：</p>
<ul>
<li>当前的即时奖励 $r(s,a)$</li>
<li>下一步的价值 $V^\pi(s’)$</li>
<li>当前的价值 $V^\pi(s)$</li>
</ul>
<p>就能估出来优势函数。</p>
<p>这意味着我们根本不需要再去训练一个庞大的 Critic 来估 Q。只要 Critic 能学好 <strong>$V(s)$</strong>，我们就能间接得到 $Q$ 和 $A$。这就极大降低了复杂度。因为 $V(s)$ 只跟状态有关，估计起来比 $Q(s,a)$ （既依赖状态又依赖动作）要简单得多。</p>
<h2 id="3-A3C-中的计算流程"><a href="#3-A3C-中的计算流程" class="headerlink" title="3. A3C 中的计算流程"></a>3. A3C 中的计算流程</h2><h3 id="3-1-数据采集与轨迹生成"><a href="#3-1-数据采集与轨迹生成" class="headerlink" title="3.1 数据采集与轨迹生成"></a>3.1 数据采集与轨迹生成</h3><p>A3C 的核心思想之一是 <strong>多线程异步采集数据</strong>，即每个线程（worker）在环境里独立运行自己的 Actor，按照当前策略 π 采样动作。线程不断生成 <strong>状态-动作-奖励序列</strong>（trajectory），比如：$s_0, a_0, r_0, s_1, a_1, r_1, …, s_T$</p>
<p>异步采样优势：</p>
<ol>
<li>提高训练效率</li>
<li>避免单线程样本相关性太强导致训练不稳定</li>
<li>不同线程探索不同策略，增加策略多样性</li>
</ol>
<h3 id="3-2-优势函数计算（近似化原因）"><a href="#3-2-优势函数计算（近似化原因）" class="headerlink" title="3.2 优势函数计算（近似化原因）"></a>3.2 优势函数计算（近似化原因）</h3><p>前面讲过优势函数 $A^\pi(s,a) \approx r + \gamma V(s’) - V(s)$，可以理解为：“这次动作比平均水平多拿了多少分”。</p>
<p>如果只用一步回报，优势函数可能 <strong>噪声大</strong>，因为单步奖励可能波动很大。例如，你走了一步，突然踩到陷阱，$r$ 很小，但后续可能有大回报，单步估计会误导 Actor，策略更新不稳定。<br>n-step 回报就是折中方案：既考虑了短期奖励，也引入了未来价值的估计。公式是：<br>$$R_t^{(n)} &#x3D; r_t + \gamma r_{t+1} + \dots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$<br>然后优势函数用它计算：$A(s_t, a_t) &#x3D; R_t^{(n)} - V(s_t)$</p>
<h3 id="3-3-双网络更新（Actor-Critic）"><a href="#3-3-双网络更新（Actor-Critic）" class="headerlink" title="3.3 双网络更新（Actor + Critic）"></a>3.3 双网络更新（Actor + Critic）</h3><h4 id="3-3-1-Actor-更新：学会选好动作"><a href="#3-3-1-Actor-更新：学会选好动作" class="headerlink" title="3.3.1 Actor 更新：学会选好动作"></a>3.3.1 Actor 更新：学会选好动作</h4><p>公式：$L_\text{actor} &#x3D; - \log \pi(a|s) \cdot A(s,a)$<br>Actor 的任务就是学会策略 $π(a|s)$，让“好动作”更容易被选。<br>我们用优势函数 $A(s,a)$ 来告诉 Actor：        </p>
<ul>
<li>A &gt; 0 → 这个动作比平均水平好 → 提高它被选的概率</li>
<li>A &lt; 0 → 这个动作比平均水平差 → 降低它被选的概率</li>
</ul>
<h4 id="3-3-2-Critic-更新：学会评估状态"><a href="#3-3-2-Critic-更新：学会评估状态" class="headerlink" title="3.3.2 Critic 更新：学会评估状态"></a>3.3.2 Critic 更新：学会评估状态</h4><p>公式：$L_\text{critic} &#x3D; (R - V(s))^2$<br>Critic 负责估计状态价值 $V(s)$，告诉 Actor “当前状态的平均表现是多少”。<br>更新目标是尽量让 $V(s)$ 接近实际的 n-step 回报 R（即未来奖励的估计值）。</p>
<h4 id="3-3-3-Actor-Critic-共享主干网络"><a href="#3-3-3-Actor-Critic-共享主干网络" class="headerlink" title="3.3.3 Actor + Critic 共享主干网络"></a>3.3.3 Actor + Critic 共享主干网络</h4><p><strong>共享主干网络</strong>：卷积层或全连接层，用来提取状态特征，好处是避免重复计算，提升效率。<br><strong>两条分支</strong>：    </p>
<ol>
<li><strong>Actor 分支</strong>：输出动作概率 $π(a|s)$</li>
<li><strong>Critic 分支</strong>：输出状态价值 $V(s)$</li>
</ol>
<p>优势：状态表示一致，Actor 和 Critic 可以互相辅助；Critic 提供优势函数给 Actor，Actor 产生的策略又反过来指导 Critic 更新。</p>
<h3 id="3-4-稳定性问题与解决方案"><a href="#3-4-稳定性问题与解决方案" class="headerlink" title="3.4 稳定性问题与解决方案"></a>3.4 稳定性问题与解决方案</h3><p>A3C 的训练本质是异步更新，可能出现：</p>
<ul>
<li><strong>梯度噪声大</strong>：多个线程同时更新全局网络</li>
<li><strong>策略崩塌</strong>：Actor 在探索不足或优势估计不稳定时，策略可能退化</li>
</ul>
<p>解决方案：</p>
<ol>
<li><strong>异步多线程</strong>：各线程采样不同轨迹，互相平均梯度，减小方差</li>
<li><strong>优势函数减方差</strong>：用 $V(s)$ 做 baseline</li>
<li><strong>熵正则化（Entropy Regularization）</strong>：鼓励策略保持一定随机性，避免过早收敛$L_\text{entropy} &#x3D; -\beta \sum_a \pi(a|s) \log \pi(a|s)$</li>
</ol>
<p>A3C 就像多个小队同时在不同战场打仗，各自收集战况（数据），把战况汇报给总部（全局网络）更新策略。Critic 提供参考评分（优势函数），n-step 回报让评分更稳，熵正则保证队伍不太偏激。</p>
<h2 id="4-A3C-整体架构"><a href="#4-A3C-整体架构" class="headerlink" title="4. A3C 整体架构"></a>4. A3C 整体架构</h2><h3 id="4-1-与-AC-的关系"><a href="#4-1-与-AC-的关系" class="headerlink" title="4.1 与 AC 的关系"></a>4.1 与 AC 的关系</h3><ul>
<li><strong>AC（Actor-Critic）</strong>：单线程版本，Actor 负责策略 $π$，Critic 负责状态价值 $V(s)$。</li>
<li><strong>A3C（Asynchronous Advantage Actor-Critic）</strong>：在 AC 的基础上有两个关键增强：<ol>
<li><strong>Asynchronous（异步）</strong>：引入多线程异步更新，同时启动多个 <strong>本地 Actor-Critic</strong> 线程，每个线程在自己环境里采集数据，然后把梯度异步更新到全局网络；</li>
<li><strong>Advantage（优势函数）</strong>：不用直接学习 $Q(s,a)$，而是通过 $A(s,a) &#x3D; Q(s,a) - V(s)$ 来衡量动作相对好坏，从而降低方差、提升训练稳定性。</li>
</ol>
</li>
</ul>
<p>A3C &#x3D; “多人同时玩游戏，每个人自己练，同时把经验汇报给总部”，而 AC 只能一个人玩。</p>
<h3 id="4-2-全局网络-vs-本地网络"><a href="#4-2-全局网络-vs-本地网络" class="headerlink" title="4.2 全局网络 vs 本地网络"></a>4.2 全局网络 vs 本地网络</h3><p>A3C 有两个层次的网络：</p>
<ol>
<li><strong>全局网络（Global Network）</strong>，负责存储全局的策略 $π$ 和状态价值 $V(s)$，接收各线程上传的梯度，更新权重，充当“策略大脑”或总部</li>
<li><strong>本地网络（Local Network）</strong>，每个线程有自己一份副本，用于与环境交互采样，采集完数据后计算梯度，然后异步发送给全局网络，最后更新后同步全局网络的参数，保证策略一致</li>
</ol>
<p>全局网络是总部，本地网络是各个分队，每个分队都在前线收集信息、训练经验，然后把梯度发回总部，总部更新策略后再下发给各分队。</p>
<h3 id="4-3-异步更新流程"><a href="#4-3-异步更新流程" class="headerlink" title="4.3 异步更新流程"></a>4.3 异步更新流程</h3><p>整体流程可以拆成四步：</p>
<ol>
<li><strong>采集</strong>：每个线程独立与环境交互，生成 状态-动作-奖励序列</li>
<li><strong>梯度计算</strong>：用采集到的轨迹计算优势函数 $A(s,a)$，得到 Actor&#x2F;Critic 的梯度</li>
<li><strong>异步更新</strong>：把梯度发送给全局网络，更新全局策略和价值网络</li>
<li><strong>同步</strong>：本地线程把更新后的全局网络参数同步回来，继续采集下一轮数据</li>
</ol>
<p>就像“前线特工 → 发送报告 → 总部更新策略 → 特工拿到新指令再行动”，循环往复。</p>
<h3 id="4-4-多智能体并行的好处"><a href="#4-4-多智能体并行的好处" class="headerlink" title="4.4 多智能体并行的好处"></a>4.4 多智能体并行的好处</h3><ol>
<li><strong>降低样本相关性</strong>：单线程 AC 连续采样的数据强相关，容易导致训练不稳定，多线程采样的数据彼此独立或弱相关，梯度更新更稳健</li>
<li><strong>提升策略探索能力</strong>：每个线程探索不同状态、动作，能发现更多策略组合，避免单线程陷入局部最优</li>
</ol>
<h2 id="5-A3C-的损失函数"><a href="#5-A3C-的损失函数" class="headerlink" title="5. A3C 的损失函数"></a>5. A3C 的损失函数</h2><h3 id="5-1-策略损失（Policy-Loss）"><a href="#5-1-策略损失（Policy-Loss）" class="headerlink" title="5.1 策略损失（Policy Loss）"></a>5.1 策略损失（Policy Loss）</h3><p>公式：$L_{policy} &#x3D; - \log \pi(a|s) \cdot A(s,a)$<br>作用：让 Actor 学会选“好动作”，放大优势动作的概率，抑制劣势动作。</p>
<ul>
<li>优势函数 $A(s,a)$ &gt; 0 → 这个动作比平均水平好 → Actor 提高被选概率     </li>
<li>优势函数 $A(s,a)$ &lt; 0 → 动作比平均水平差 → Actor 降低被选概率</li>
</ul>
<p>Actor 就像一个决策者，优势函数是评分表。好动作分数高 → 多用，坏动作分数低 → 少用。</p>
<h3 id="5-2-价值损失（Value-Loss）"><a href="#5-2-价值损失（Value-Loss）" class="headerlink" title="5.2 价值损失（Value Loss）"></a>5.2 价值损失（Value Loss）</h3><p>公式：$L_{value} &#x3D; (R - V(s))^2$<br>作用：让 Critic 学会准确预测状态价值 V(s)，学会打分，减小估计误差。    </p>
<ul>
<li>Critic 给每个状态打分，R 是 n-step 回报的真实估计</li>
<li>V(s) 是 Critic 的预测        </li>
<li>损失函数是 MSE（均方误差），优化 V(s) 逼近真实回报</li>
</ul>
<p> Critic 是裁判，价值损失是校准评分表，让评分更准确，只有评分准确了，Actor 才能根据优势函数判断动作优劣。</p>
<h3 id="5-3-熵损失（Entropy-Loss）"><a href="#5-3-熵损失（Entropy-Loss）" class="headerlink" title="5.3 熵损失（Entropy Loss）"></a>5.3 熵损失（Entropy Loss）</h3><p>公式：$L_{entropy} &#x3D; -\sum_a \pi(a|s) \log \pi(a|s)$<br>作用：鼓励策略保持一定随机性，避免过早收敛到单一动作。</p>
<ul>
<li>如果策略过于确定（概率几乎集中在一个动作），探索能力下降</li>
<li>加入熵正则项，让策略保持多样性，提高探索能力</li>
</ul>
<p>熵损失就像给 Actor 一个“小小的冒险精神”，提醒它偶尔尝试新动作，不要只走老路。</p>
<h3 id="5-4-总体损失公式"><a href="#5-4-总体损失公式" class="headerlink" title="5.4 总体损失公式"></a>5.4 总体损失公式</h3><p>综合三项损失：$L &#x3D; L_{policy} + \alpha L_{value} + \beta L_{entropy}$</p>
<ul>
<li><strong>α</strong>：控制价值损失的重要性</li>
<li><strong>β</strong>：控制熵正则的重要性</li>
</ul>
<p>总损失就是三者的平衡，既保证策略学习正确，又让策略稳定且有探索性。</p>
<h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>本文主要讲了 Actor-Critic (AC) 框架及其高效实现——Actor + Critic (A3C) 算法。核心内容可概括为以下几点：</p>
<h3 id="6-1-​策略梯度与基线（Baseline）的引入​"><a href="#6-1-​策略梯度与基线（Baseline）的引入​" class="headerlink" title="6.1 ​策略梯度与基线（Baseline）的引入​"></a>6.1 ​策略梯度与基线（Baseline）的引入​</h3><p>策略梯度的核心思想是<strong>通过梯度上升，调整策略参数，增加高回报动作的概率，减少低回报动作的概率</strong>，以最大化期望累积奖励。<br>直接使用动作价值 $Q^\pi(s,a)$ 进行更新会导致<strong>高方差</strong>，训练不稳定。引入状态价值函数 $V^\pi(s)$ 作为基线（Baseline），将更新基准从“绝对奖励”变为“相对优势”（即 $A^\pi(s,a) &#x3D; Q^\pi(s,a) - V^\pi(s)$），显著降低了方差，使学习过程更平稳、高效。</p>
<h3 id="6-2-​Actor-Critic-架构的分工​"><a href="#6-2-​Actor-Critic-架构的分工​" class="headerlink" title="6.2 ​Actor-Critic 架构的分工​"></a>6.2 ​Actor-Critic 架构的分工​</h3><p>​<strong>Actor（行动者）​</strong>​：负责执行策略 $\pi_\theta(a|s)$，并根据优势函数的信号更新策略，决定“如何行动”。<br>​<strong>Critic（评论者）​</strong>​：负责评估状态价值 $V^\pi(s)$，提供基线，并计算优势函数，扮演“评价者”的角色。<br>两者协作，形成了“行动-评价-更新”的闭环，这是AC系列算法的基石。</p>
<h3 id="6-3-​优势函数（Advantage-Function）的核心作用​"><a href="#6-3-​优势函数（Advantage-Function）的核心作用​" class="headerlink" title="6.3 ​优势函数（Advantage Function）的核心作用​"></a>6.3 ​优势函数（Advantage Function）的核心作用​</h3><p>优势函数 $A^\pi(s,a)$ 量化了<strong>特定动作相对于该状态下平均水平的优劣程度</strong>。<br>再通过时序差分（TD）思想，其计算被近似为 $A^\pi(s,a) \approx r(s,a) + \gamma V^\pi(s’) - V^\pi(s)$。这意味着只需训练一个估计 $V(s)$ 的Critic网络，即可同时得到$Q$值和优势函数，大大简化了架构和训练难度。</p>
<h3 id="6-4-A3C-算法的异步并行架构与创新​"><a href="#6-4-A3C-算法的异步并行架构与创新​" class="headerlink" title="6.4 A3C 算法的异步并行架构与创新​"></a>6.4 A3C 算法的异步并行架构与创新​</h3><p>A3C 在AC基础上引入了<strong>多线程异步并行</strong>的框架。每个线程拥有一个<strong>本地网络</strong>与环境交互、采集数据、计算梯度。 一个<strong>全局网络</strong>作为中央大脑，异步接收并聚合所有线程的梯度进行更新，随后将更新后的参数同步给各线程。</p>
<p>这种架构带来了两大核心优势：</p>
<ol>
<li><strong>数据来源多样化</strong>，降低了样本相关性，使训练更稳定</li>
<li><strong>效率极大提升</strong>，通过并行探索加快了学习速度</li>
</ol>
<h3 id="6-5-A3C-的损失函数与稳定化技术​"><a href="#6-5-A3C-的损失函数与稳定化技术​" class="headerlink" title="6.5 A3C 的损失函数与稳定化技术​"></a>6.5 A3C 的损失函数与稳定化技术​</h3><p>A3C的优化目标是一个综合损失函数，包含三项：</p>
<ul>
<li>​<strong>策略损失 ($L_{policy}$)​</strong>​：引导Actor根据优势函数优化策略</li>
<li>​<strong>价值损失 ($L_{value}$)​</strong>​：训练Critic更准确地估计状态价值</li>
<li>​<strong>熵正则项 ($L_{entropy}$)​</strong>​：鼓励探索，防止策略过早收敛到局部最优，是保障算法稳定性和性能的关键技术</li>
</ul>
<p>总而言之，从策略梯度到AC，再到A3C，其演进脉络清晰：​<strong>为了更稳定、更高效地求解强化学习问题</strong>。A3C通过<strong>优势函数</strong>解决了AC的方差问题，通过<strong>异步并行</strong>框架解决了效率和探索问题，通过<strong>熵正则化</strong>等技巧确保了稳定性，成为深度强化学习发展史上一个里程碑式的高效算法。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/" title="A3C 算法原理与超级马里奥实践（上）">https://keychankc.github.io/2025/08/22/028-rl-ac-a3c-mario-case-1/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
              <a href="/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" rel="tag"># 策略梯度</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/14/027-rl-dqn-case/" rel="prev" title="DQN(Deep Q-Network)系列算法解析与实践">
                  <i class="fa fa-angle-left"></i> DQN(Deep Q-Network)系列算法解析与实践
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/08/29/029-rl-ac-a3c-mario-case-2/" rel="next" title="A3C 算法原理与超级马里奥实践（下）">
                  A3C 算法原理与超级马里奥实践（下） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">198k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/08/22/028-rl-ac-a3c-mario-case-1/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
