<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1.强化学习：从试错中学习策略我们先从一个轻松的生活片段切入：某天夜里，小明肚子咕咕叫，他想去找点吃的，但房间漆黑一片，他不敢开灯，只能凭借记忆和感知，一步一步摸索前进，一开始他撞到了桌角，又不小心踩到了猫，猫的尖叫声还吓了他一跳（负反馈），他又调整方向，继续摸索。他记住了这个方向有桌子不能走，那个方向可能有猫，不断的修正自己的路线，最终摸到了冰箱，找到了食物（正反馈）。这就是强化学习（Reinf">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习 — 试错、策略与长期奖励">
<meta property="og:url" content="https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1.强化学习：从试错中学习策略我们先从一个轻松的生活片段切入：某天夜里，小明肚子咕咕叫，他想去找点吃的，但房间漆黑一片，他不敢开灯，只能凭借记忆和感知，一步一步摸索前进，一开始他撞到了桌角，又不小心踩到了猫，猫的尖叫声还吓了他一跳（负反馈），他又调整方向，继续摸索。他记住了这个方向有桌子不能走，那个方向可能有猫，不断的修正自己的路线，最终摸到了冰箱，找到了食物（正反馈）。这就是强化学习（Reinf">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/image_20250714114119.png">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/image_20250714164216.png">
<meta property="article:published_time" content="2025-07-14T09:40:12.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.541Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="策略梯度">
<meta property="article:tag" content="非马尔可夫环境">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif">


<link rel="canonical" href="https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/","path":"2025/07/14/024-reinforcement-learning-start/","title":"强化学习 — 试错、策略与长期奖励"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>强化学习 — 试错、策略与长期奖励 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BB%8E%E8%AF%95%E9%94%99%E4%B8%AD%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5"><span class="nav-text">1.强化学习：从试错中学习策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%9B%E4%B8%AA%E5%85%B3%E9%94%AE%E7%89%B9%E5%BE%81"><span class="nav-text">1.强化学习的四个关键特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AE%A4%E7%9F%A5%E7%B1%BB%E6%AF%94"><span class="nav-text">2.强化学习的认知类比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%AD%A9%E5%AD%90%E7%9A%84%E6%88%90%E9%95%BF%E8%BF%87%E7%A8%8B-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84"><span class="nav-text">1. 孩子的成长过程 &#x3D; 强化学习的学习路径</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BC%BA%E8%B0%83%E2%80%9C%E5%85%88%E5%81%9A%EF%BC%8C%E5%86%8D%E8%AF%84%E4%BC%B0%E2%80%9D"><span class="nav-text">2. 强化学习强调“先做，再评估”</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%BB%8F%E5%85%B8%E7%A4%BA%E4%BE%8B%EF%BC%9A%E9%A3%9E%E8%88%B9%E7%9D%80%E9%99%86%E4%BB%BB%E5%8A%A1"><span class="nav-text">2.经典示例：飞船着陆任务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%88%9D%E5%A7%8B%E9%98%B6%E6%AE%B5%EF%BC%9A%E8%A1%8C%E4%B8%BA%E9%9A%8F%E6%9C%BA%E3%80%81%E7%9B%AE%E6%A0%87%E6%9C%AA%E7%9F%A5"><span class="nav-text">1. 初始阶段：行为随机、目标未知</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%AE%BE%E8%AE%A1%E5%A5%96%E5%8A%B1%E6%9C%BA%E5%88%B6%EF%BC%9A%E7%94%A8%E2%80%9C%E7%BB%93%E6%9E%9C%E2%80%9D%E5%BC%95%E5%AF%BC%E5%AD%A6%E4%B9%A0"><span class="nav-text">2. 设计奖励机制：用“结果”引导学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AD%A6%E4%B9%A0%E6%BC%94%E8%BF%9B%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-text">3. 学习演进过程可视化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%9B%91%E7%9D%A3%E3%80%81%E6%97%A0%E7%9B%91%E7%9D%A3%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">3.监督、无监督和强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">1.监督学习与深度学习的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%9F%BA%E6%9C%AC%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="nav-text">2.基本定义与学习方式对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E7%B1%BB%E6%AF%94%EF%BC%88%E7%B1%BB%E6%AF%94%E4%B8%BA%E5%AD%A6%E4%B9%A0%E5%9C%BA%E6%99%AF%EF%BC%89"><span class="nav-text">3.核心区别类比（类比为学习场景）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%B8%B8%E8%A7%81%E7%94%A8%E9%80%94%E4%B8%8E%E4%BB%A3%E8%A1%A8%E4%BB%BB%E5%8A%A1"><span class="nav-text">4.常见用途与代表任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%E4%B8%8E%E6%8C%91%E6%88%98%E5%B7%AE%E5%BC%82"><span class="nav-text">5.训练方式与挑战差异</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E8%81%94%E7%B3%BB%E4%B8%8E%E8%9E%8D%E5%90%88%E8%B6%8B%E5%8A%BF"><span class="nav-text">6.联系与融合趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%81%94%E7%B3%BB"><span class="nav-text">1.联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%9E%8D%E5%90%88%E8%B6%8B%E5%8A%BF"><span class="nav-text">2.融合趋势</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-text">4.强化学习的工作流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88"><span class="nav-text">1.强化学习整体流程概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E8%AF%A6%E7%BB%86%E5%88%86%E8%A7%A3"><span class="nav-text">2.强化学习工作流程详细分解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">1. 初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%BE%AA%E7%8E%AF%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92"><span class="nav-text">2. 循环进行交互</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E8%A7%82%E5%AF%9F%E7%8A%B6%E6%80%81"><span class="nav-text">1.观察状态</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E9%80%89%E6%8B%A9%E5%8A%A8%E4%BD%9C"><span class="nav-text">2.选择动作</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E7%AD%96%E7%95%A5-%CF%80-a-s-%EF%BC%9F"><span class="nav-text">1.什么是策略 π(a|s)？</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E5%B8%B8%E8%A7%81%E5%8A%A8%E4%BD%9C%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5"><span class="nav-text">2.常见动作选择策略</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E4%B8%89%E8%80%85%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="nav-text">3.三者对比总结</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%89%A7%E8%A1%8C%E5%8A%A8%E4%BD%9C%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BA%92"><span class="nav-text">3.执行动作与环境交互</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-%E5%AD%A6%E4%B9%A0-%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5"><span class="nav-text">4.学习&#x2F;更新策略</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E2%80%9C%E5%AD%A6%E4%B9%A0-%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5%E2%80%9D%EF%BC%9F"><span class="nav-text">1.什么是“学习&#x2F;更新策略”？</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F%E8%AF%A6%E8%A7%A3"><span class="nav-text">2.三种常见更新方式详解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%87%8D%E5%A4%8D%E6%AD%A5%E9%AA%A4-2"><span class="nav-text">3. 重复步骤 2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E6%9C%80%E7%BB%88%E8%BE%93%E5%87%BA"><span class="nav-text">4. 最终输出</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6"><span class="nav-text">5.强化学习中的几个关键组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%8E%AF%E5%A2%83%EF%BC%88Environment%EF%BC%89"><span class="nav-text">1.环境（Environment）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%8A%B6%E6%80%81%EF%BC%88State-s-%EF%BC%89"><span class="nav-text">2.状态（State, $s$）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%8A%A8%E4%BD%9C%EF%BC%88Action-a-%EF%BC%89"><span class="nav-text">3.动作（Action, $a$）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%A5%96%E5%8A%B1%EF%BC%88Reward-r-%EF%BC%89"><span class="nav-text">4.奖励（Reward, $r$）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%AD%96%E7%95%A5%EF%BC%88Policy-%CF%80%EF%BC%89"><span class="nav-text">5.策略（Policy, π）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="nav-text">1.实现方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-text">2.策略学习方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88Value-Function%EF%BC%89"><span class="nav-text">6. 价值函数（Value Function）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E6%A8%A1%E5%9E%8B%EF%BC%88Model%EF%BC%89"><span class="nav-text">7.模型（Model）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E4%BC%AA%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-text">6.工作流程伪代码示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%9A%E5%B0%8F%E7%99%BD%E9%BC%A0%E6%89%BE%E5%A5%B6%E9%85%AA"><span class="nav-text">7.举个例子：小白鼠找奶酪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%80%BB%E7%BB%93"><span class="nav-text">8.总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">122</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="强化学习 — 试错、策略与长期奖励 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习 — 试错、策略与长期奖励
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-14 17:40:12" itemprop="dateCreated datePublished" datetime="2025-07-14T17:40:12+08:00">2025-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/07/14/024-reinforcement-learning-start/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/07/14/024-reinforcement-learning-start/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-强化学习：从试错中学习策略"><a href="#1-强化学习：从试错中学习策略" class="headerlink" title="1.强化学习：从试错中学习策略"></a>1.强化学习：从试错中学习策略</h2><p>我们先从一个轻松的生活片段切入：某天夜里，小明肚子咕咕叫，他想去找点吃的，但房间漆黑一片，他不敢开灯，只能凭借记忆和感知，一步一步摸索前进，一开始他撞到了桌角，又不小心踩到了猫，猫的尖叫声还吓了他一跳（负反馈），他又调整方向，继续摸索。他记住了这个方向有桌子不能走，那个方向可能有猫，不断的修正自己的路线，最终摸到了冰箱，找到了食物（正反馈）。这就是强化学习（Reinforcement Learning, RL）核心思想的具象呈现：</p>
<blockquote>
<p>智能体在完全未知的环境中，靠“做出行为 → 接受反馈 → 调整策略”这一闭环，在不断试错中学习完成任务的最佳方式。</p>
</blockquote>
<span id="more"></span>
<p>强化学习与人的成长过程极为相似：从不懂规则的孩子，到逐步掌握做事规律的成年人，这个过程本质上就是一个持续与环境互动、在反馈中进步的过程。</p>
<h3 id="1-强化学习的四个关键特征"><a href="#1-强化学习的四个关键特征" class="headerlink" title="1.强化学习的四个关键特征"></a>1.强化学习的四个关键特征</h3><ol>
<li><strong>从“无知”到“掌握策略”</strong>：智能体起初对环境一无所知，只能通过反复尝试来摸索出完成任务</li>
<li><strong>依赖与环境的持续交互</strong>：强化学习不是“看图识字”或“记答案”，而是通过做出动作并观察环境反应来调整行为</li>
<li><strong>学习过程由试错驱动</strong>：每次尝试产生的反馈（奖励&#x2F;惩罚）构成了智能体积累经验、改进策略的基础</li>
<li><strong>学习目标导向明确，但过程探索自由</strong>：强化学习没有明确标签指引每一步行为是否正确，但整体方向是明确的：<strong>最大化长期奖励</strong></li>
</ol>
<h3 id="2-强化学习的认知类比"><a href="#2-强化学习的认知类比" class="headerlink" title="2.强化学习的认知类比"></a>2.强化学习的认知类比</h3><p>将强化学习比作孩子的成长过程，我们可以更直观地理解其学习机制。</p>
<h4 id="1-孩子的成长过程-强化学习的学习路径"><a href="#1-孩子的成长过程-强化学习的学习路径" class="headerlink" title="1. 孩子的成长过程 &#x3D; 强化学习的学习路径"></a>1. 孩子的成长过程 &#x3D; 强化学习的学习路径</h4><ul>
<li>起点：一无所知，只能试错</li>
<li>过程：与环境交互，接收反馈（如家长批评、奖励）</li>
<li>结果：总结经验、形成规则、学会做决策</li>
</ul>
<p>例如，因为考试没考好被家长批评，从中意识到“努力学习”能带来正向反馈，这种从“结果”中反推“行动价值”的学习方式，就类似强化学习。</p>
<h4 id="2-强化学习强调“先做，再评估”"><a href="#2-强化学习强调“先做，再评估”" class="headerlink" title="2. 强化学习强调“先做，再评估”"></a>2. 强化学习强调“先做，再评估”</h4><p>与监督学习中“先给出标签再预测”的范式不同，强化学习中的智能体<strong>必须先行动，才能知道结果好坏</strong>。这就导致：RL没有每步的明确标签（而是延迟奖励），就需要大量尝试来估计长期收益，这更接近真实世界中的学习方式。</p>
<h2 id="2-经典示例：飞船着陆任务"><a href="#2-经典示例：飞船着陆任务" class="headerlink" title="2.经典示例：飞船着陆任务"></a>2.经典示例：飞船着陆任务</h2><p>为了更清晰地展示强化学习的流程和目标，我们来看一个经典案例——<strong>让一艘小飞船安全着陆</strong>。<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif"></p>
<h3 id="1-初始阶段：行为随机、目标未知"><a href="#1-初始阶段：行为随机、目标未知" class="headerlink" title="1. 初始阶段：行为随机、目标未知"></a>1. 初始阶段：行为随机、目标未知</h3><p>飞船起初不知哪里适合降落，所有动作都是盲目的探索。</p>
<h3 id="2-设计奖励机制：用“结果”引导学习"><a href="#2-设计奖励机制：用“结果”引导学习" class="headerlink" title="2. 设计奖励机制：用“结果”引导学习"></a>2. 设计奖励机制：用“结果”引导学习</h3><p>我们为该任务设计如下奖励策略：</p>
<ul>
<li>成功着陆 → +100 分</li>
<li>偏离目标 → -50 分或 -100 分</li>
<li>每一步偏离方向 → -10 分</li>
<li>每一步朝目标靠近 → +5 或 +10 分</li>
</ul>
<p>这种既奖励结果，又鼓励过程表现的设计，有助于模型形成“通向成功的正确路径”，而不是仅仅“完成任务”即可。</p>
<h3 id="3-学习演进过程可视化"><a href="#3-学习演进过程可视化" class="headerlink" title="3. 学习演进过程可视化"></a>3. 学习演进过程可视化</h3><table>
<thead>
<tr>
<th><strong>阶段</strong></th>
<th><strong>行为特征</strong></th>
<th><strong>学习机制</strong></th>
</tr>
</thead>
<tbody><tr>
<td>初期探索</td>
<td>随机飞行，动作无规律</td>
<td>采集经验，构建基本认知</td>
</tr>
<tr>
<td>中期训练</td>
<td>初步形成策略，开始避开错误路径</td>
<td>估算每个状态下的动作价值</td>
</tr>
<tr>
<td>后期收敛</td>
<td>稳定实现目标，路径接近最优</td>
<td>策略优化完成，实现稳定任务完成</td>
</tr>
</tbody></table>
<blockquote>
<p>强化学习关注的是“长期回报”的最大化，而不是某一步骤的对错。</p>
</blockquote>
<h2 id="3-监督、无监督和强化学习"><a href="#3-监督、无监督和强化学习" class="headerlink" title="3.监督、无监督和强化学习"></a>3.监督、无监督和强化学习</h2><h3 id="1-监督学习与深度学习的关系"><a href="#1-监督学习与深度学习的关系" class="headerlink" title="1.监督学习与深度学习的关系"></a>1.监督学习与深度学习的关系</h3><p>深度学习是通过<strong>多层神经网络</strong>自动学习数据的层次化特征，而监督学习是通过<strong>已标注的数据</strong>​（输入-输出对）学习一个映射函数（模型），用于预测新数据的输出。深度学习是一种<strong>模型实现方式</strong>​（用深层神经网络），而监督学习是一个<strong>学习范式</strong>​（需要标注数据），两者属于不同维度。同样强化学习也是一种学习范式。</p>
<h3 id="2-基本定义与学习方式对比"><a href="#2-基本定义与学习方式对比" class="headerlink" title="2.基本定义与学习方式对比"></a>2.基本定义与学习方式对比</h3><table>
<thead>
<tr>
<th><strong>学习类型</strong></th>
<th><strong>定义</strong></th>
<th><strong>输入</strong></th>
<th><strong>输出</strong></th>
<th><strong>学习目标</strong></th>
<th><strong>是否需要标签</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>监督学习</strong></td>
<td>学习从输入到输出的映射关系</td>
<td>特征 + 正确标签</td>
<td>预测标签</td>
<td>拟合已知数据的输入输出映射，最小化误差</td>
<td>有标签</td>
</tr>
<tr>
<td><strong>无监督学习</strong></td>
<td>学习数据的内在结构或分布</td>
<td>仅有特征数据</td>
<td>聚类 &#x2F; 维度 &#x2F; 分布结构</td>
<td>发现数据的隐藏模式</td>
<td>无标签</td>
</tr>
<tr>
<td><strong>强化学习</strong></td>
<td>学习在环境中如何行动</td>
<td>状态（来自环境）</td>
<td>行为 &#x2F; 动作</td>
<td>最大化<strong>长期累积奖励</strong></td>
<td>无标签（但有奖励）</td>
</tr>
</tbody></table>
<h3 id="3-核心区别类比（类比为学习场景）"><a href="#3-核心区别类比（类比为学习场景）" class="headerlink" title="3.核心区别类比（类比为学习场景）"></a>3.核心区别类比（类比为学习场景）</h3><table>
<thead>
<tr>
<th><strong>学习类型</strong></th>
<th><strong>类比为人类学习方式</strong></th>
<th><strong>教学过程</strong></th>
<th><strong>是否立即得到反馈</strong></th>
<th><strong>反馈来源</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>监督学习</strong></td>
<td>老师讲题 + 提供标准答案</td>
<td>有“对错参考答案”</td>
<td>马上知道对错</td>
<td>明确标签</td>
</tr>
<tr>
<td><strong>无监督学习</strong></td>
<td>看书学习</td>
<td>没有参考答案</td>
<td>不知道对错</td>
<td>无标签，靠模式归纳</td>
</tr>
<tr>
<td><strong>强化学习</strong></td>
<td>玩游戏找通关方法</td>
<td>没人教你，只给你“通关奖励”</td>
<td>奖励可能延迟</td>
<td>环境反馈（奖励&#x2F;惩罚）</td>
</tr>
</tbody></table>
<h3 id="4-常见用途与代表任务"><a href="#4-常见用途与代表任务" class="headerlink" title="4.常见用途与代表任务"></a>4.常见用途与代表任务</h3><table>
<thead>
<tr>
<th><strong>学习类型</strong></th>
<th><strong>典型任务</strong></th>
<th><strong>常见应用</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>监督学习</strong></td>
<td>分类（图像识别）、回归（房价预测）</td>
<td>猫狗识别、人脸识别、情感分析、语音识别、自动驾驶感知模块</td>
</tr>
<tr>
<td><strong>无监督学习</strong></td>
<td>聚类（用户分群）、降维（PCA）、异常检测</td>
<td>客户画像、推荐系统、异常检测、文本建模</td>
</tr>
<tr>
<td><strong>强化学习</strong></td>
<td>决策规划、策略优化</td>
<td>游戏AI、自动驾驶控制、机器人路径规划、金融交易、智能推荐系统</td>
</tr>
</tbody></table>
<h3 id="5-训练方式与挑战差异"><a href="#5-训练方式与挑战差异" class="headerlink" title="5.训练方式与挑战差异"></a>5.训练方式与挑战差异</h3><table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>监督学习</strong></th>
<th><strong>无监督学习</strong></th>
<th><strong>强化学习</strong></th>
</tr>
</thead>
<tbody><tr>
<td>数据依赖</td>
<td>大量标注数据</td>
<td>原始数据即可</td>
<td>环境交互数据（+ 模拟器）</td>
</tr>
<tr>
<td>训练成本</td>
<td>标签标注贵</td>
<td>模型选择难</td>
<td>数据收集贵、训练不稳定</td>
</tr>
<tr>
<td>学习稳定性</td>
<td>高，能收敛</td>
<td>中，依赖算法</td>
<td>低，奖励稀疏难优化</td>
</tr>
<tr>
<td>收敛难度</td>
<td>相对较低</td>
<td>中等偏高</td>
<td>很高，尤其是在复杂环境中</td>
</tr>
<tr>
<td>反馈机制</td>
<td>有明确标签</td>
<td>无标签反馈</td>
<td>延迟奖励、靠累计经验</td>
</tr>
</tbody></table>
<h3 id="6-联系与融合趋势"><a href="#6-联系与融合趋势" class="headerlink" title="6.联系与融合趋势"></a>6.联系与融合趋势</h3><h4 id="1-联系"><a href="#1-联系" class="headerlink" title="1.联系"></a>1.联系</h4><p>三者都是“从数据中自动学习”，目标是形成<strong>泛化能力</strong>，都可以使用深度神经网络作为模型，也都可归类为<strong>机器学习范式</strong>的一种。</p>
<h4 id="2-融合趋势"><a href="#2-融合趋势" class="headerlink" title="2.融合趋势"></a>2.融合趋势</h4><ul>
<li><strong>自监督学习（Self-Supervised Learning）</strong>：介于无监督与监督之间，用数据本身生成伪标签，极大提升了大模型的训练效率（如 GPT、BERT）</li>
<li><strong>强化学习 + 监督学习</strong>：在策略学习中结合人类演示（如 AlphaGo 用专家棋谱进行初始监督学习）</li>
<li><strong>无监督预训练 + 强化学习微调</strong>：用于让智能体“先感知世界”，再进行试错学习（如视觉感知 + 决策控制）</li>
</ul>
<p>总结一下，监督学习是知道答案，目标是“学会模仿”；无监督学习是不知道答案，目标是“发现结构”；强化学习是不知道答案也没人告诉你，但你知道“通关有奖”，目标是“学会做决策”。</p>
<h2 id="4-强化学习的工作流程"><a href="#4-强化学习的工作流程" class="headerlink" title="4.强化学习的工作流程"></a>4.强化学习的工作流程</h2><h3 id="1-强化学习整体流程概览"><a href="#1-强化学习整体流程概览" class="headerlink" title="1.强化学习整体流程概览"></a>1.强化学习整体流程概览</h3><p>我们可以用一句话概括强化学习的基本框架：</p>
<blockquote>
<p><strong>Agent 在环境中观察状态，选择动作，接收奖励与下一个状态，通过不断试错学习最优策略。</strong></p>
</blockquote>
<p>其流程如下图所示：<br><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/image_20250714114119.png"></p>
<h3 id="2-强化学习工作流程详细分解"><a href="#2-强化学习工作流程详细分解" class="headerlink" title="2.强化学习工作流程详细分解"></a>2.强化学习工作流程详细分解</h3><p>下面我们用一个标准的迭代过程来说明强化学习的完整循环过程。</p>
<h4 id="1-初始化"><a href="#1-初始化" class="headerlink" title="1. 初始化"></a>1. 初始化</h4><p>初始化智能体的策略、值函数或模型（取决于使用哪种算法），以及初始化环境，设置起始状态。</p>
<h4 id="2-循环进行交互"><a href="#2-循环进行交互" class="headerlink" title="2. 循环进行交互"></a>2. 循环进行交互</h4><h5 id="1-观察状态"><a href="#1-观察状态" class="headerlink" title="1.观察状态"></a>1.观察状态</h5><p>$s_t$: 智能体从环境中接收当前状态 $s_t$（例如游戏画面、机器人传感器读数等）。</p>
<h5 id="2-选择动作"><a href="#2-选择动作" class="headerlink" title="2.选择动作"></a>2.选择动作</h5><p>选择动作 $a_t$ 是强化学习中<strong>核心环节之一</strong>，它决定了智能体如何在当前状态 $s_t$ 下进行决策。这个决策过程由“策略（Policy）”控制，而策略可以是显式的，也可以是由神经网络隐式建模的。</p>
<h6 id="1-什么是策略-π-a-s-？"><a href="#1-什么是策略-π-a-s-？" class="headerlink" title="1.什么是策略 π(a|s)？"></a>1.什么是策略 π(a|s)？</h6><p>策略是一个函数，描述了在给定状态 $s$ 下，采取动作 $a$ 的概率：</p>
<ul>
<li><strong>确定性策略</strong>：总是选择固定动作，例如：$π(s) &#x3D; argmax Q(s, a)$</li>
<li><strong>随机性策略</strong>：为每个动作分配一个概率，例如：$π(a|s) &#x3D; 0.7$ 走左，0.3 走右</li>
</ul>
<p>策略的设计决定了<strong>智能体的“性格”</strong>：是喜欢冒险（探索），还是喜欢安稳地走老路（利用）。</p>
<h6 id="2-常见动作选择策略"><a href="#2-常见动作选择策略" class="headerlink" title="2.常见动作选择策略"></a>2.常见动作选择策略</h6><ol>
<li>ε-greedy 策略（最常见）：大部分时候选最优动作，但偶尔随机探索。</li>
</ol>
<table>
<thead>
<tr>
<th><strong>步骤</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ε 的概率</td>
<td><strong>随机选一个动作</strong>（探索 Explore）</td>
</tr>
<tr>
<td>1 - ε 的概率</td>
<td><strong>选当前最优动作</strong>，如：$argmax Q(s, a)$</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if random() &lt; ε:</span><br><span class="line">    action = random.choice(all_actions)</span><br><span class="line">else:</span><br><span class="line">    action = argmax(Q[s])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：简单易实现，能保证探索</li>
<li><strong>缺点</strong>：探索是“盲目”的，随机选动作可能不合理</li>
<li><strong>适用场景</strong>：Q-learning、DQN 等值函数方法</li>
</ul>
<blockquote>
<p>类比：大部分时候你点最爱吃的外卖（炸鸡），但偶尔点点新菜试试（探索）</p>
</blockquote>
<ol start="2">
<li>Softmax 策略（带温度的概率选择）：将每个动作的“值”转换为概率，值越大概率越高。<br>公式如下：$P(a_i) &#x3D; \frac{\exp(Q(s, a_i)&#x2F;\tau)}{\sum_j \exp(Q(s, a_j)&#x2F;\tau)}$</li>
</ol>
<ul>
<li>$\tau$：温度参数，控制探索程度：<ul>
<li><strong>高温度</strong>（如 τ&#x3D;10）：接近均匀分布 → 更随机</li>
<li><strong>低温度</strong>（如 τ&#x3D;0.01）：接近贪婪策略 → 近似 argmax</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">probs = softmax(Q[s] / temperature)</span><br><span class="line">action = np.random.choice(actions, p=probs)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：比 ε-greedy 更“智能”的探索</li>
<li><strong>缺点</strong>：参数敏感，调不好容易崩</li>
<li><strong>适用场景</strong>：策略梯度、Actor-Critic 也常用类似思想</li>
</ul>
<blockquote>
<p>类比：你喜欢炸鸡 90 分，烤肉 85 分，素食 70 分，softmax 会让你更偏向点炸鸡，但也可能点烤肉（而不是盲选素食）</p>
</blockquote>
<ol start="3">
<li>神经网络输出动作概率（策略网络&#x2F;Actor）<br>这种方式下，策略 $π(a|s)$ 由一个<strong>神经网络建模</strong>，直接输出每个动作的概率分布：</li>
</ol>
<ul>
<li>输入：状态 $s$</li>
<li>输出：动作的概率分布 $\pi(a|s; \theta)$</li>
<li>采样：根据这个概率进行抽样动作</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">probs = actor_network(state)  # 输出 [0.7, 0.2, 0.1]</span><br><span class="line">action = sample_from_distribution(probs)</span><br></pre></td></tr></table></figure>
<p>适用于策略梯度方法，如：    </p>
<ul>
<li><strong>REINFORCE</strong></li>
<li><strong>Actor-Critic</strong></li>
<li><strong>PPO（Proximal Policy Optimization）</strong></li>
</ul>
<p>优势：</p>
<ul>
<li>可以学习复杂策略（非线性映射）</li>
<li>输出连续动作时更自然（输出均值&#x2F;方差）</li>
<li>和环境反馈协同训练，端到端</li>
</ul>
<h6 id="3-三者对比总结"><a href="#3-三者对比总结" class="headerlink" title="3.三者对比总结"></a>3.三者对比总结</h6><table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>特点</strong></th>
<th><strong>适用算法</strong></th>
<th><strong>探索方式</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ε-greedy</td>
<td>简单、高效</td>
<td>Q-learning、DQN</td>
<td>ε 概率随机</td>
</tr>
<tr>
<td>Softmax</td>
<td>更温和的探索方式</td>
<td>policy gradient、Dyna-Q</td>
<td>概率分布</td>
</tr>
<tr>
<td>策略网络输出</td>
<td>强表达能力，支持连续动作</td>
<td>Actor-Critic、PPO</td>
<td>神经网络建模的分布</td>
</tr>
</tbody></table>
<blockquote>
<p>动作选择策略的设计，是强化学习探索-利用权衡的核心。你是“保守派”，还是“冒险派”，看 π(a|s) 如何设定。</p>
</blockquote>
<h5 id="3-执行动作与环境交互"><a href="#3-执行动作与环境交互" class="headerlink" title="3.执行动作与环境交互"></a>3.执行动作与环境交互</h5><p>这一步就是智能体将选择好的动作 $a_t$ 真正“付诸实践”，环境也会据此作出“回应”——这正是<strong>强化学习“试错”的核心舞台</strong>。</p>
<p><strong>这一步的目的是什么?</strong><br>在时间步 $t$，智能体选定动作 $a_t$，然后将其作用于环境 $\mathcal{E}$，环境根据自己的内部状态与规则，给予智能体一个反馈：<strong>奖励</strong> $r_t$，同时更新自身并输出下一个状态：<strong>状态</strong> $s_{t+1}$。</p>
<p>用一句话概括就是：<strong>“你干了件事，环境告诉你这事干得好不好，并给你一个新的局面。”</strong></p>
<p><strong>环境做了什么事情？</strong><br>这个过程通常通过一个函数来建模：$(s_{t+1}, r_t) &#x3D; \mathcal{E}(s_t, a_t)$，也可以是概率形式：$P(s_{t+1}, r_t | s_t, a_t)$<br>说明：</p>
<ul>
<li>环境是 <strong>马尔可夫决策过程（MDP）</strong> 的一部分。</li>
<li>它根据当前状态 $s_t$ 和动作 $a_t$，<strong>决定下一个状态和奖励</strong>。</li>
<li>可以是<strong>确定的</strong>（固定结果）或<strong>随机的</strong>（环境有内在随机性）</li>
</ul>
<p><strong>奖励 $r_t$ 是什么？</strong></p>
<ol>
<li>奖励是强化学习的核心信号：它衡量动作的“好坏”，是学习的唯一指导。<ul>
<li><strong>正奖励（+）</strong>：鼓励行为，如机器人成功避开障碍</li>
<li><strong>负奖励（−）</strong>：惩罚行为，如撞墙、超时</li>
<li><strong>零奖励</strong>：无显著反馈，例如在迷宫中没走错也没走对</li>
</ul>
</li>
<li>奖励的设计极其重要（Reward Design）因为设计太“稀疏”：智能体很难学到，而奖励太“贪婪”：可能导致过拟合策略或作弊</li>
</ol>
<blockquote>
<p>类比：你玩游戏，每赢一局给你 100 分（正奖励），被打败扣你 50 分（负奖励），这就是你的激励机制。</p>
</blockquote>
<p><strong>状态 $s_{t+1}$是什么？</strong></p>
<ol>
<li>状态是描述“环境当前状况”的向量或图像，状态可以非常多样，具体取决于任务类型：</li>
</ol>
<table>
<thead>
<tr>
<th><strong>场景</strong></th>
<th><strong>状态例子</strong></th>
</tr>
</thead>
<tbody><tr>
<td>迷宫游戏</td>
<td>老鼠当前坐标</td>
</tr>
<tr>
<td>Atari 游戏</td>
<td>当前帧图像</td>
</tr>
<tr>
<td>棋类游戏</td>
<td>棋盘布局</td>
</tr>
<tr>
<td>自动驾驶</td>
<td>当前车速、路况、传感器信息</td>
</tr>
<tr>
<td>股票交易</td>
<td>当前价格、趋势图、技术指标</td>
</tr>
</tbody></table>
<ol start="2">
<li>状态更新反映环境的变化，动作导致环境进入新状态，智能体在下一步就要基于 $s_{t+1}$ 再次做决策，进入下一个循环。</li>
</ol>
<p><strong>具体例子演示</strong><br>游戏场景：Flappy Bird:</p>
<ol>
<li>当前状态 $s_t$：小鸟的位置、速度、管道距离</li>
<li>智能体选择动作 $a_t$：是否跳跃</li>
<li>执行动作，环境反馈：<ul>
<li>如果跳起来避过障碍，$r_t$ &#x3D; +1，新的 $s_{t+1}$ 是更新后的小鸟状态</li>
<li>如果撞墙了，$r_t &#x3D; -10$，游戏结束，done&#x3D;True</li>
</ul>
</li>
</ol>
<p><strong>这一步与训练的关系？</strong><br>这一步提供了强化学习中“经验回放”（Experience Replay）或“轨迹采样”的原始数据：<br>每一步采样一个四元组（transition）：$(s_t, a_t, r_t, s_{t+1})$<br>这些数据将存入经验池（replay buffer），供后续学习算法（如 Q-learning, DDPG, PPO）进行优化。</p>
<p><strong>常见坑：环境交互设计不合理会导致学习失败</strong></p>
<ul>
<li>奖励太稀疏：比如只在任务完成时才给分，导致训练难以进行</li>
<li>状态缺失信息：智能体感知不到重要变量</li>
<li>非马尔可夫环境：当前状态不包含未来决策所需信息（必须堆叠历史）</li>
</ul>
<blockquote>
<p>马尔可夫环境（Markov Environment）是指环境的状态转移和奖励生成过程满足马尔可夫性质（Markov Property）​，即：  <strong>​“未来只依赖于当前状态和动作，而与过去的历史无关。”​</strong>​</p>
</blockquote>
<p><strong>总结：执行动作与环境交互是 RL 的关键闭环</strong></p>
<table>
<thead>
<tr>
<th><strong>环节</strong></th>
<th><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td>执行动作 $a_t$</td>
<td>智能体“尝试”做决策</td>
</tr>
<tr>
<td>环境反馈 $r_t$</td>
<td>给智能体打分，引导方向</td>
</tr>
<tr>
<td>环境转移 $s_{t+1}$</td>
<td>带来新的局面，进入下一轮学习</td>
</tr>
</tbody></table>
<blockquote>
<p>这一步就像<strong>打怪升级的回合制游戏</strong>：你出招，世界反应，你总结经验，再迎下一轮，如此循环往复直至结束。</p>
</blockquote>
<h5 id="4-学习-更新策略"><a href="#4-学习-更新策略" class="headerlink" title="4.学习&#x2F;更新策略"></a>4.学习&#x2F;更新策略</h5><p>前面几步（观察状态、选择动作、执行动作、获取奖励）都是<strong>数据采集</strong>，而<strong>这一步是智能体真正“变聪明”的过程</strong>。它的目标是——让智能体不断改进策略 $π$，使得它未来能做出更好的动作，获得更多奖励。</p>
<h6 id="1-什么是“学习-更新策略”？"><a href="#1-什么是“学习-更新策略”？" class="headerlink" title="1.什么是“学习&#x2F;更新策略”？"></a>1.什么是“学习&#x2F;更新策略”？</h6><p>在强化学习中，智能体通过一系列经验：$(s_t, a_t, r_t, s_{t+1})$，来调整自己的内部结构（如 Q 表或神经网络权重），从而实现<strong>更准确地评估“哪个动作更好”</strong> 以及<strong>更有效地选择策略动作 π(a|s)</strong>。<br>根据不同的算法，学习目标略有不同，但都遵循一个核心理念： <strong>最大化未来累计奖励（即最大化预期回报）</strong></p>
<h6 id="2-三种常见更新方式详解"><a href="#2-三种常见更新方式详解" class="headerlink" title="2.三种常见更新方式详解"></a>2.三种常见更新方式详解</h6><ol>
<li><p>Q-Learning：基于表格的值函数方法（离策略），适用于<strong>小型离散状态空间的任务（如迷宫、简单棋盘游戏）</strong>，思路是：</p>
<ul>
<li>维护一个 Q 表，记录每个状态-动作对的预期累计奖励：<br> $Q(s, a) \approx \text{当前状态下采取该动作所获得的长期回报}$</li>
<li>通过贝尔曼方程进行更新：<br>$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \cdot \left[r_t + \gamma \cdot \max_{a’} Q(s_{t+1}, a’) - Q(s_t, a_t) \right]$$<br>  其中：<ul>
<li>$\alpha$：学习率</li>
<li>$\gamma$：折扣因子，衡量未来奖励的价值</li>
<li>$r_t$：当前奖励</li>
<li>$\max Q(s_{t+1}, a’)$：估计从下一状态起最好的动作的价值</li>
</ul>
</li>
</ul>
<p> 示例代码：<br> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q[s][a] += lr * (r + gamma * max(Q[s_next]) - Q[s][a])</span><br></pre></td></tr></table></figure><br> 这种方式的优点，简单直观、好实现。缺点是不适合连续或大状态空间，Q 表会爆炸。</p>
</li>
<li><p>DQN（Deep Q-Network）：用神经网络逼近 Q 函数，适用于<strong>状态空间较大（如图像输入）、动作空间离散的任务（如 Atari 游戏）</strong>。思路是：</p>
<ul>
<li>不再用 Q 表，而用神经网络 $Q_\theta(s, a)$ 来近似 Q 函数</li>
<li>通过一批样本 ($s_t, a_t, r_t, s_{t+1}$) 构造损失函数：<br>$$\mathcal{L}(\theta) &#x3D; \left[r_t + \gamma \cdot \max_{a’} Q_{\theta^-}(s_{t+1}, a’) - Q_{\theta}(s_t, a_t)\right]^2$$<ul>
<li>通过<strong>梯度下降</strong>更新神经网络参数 $\theta$</li>
</ul>
</li>
</ul>
<p> 关键技术：</p>
<ul>
<li><strong>经验回放（Experience Replay）</strong>：打乱数据相关性，提升稳定性</li>
<li><strong>目标网络（Target Network）</strong>：使用固定参数的 Q_target 防止震荡<br> 这种方式的优点是可处理高维输入（如图像），强大，可适用于大型游戏，缺点是动作空间必须是离散的，学习不稳定，需精心设计</li>
</ul>
</li>
<li><p>PPO（Proximal Policy Optimization）：策略梯度方法（Actor-Critic）。适用于：连续动作空间、高维策略控制（如机器人控制、自然语言）。思路是：</p>
<ul>
<li>策略 $π(a|s; θ)$ 由一个神经网络建模，直接优化策略函数</li>
<li>使用优势函数（Advantage）来评估当前动作好坏：<br> $$A(s_t, a_t) &#x3D; Q(s_t, a_t) - V(s_t)$$</li>
<li>PPO 的目标函数如下（简化版）：<br>$$\mathcal{L}^{\text{PPO}} &#x3D; \mathbb{E}_t\left[\min\left(r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \cdot A_t\right)\right]$$<br>  其中：<ul>
<li>$r_t(\theta) &#x3D; \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$</li>
<li>clip 是核心创新，限制更新幅度，防止策略崩坏<br> 这种方式的优点是非常稳定，适合大规模训练，支持连续动作、复用采样数据，但缺点是实现较复杂，需并行训练。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>三者总结对比：</p>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>特点</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
<th><strong>适用任务</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Q-Learning</td>
<td>表格法</td>
<td>简单易懂</td>
<td>不适用于大状态空间</td>
<td>小型离散问题</td>
</tr>
<tr>
<td>DQN</td>
<td>深度值函数</td>
<td>处理图像等高维输入</td>
<td>动作必须离散、训练不稳定</td>
<td>Atari 游戏、图像输入任务</td>
</tr>
<tr>
<td>PPO</td>
<td>策略优化</td>
<td>稳定强大、支持连续动作</td>
<td>实现复杂、计算重</td>
<td>连续控制、机器人、NLP</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>策略更新 &#x3D; 智能体看了过去的表现（经验），然后调整自己未来的决策方式，让自己下次更聪明。</strong></p>
</blockquote>
<h4 id="3-重复步骤-2"><a href="#3-重复步骤-2" class="headerlink" title="3. 重复步骤 2"></a>3. 重复步骤 2</h4><p>重复上述过程多个回合（episode），直到收敛、训练结束或达到一定评估标准。</p>
<h4 id="4-最终输出"><a href="#4-最终输出" class="headerlink" title="4. 最终输出"></a>4. 最终输出</h4><p>得到一个训练好的策略 π*，可用于测试或部署。在实际任务中，策略可直接控制机器人、自动交易、游戏角色等。</p>
<h2 id="5-强化学习中的几个关键组件"><a href="#5-强化学习中的几个关键组件" class="headerlink" title="5.强化学习中的几个关键组件"></a>5.强化学习中的几个关键组件</h2><h3 id="1-环境（Environment）"><a href="#1-环境（Environment）" class="headerlink" title="1.环境（Environment）"></a>1.环境（Environment）</h3><p><strong>环境</strong>是智能体所“生活”的世界，它定义了任务规则、物理约束、奖励机制等。</p>
<h3 id="2-状态（State-s-）"><a href="#2-状态（State-s-）" class="headerlink" title="2.状态（State, $s$）"></a>2.状态（State, $s$）</h3><p><strong>状态</strong>是环境当前的描述，是智能体决策所依据的信息输入。状态要尽可能<strong>完整地描述当前局势</strong>，例如：</p>
<ul>
<li>图像（像素）：用于游戏或视觉任务</li>
<li>数值向量：机器人的位置、速度、传感器数值</li>
<li>离散符号：棋盘位置、标记标签等</li>
</ul>
<h3 id="3-动作（Action-a-）"><a href="#3-动作（Action-a-）" class="headerlink" title="3.动作（Action, $a$）"></a>3.动作（Action, $a$）</h3><p>智能体可采取的操作（如移动、跳跃）。</p>
<table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>举例</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td>离散动作</td>
<td>左、右、跳</td>
<td>用整数 ID 表示</td>
</tr>
<tr>
<td>连续动作</td>
<td>转向角、推进力</td>
<td>用向量表示，如 a &#x3D; [-0.2, 0.9]</td>
</tr>
</tbody></table>
<p>离散动作空间，用 softmax 输出概率分布。连续动作空间，用神经网络输出动作均值 + 方差，采样得到动作。</p>
<blockquote>
<p>类比：人在迷宫里选择“往东走”就是一个动作；机器人调整关节角度也是动作。</p>
</blockquote>
<h3 id="4-奖励（Reward-r-）"><a href="#4-奖励（Reward-r-）" class="headerlink" title="4.奖励（Reward, $r$）"></a>4.奖励（Reward, $r$）</h3><p><strong>奖励</strong>是环境给智能体的反馈信号，用于指示动作好坏，是强化学习学习的<strong>唯一目标信号</strong>。<br>奖励形式一般是<strong>标量数值</strong>（正数奖励，负数惩罚），通常只代表<strong>即时收益</strong>，不包含长期利益。<br>奖励设计陷阱如果太稀疏：模型学不到信息（例如：只有完成才得分），如果太容易作弊：智能体可能“投机取巧”来最大化奖励。</p>
<h3 id="5-策略（Policy-π）"><a href="#5-策略（Policy-π）" class="headerlink" title="5.策略（Policy, π）"></a>5.策略（Policy, π）</h3><p><strong>策略</strong>是智能体在给定状态下采取动作的方式，是学习的主要目标。</p>
<h4 id="1-实现方式"><a href="#1-实现方式" class="headerlink" title="1.实现方式"></a>1.实现方式</h4><ul>
<li>离散动作策略：神经网络输出 softmax 概率分布 </li>
<li>连续动作策略：输出均值 + 标准差，构建高斯分布</li>
</ul>
<h4 id="2-策略学习方法"><a href="#2-策略学习方法" class="headerlink" title="2.策略学习方法"></a>2.策略学习方法</h4><ul>
<li><strong>基于值函数</strong>：例如 ε-greedy from Q(s, a)</li>
<li><strong>直接优化策略</strong>：例如 REINFORCE、PPO</li>
</ul>
<h3 id="6-价值函数（Value-Function）"><a href="#6-价值函数（Value-Function）" class="headerlink" title="6. 价值函数（Value Function）"></a>6. 价值函数（Value Function）</h3><p><strong>价值函数</strong>衡量在某个状态（或状态-动作）下，未来可能获得的<strong>累计奖励的期望值</strong>。</p>
<h3 id="7-模型（Model）"><a href="#7-模型（Model）" class="headerlink" title="7.模型（Model）"></a>7.模型（Model）</h3><p><strong>模型</strong>是对环境动态的预测器，即预测状态转移和奖励函数。</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>定义与作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>环境 Environment</strong></td>
<td>任务运行的平台，提供状态与奖励，响应智能体行为</td>
</tr>
<tr>
<td><strong>状态 State</strong> $s$</td>
<td>当前环境的观测信息，是智能体做出决策的输入</td>
</tr>
<tr>
<td><strong>动作 Action</strong> $a$</td>
<td>智能体对环境的操作或决策，控制环境转移</td>
</tr>
<tr>
<td><strong>奖励 Reward</strong> $r$</td>
<td>表示行为好坏的反馈信号，引导学习目标</td>
</tr>
<tr>
<td><strong>策略 Policy</strong> $\pi$</td>
<td>从状态映射到动作，是智能体的“行为方式”</td>
</tr>
<tr>
<td><strong>价值函数 Value</strong></td>
<td>衡量状态或状态-动作组合的“长期价值”</td>
</tr>
<tr>
<td><strong>模型 Model</strong></td>
<td>预测环境未来的变化和反馈，用于模拟和规划</td>
</tr>
</tbody></table>
<p>强化学习就像是一个自主游戏的循环系统：<strong>环境提供舞台，奖励设定目标，状态记录局势，动作是决策，策略指引方向，价值函数评估局势，模型预测未来</strong>。它们构成了整个学习与决策的大闭环。</p>
<h2 id="6-工作流程伪代码示例"><a href="#6-工作流程伪代码示例" class="headerlink" title="6.工作流程伪代码示例"></a>6.工作流程伪代码示例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">for episode in range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    done = False</span><br><span class="line"></span><br><span class="line">    while not done:</span><br><span class="line">        # 1. 根据策略选择动作</span><br><span class="line">        action = agent.select_action(state)</span><br><span class="line"></span><br><span class="line">        # 2. 执行动作，获得奖励和下一个状态</span><br><span class="line">        next_state, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        # 3. 存储经验并学习</span><br><span class="line">        agent.learn(state, action, reward, next_state, done)</span><br><span class="line"></span><br><span class="line">        # 4. 状态更新</span><br><span class="line">        state = next_state</span><br></pre></td></tr></table></figure>

<h2 id="7-举个例子：小白鼠找奶酪"><a href="#7-举个例子：小白鼠找奶酪" class="headerlink" title="7.举个例子：小白鼠找奶酪"></a>7.举个例子：小白鼠找奶酪</h2><div style="text-align:center">
<img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/image_20250714164216.png" width="300" />
</div>

<p>结合上面的几个关键组件举个例子，假设你是一只小老鼠，在一个迷宫中找奶酪：</p>
<ol>
<li>你看到前面有几个岔路口（状态）</li>
<li>你决定往左还是往右走（动作）</li>
<li>走了一段，你没找到奶酪，还被电了一下（奖励为负）</li>
<li>你记住这个教训，下次不再走这条路（学习策略）</li>
<li>多次尝试后，你发现一条路径总能吃到奶酪，于是你记住了（策略收敛）</li>
</ol>
<h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8.总结"></a>8.总结</h2><p>强化学习（RL）是让智能体通过试错与环境交互来学习最优策略的机器学习方法。其核心流程为：智能体观察环境状态→选择动作→获得奖励→更新策略，循环迭代直至收敛。<br>关键组件包括环境、状态、动作、奖励和策略。与监督学习不同，RL没有标注数据，仅依赖环境反馈的奖励信号。<br>典型算法分三类：基于价值（如Q-Learning）、基于策略（如PPO）和两者结合的Actor-Critic。<br>RL适用于游戏AI、机器人控制等序贯决策任务，但面临奖励稀疏、训练不稳定等挑战。而深度强化学习（DRL）通过神经网络处理高维状态，进一步扩展了RL的应用边界。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/" title="强化学习 — 试错、策略与长期奖励">https://www.keychan.xyz/2025/07/14/024-reinforcement-learning-start/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" rel="tag"># 策略梯度</a>
              <a href="/tags/%E9%9D%9E%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E7%8E%AF%E5%A2%83/" rel="tag"># 非马尔可夫环境</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/08/023-segmentation-mask2former/" rel="prev" title="从像素到区域：MaskFormer 系列详解">
                  <i class="fa fa-angle-left"></i> 从像素到区域：MaskFormer 系列详解
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/22/025-reinforcement-learning-ppo/" rel="next" title="强化学习 — PPO策略优化算法">
                  强化学习 — PPO策略优化算法 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">458k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/07/14/024-reinforcement-learning-start/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
