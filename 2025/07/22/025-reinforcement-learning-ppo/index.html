<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-512x512.png" color="#222">
  <meta name="google-site-verification" content="jZ7dJJlouQrswxytAryX3LanLNrTthfFdMUkDJzRqIU">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.keychan.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1.PPO 算法概述1.PPO 的提出背景我们还是以智能体如何控制飞船落地的小游戏为例，智能体的目标是通过一系列操作（如向左移动或向右移动）实现平稳着陆。在训练初期，智能体并不知道应该如何操作，它需要通过反复的试探操作，从环境中不断获得反馈并调整策略，最终掌握一套“高奖励”操作方式。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习 — PPO策略优化算法">
<meta property="og:url" content="https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/index.html">
<meta property="og:site_name" content="KeyChan&#39;s blog">
<meta property="og:description" content="1.PPO 算法概述1.PPO 的提出背景我们还是以智能体如何控制飞船落地的小游戏为例，智能体的目标是通过一系列操作（如向左移动或向右移动）实现平稳着陆。在训练初期，智能体并不知道应该如何操作，它需要通过反复的试探操作，从环境中不断获得反馈并调整策略，最终掌握一套“高奖励”操作方式。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif">
<meta property="article:published_time" content="2025-07-22T03:40:12.000Z">
<meta property="article:modified_time" content="2025-09-21T12:04:48.541Z">
<meta property="article:author" content="KeyChan">
<meta property="article:tag" content="策略优化">
<meta property="article:tag" content="PPO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif">


<link rel="canonical" href="https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/","path":"2025/07/22/025-reinforcement-learning-ppo/","title":"强化学习 — PPO策略优化算法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>强化学习 — PPO策略优化算法 | KeyChan's blog</title>
  







<link rel="dns-prefetch" href="https://comment.mengyajia.com">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="KeyChan's blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">KeyChan's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-系列"><a href="/series/" rel="section"><i class="fa fa-list-ol fa-fw"></i>系列</a></li><li class="menu-item menu-item-随想"><a href="/think/" rel="section"><i class="fa fa-lightbulb fa-fw"></i>随想</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-PPO-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="nav-text">1.PPO 算法概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-PPO-%E7%9A%84%E6%8F%90%E5%87%BA%E8%83%8C%E6%99%AF"><span class="nav-text">1.PPO 的提出背景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="nav-text">1.策略优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%B8%B8%E8%A7%81%E7%9A%84%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-text">2.常见的策略优化方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="nav-text">2.强化学习中的基本要素</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%8E%9F%E5%A7%8B%E4%BA%94%E8%A6%81%E7%B4%A0"><span class="nav-text">1.原始五要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%AE%97%E6%B3%95%E5%A2%9E%E5%BC%BA%E8%A6%81%E7%B4%A0"><span class="nav-text">2.算法增强要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="nav-text">3.交互流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-PPO-%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BB%BB%E5%8A%A1%E4%B8%8E%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5"><span class="nav-text">3.PPO 的核心任务与设计理念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E4%BB%BB%E5%8A%A1"><span class="nav-text">1.核心任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5"><span class="nav-text">2.设计理念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E7%A8%B3%E5%AE%9A%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%EF%BC%9A%E4%B8%8D%E8%A6%81%E8%B5%B0%E5%A4%AA%E8%BF%9C"><span class="nav-text">1.稳定策略更新：不要走太远</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%AE%BE%E8%AE%A1%E5%89%AA%E5%88%87%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%EF%BC%9A%E8%BD%AF%E7%BA%A6%E6%9D%9F%E4%BB%A3%E6%9B%BF%E7%A1%AC%E7%BA%A6%E6%9D%9F"><span class="nav-text">2.设计剪切目标函数：软约束代替硬约束</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E7%AE%80%E6%B4%81%E5%AE%9E%E7%94%A8%EF%BC%9A%E9%80%82%E9%85%8D%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F"><span class="nav-text">3.简洁实用：适配现代深度学习系统</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-PPO-%E7%9A%84%E6%95%B4%E4%BD%93%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-text">4.PPO 的整体目标函数</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-PPO-%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BA%92"><span class="nav-text">2.PPO 策略网络与环境交互</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%8A%B6%E6%80%81%E3%80%81%E5%8A%A8%E4%BD%9C%E4%B8%8E%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E5%BB%BA%E6%A8%A1"><span class="nav-text">1.状态、动作与策略网络建模</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4%EF%BC%88State-Space%EF%BC%89"><span class="nav-text">1.状态空间（State Space）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4%EF%BC%88Action-Space%EF%BC%89"><span class="nav-text">2.动作空间（Action Space）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E7%A6%BB%E6%95%A3%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4%EF%BC%88Discrete%EF%BC%89"><span class="nav-text">1. 离散动作空间（Discrete）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4%EF%BC%88Continuous%EF%BC%89"><span class="nav-text">2.连续动作空间（Continuous）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E5%BB%BA%E6%A8%A1%EF%BC%88Policy-Network%EF%BC%89"><span class="nav-text">3.策略网络建模（Policy Network）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E6%A8%A1%E5%9E%8B%E5%BB%BA%E6%A8%A1%E7%9A%84%E5%85%B3%E9%94%AE%E8%A6%81%E7%82%B9"><span class="nav-text">4.模型建模的关键要点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E6%80%BB%E7%BB%93"><span class="nav-text">5.总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%B8%B8%E6%88%8F%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E8%BD%A8%E8%BF%B9%E9%87%87%E6%A0%B7"><span class="nav-text">2.游戏生命周期与轨迹采样</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%B8%B8%E6%88%8F%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%EF%BC%88Episode-Lifecycle%EF%BC%89"><span class="nav-text">1.游戏生命周期（Episode Lifecycle）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%BD%A8%E8%BF%B9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%87%87%E6%A0%B7"><span class="nav-text">2.轨迹与经验采样</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%BD%A8%E8%BF%B9%EF%BC%88Trajectory%EF%BC%89%EF%BC%9F"><span class="nav-text">1.什么是轨迹（Trajectory）？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-PPO-%E4%B8%AD%E9%87%87%E6%A0%B7%E8%BD%A8%E8%BF%B9%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="nav-text">2.PPO 中采样轨迹的目的</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E8%BD%A8%E8%BF%B9%E9%87%87%E6%A0%B7%E4%B8%AD%E7%9A%84%E7%BB%86%E8%8A%82%E8%AE%BE%E8%AE%A1"><span class="nav-text">3.轨迹采样中的细节设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%B5%81%E7%A8%8B%E4%B8%8E%E4%BA%A4%E4%BA%92%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-text">3.数据采集流程与交互代码示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%87%87%E6%A0%B7%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%9ARollout-Buffer"><span class="nav-text">1.采样结构设计：Rollout Buffer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-PyTorch-Gym-%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%87%E6%A0%B7%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-text">2.PyTorch + Gym 环境下的数据采样代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82"><span class="nav-text">3.一些细节</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-PPO-%E7%9A%84%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E4%B8%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">3.PPO 的优化目标与策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="nav-text">1.强化学习的目标函数定义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%9A%84%E6%9C%AC%E8%B4%A8%E7%9B%AE%E6%A0%87"><span class="nav-text">1.策略优化的本质目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-text">2.目标函数直观理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">3.状态价值函数与动作价值函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88State-Value-Function%EF%BC%89"><span class="nav-text">1.状态价值函数（State Value Function）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88Action-Value-Function%EF%BC%89"><span class="nav-text">2.动作价值函数（Action Value Function）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-PPO-%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%89%B9%E7%82%B9"><span class="nav-text">4.PPO 中使用的目标函数特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-text">5.与监督学习目标的对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%9C%9F%E6%9C%9B%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%87%87%E6%A0%B7%E4%BC%B0%E8%AE%A1"><span class="nav-text">2.期望计算与采样估计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%87%87%E6%A0%B7%E4%BC%B0%E8%AE%A1%EF%BC%9F"><span class="nav-text">1.为什么需要采样估计？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%87%87%E6%A0%B7%E4%BC%B0%E8%AE%A1%E7%AD%96%E7%95%A5%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-text">2.采样估计策略目标函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="nav-text">4.策略更新与训练流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%9A%E8%BD%AE%E4%BA%A4%E4%BA%92-%E5%A4%9A%E8%BD%AE%E7%AD%96%E7%95%A5%E5%BE%AE%E8%B0%83"><span class="nav-text">1.多轮交互 + 多轮策略微调</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%A4%9A%E8%BD%AE%E4%BA%A4%E4%BA%92%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="nav-text">1.多轮交互的目的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A4%9A%E8%BD%AE%E7%AD%96%E7%95%A5%E5%BE%AE%E8%B0%83"><span class="nav-text">2.多轮策略微调</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%BC%98%E5%8C%96"><span class="nav-text">2.优势函数与稳定性优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%88Advantage-Function%EF%BC%89%EF%BC%9F"><span class="nav-text">1.什么是优势函数（Advantage Function）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88-PPO-%E8%A6%81%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="nav-text">2.为什么 PPO 要使用优势函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%A6%82%E4%BD%95%E4%BC%B0%E8%AE%A1%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="nav-text">3.如何估计优势函数？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E6%8A%80%E5%B7%A7%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-text">3.训练中的常见技巧与超参数设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%87%E5%87%86%E5%8C%96%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%88Advantage-Normalization%EF%BC%89"><span class="nav-text">1.标准化优势函数（Advantage Normalization）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%86%B5%E5%A5%96%E5%8A%B1%EF%BC%88Entropy-Bonus%EF%BC%89"><span class="nav-text">2.熵奖励（Entropy Bonus）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%EF%BC%88Gradient-Clipping%EF%BC%89"><span class="nav-text">3.梯度裁剪（Gradient Clipping）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Reward-Normalization%EF%BC%88%E5%9B%9E%E6%8A%A5%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89"><span class="nav-text">4.Reward Normalization（回报归一化）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%91%A8%E6%9C%9F%E6%80%A7%E8%AF%84%E4%BC%B0%E4%B8%8E-early-stopping"><span class="nav-text">5.周期性评估与 early stopping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-PPO-%E4%B8%AD%E5%85%B3%E9%94%AE%E8%B6%85%E5%8F%82%E6%95%B0%E6%8E%A8%E8%8D%90"><span class="nav-text">6.PPO 中关键超参数推荐</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%80%BB%E7%BB%93-1"><span class="nav-text">5.总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="KeyChan"
      src="/images/key_avatar.png">
  <p class="site-author-name" itemprop="name">KeyChan</p>
  <div class="site-description" itemprop="description">全干工程师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/keychankc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kckeychan@gmail.com" title="E-Mail → mailto:kckeychan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/keychankc" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;keychankc" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/key_avatar.png">
      <meta itemprop="name" content="KeyChan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KeyChan's blog">
      <meta itemprop="description" content="全干工程师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="强化学习 — PPO策略优化算法 | KeyChan's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习 — PPO策略优化算法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-22 11:40:12" itemprop="dateCreated datePublished" datetime="2025-07-22T11:40:12+08:00">2025-07-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:04:48" itemprop="dateModified" datetime="2025-09-21T20:04:48+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2025/07/22/025-reinforcement-learning-ppo/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2025/07/22/025-reinforcement-learning-ppo/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>30 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="1-PPO-算法概述"><a href="#1-PPO-算法概述" class="headerlink" title="1.PPO 算法概述"></a>1.PPO 算法概述</h2><h3 id="1-PPO-的提出背景"><a href="#1-PPO-的提出背景" class="headerlink" title="1.PPO 的提出背景"></a>1.PPO 的提出背景</h3><p>我们还是以智能体如何控制飞船落地的小游戏为例，智能体的目标是通过一系列操作（如向左移动或向右移动）实现平稳着陆。在训练初期，智能体并不知道应该如何操作，它需要通过反复的试探操作，从环境中不断获得反馈并调整策略，最终掌握一套“高奖励”操作方式。</p>
<span id="more"></span>
<p><img src="https://mark-down-dc.oss-cn-hangzhou.aliyuncs.com/key_blog/PPO_LunarLander-v2.gif"><br>虽然游戏过程在视觉上看起来非常快速，但实际上每一步都包含了大量计算与判断，比如，当前所处位置（状态）；当前可执行的动作；作执行后的环境反馈；下一步动作的选择。</p>
<h4 id="1-策略优化"><a href="#1-策略优化" class="headerlink" title="1.策略优化"></a>1.策略优化</h4><p>这些操作正是强化学习（Reinforcement Learning, RL）中<strong>策略优化</strong>需要解决的问题，即通过训练不断<strong>调整策略参数</strong>，使得智能体在与环境交互中获得的<strong>长期累积奖励最大化</strong>。<br>策略优化的基本流程：</p>
<ol>
<li>初始化策略 $πθ$</li>
<li>与环境交互，收集轨迹（状态-动作-奖励序列）</li>
<li>评估当前策略的表现（如回报或优势值）</li>
<li>根据表现更新策略参数 $θ$</li>
<li>重复步骤 2~4，直到收敛</li>
</ol>
<h4 id="2-常见的策略优化方法"><a href="#2-常见的策略优化方法" class="headerlink" title="2.常见的策略优化方法"></a>2.常见的策略优化方法</h4><table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>简介</strong></th>
<th><strong>问题</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>REINFORCE</strong> (Policy Gradient, 1992)</td>
<td>最基本的策略梯度算法，通过采样轨迹直接优化策略期望收益</td>
<td>高方差、收敛慢</td>
</tr>
<tr>
<td><strong>Actor-Critic (A2C&#x2F;A3C)</strong></td>
<td>使用一个 Critic 网络评估当前策略，减少 REINFORCE 的方差</td>
<td>依然可能导致策略不稳定、易陷入局部最优</td>
</tr>
<tr>
<td><strong>Trust Region Policy Optimization (TRPO)</strong> (2015)</td>
<td>引入信赖域约束，通过限制新旧策略 KL 散度，稳定策略更新</td>
<td>数学推导复杂，实际实现困难，约束不易控制</td>
</tr>
<tr>
<td><strong>PPO (2017)</strong></td>
<td>使用剪切目标函数（clip objective）近似信赖域，保持 TRPO 的稳定性优势，同时实现更简单高效的更新</td>
<td><br><br><br></td>
</tr>
</tbody></table>
<p><strong>TRPO（Trust Region Policy Optimization）</strong> 是第一个明确提出“策略更新不能太大”的强化学习方法。它通过约束策略之间的 KL 散度来控制新旧策略之间的距离，使训练更稳定。但还有如下问题：</p>
<ul>
<li>需要进行二阶优化（如共轭梯度法）计算 Fisher 信息矩阵，复杂且效率低。</li>
<li>实际应用中，不容易调节 KL 散度阈值，易导致过拟合或崩溃。</li>
</ul>
<p><strong>PPO（Proximal Policy Optimization）</strong> 正是为了解决 TRPO 的工程复杂性和策略优化的稳定性问题而提出的，目标是在<strong>保持 TRPO 稳定性的同时简化实现和提高效率</strong>。即可以不断试探 —— 收集经验 —— 调整策略，让策略更可能选择“能拿高奖励的动作”。</p>
<h3 id="2-强化学习中的基本要素"><a href="#2-强化学习中的基本要素" class="headerlink" title="2.强化学习中的基本要素"></a>2.强化学习中的基本要素</h3><h4 id="1-原始五要素"><a href="#1-原始五要素" class="headerlink" title="1.原始五要素"></a>1.原始五要素</h4><p>要理解 PPO 首先还要再熟悉一下强化学习框架中的五个基本要素，这源自经典马尔可夫决策过程（MDP）的定义，它们一起共同组成了<strong>基本强化学习交互系统</strong>。</p>
<ol>
<li><strong>智能体（Agent）</strong>：决策者。负责观察环境状态，基于当前策略选择动作。在 PPO 中，智能体由一个策略网络（通常是神经网络）表示，通过优化它的参数 $θ$ 来提升表现。</li>
<li><strong>环境（Environment）</strong>：智能体所处的世界，它接收智能体的动作，并返回新的状态和奖励。在 PPO 中，环境通常由 Gym 或 Unity ML 等平台提供。<br> <strong>作用</strong>：<ul>
<li>根据动作决定下一状态 $s_{t+1}$</li>
<li>给出即时奖励 $r_t$</li>
<li>判断是否结束一轮游戏（done）</li>
</ul>
</li>
<li><strong>状态（State, $s_t$）</strong>：环境在某一时刻的观察值。它反映了当前的环境信息。在 PPO 中，状态作为策略网络的输入，用来决定当前应采取的动作。<br> <strong>举例</strong>：<ul>
<li>棋盘的当前布局</li>
<li>游戏中小人的位置、速度、血量</li>
<li>自动驾驶中摄像头或雷达的观测数据</li>
</ul>
</li>
<li><strong>动作（Action, $a_t$）</strong>：智能体在状态 $s_t$ 下可以选择的行为。在 PPO 中，策略网络输出的是一个概率分布 $\pi_\theta(a|s)$，训练中通过采样 $a_t \sim \pi_\theta(\cdot|s_t)$决定行为，动作影响后续状态与奖励，因此是优化的核心变量。<br> <strong>两种类型</strong>：<ul>
<li>离散动作：如“向左&#x2F;右移动”、“跳跃”</li>
<li>连续动作：如“车速 &#x3D; 0.8”，“关节角度 &#x3D; 35°”</li>
</ul>
</li>
<li><strong>奖励（Reward, $r_t$）</strong>：环境对智能体行为的反馈。奖励的累积值衡量策略的好坏。在 PPO 中，奖励用来估计“优势函数” $A_t$，决定某个动作比平均行为好多少，从而更新策略。<br> <strong>举例</strong>：<ul>
<li>玩游戏得分 +1</li>
<li>棋局胜利奖励 +10</li>
<li>撞墙或掉入坑中给予负奖励 -1</li>
</ul>
</li>
</ol>
<h4 id="2-算法增强要素"><a href="#2-算法增强要素" class="headerlink" title="2.算法增强要素"></a>2.算法增强要素</h4><p>在实际强化学习算法（如 PPO）中，会<strong>引入两个额外要素</strong>（<strong>策略</strong>和<strong>回报与价值函数</strong>），用来辅助或实现优化目标，这两个是<strong>算法层面引入的学习结构</strong>，在现代 RL 中非常关键，尤其是在 PPO、A2C 等基于策略梯度的方法中。</p>
<ol>
<li><strong>策略（Policy, $\pi(a|s)$）</strong>：智能体选择动作的行为准则。可能是确定性的，也可能是概率性的。在 PPO 中，策略是可微的神经网络：$\pi_\theta(a|s)$。PPO 的目标是通过“剪切目标函数”更新策略，使其选择更优动作但又不剧烈偏移</li>
<li><strong>回报与价值函数（Return,$R_t$; Value Function）</strong>：回报 $R_t$，从时刻 t 开始未来累计奖励（可折扣），价值函数，某个动作相比平均行为好多少。在 PPO 中，价值函数用于构造优化目标，一般会结合 GAE（Generalized Advantage Estimation）进行平滑估计。</li>
</ol>
<h4 id="3-交互流程"><a href="#3-交互流程" class="headerlink" title="3.交互流程"></a>3.交互流程</h4><p>状态 $s_t$ → 策略 $πθ$ → 采样动作 $a_t$ → 交互环境 → 得到 $r_t$ 和 $s_{t+1}$ → 存储轨迹 → 策略更新</p>
<p>强化学习的核心是<strong>一个智能体根据策略与环境互动，在状态下选择动作、获得奖励，并用这些经验不断优化策略</strong>。而PPO 等策略优化算法正是这种优化过程中的关键方法。</p>
<h3 id="3-PPO-的核心任务与设计理念"><a href="#3-PPO-的核心任务与设计理念" class="headerlink" title="3.PPO 的核心任务与设计理念"></a>3.PPO 的核心任务与设计理念</h3><h4 id="1-核心任务"><a href="#1-核心任务" class="headerlink" title="1.核心任务"></a>1.核心任务</h4><p>PPO 属于<strong>策略优化类</strong>强化学习算法，其核心任务是：在智能体与环境的交互中，通过采样获得的经验轨迹，<strong>不断更新策略函数 πθ，使其输出更优的动作选择概率分布，从而最大化长期累积奖励。</strong><br>这意味着 PPO 的目标是实现：</p>
<ul>
<li><strong>策略更新要有方向感</strong> → 即优化方向要对</li>
<li><strong>策略更新不能太激进</strong> → 防止训练发散或性能崩塌</li>
<li><strong>策略更新还要高效</strong> → 能在大规模神经网络下快速迭代</li>
</ul>
<h4 id="2-设计理念"><a href="#2-设计理念" class="headerlink" title="2.设计理念"></a>2.设计理念</h4><p>PPO 的提出，是对早期策略梯度方法的<strong>一系列关键性改进与简化</strong>。它的设计理念可以归结为以下三点：</p>
<h5 id="1-稳定策略更新：不要走太远"><a href="#1-稳定策略更新：不要走太远" class="headerlink" title="1.稳定策略更新：不要走太远"></a>1.稳定策略更新：不要走太远</h5><p>在早期策略优化中（如 REINFORCE、A2C），策略每次更新都可能偏离旧策略太远，导致性能不稳定或策略崩溃。<br><strong>PPO 的理念</strong>：学习应该“靠近旧策略附近”，<strong>防止策略剧烈变化</strong>。这是对 TRPO “信赖域”思想的继承。</p>
<ul>
<li>PPO 不使用复杂的二阶优化和 KL 约束</li>
<li>而是引入 <strong>Clip 策略比值函数</strong>，控制策略更新的幅度</li>
</ul>
<p><strong>核心思想：限制策略比值的范围</strong> $r_t &#x3D; \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ <strong>不要偏离 1 太多</strong></p>
<h5 id="2-设计剪切目标函数：软约束代替硬约束"><a href="#2-设计剪切目标函数：软约束代替硬约束" class="headerlink" title="2.设计剪切目标函数：软约束代替硬约束"></a>2.设计剪切目标函数：软约束代替硬约束</h5><p>PPO 用如下的目标函数来近似信赖域的思想：<br>$$L^{\text{CLIP}}(\theta) &#x3D; \mathbb{E}_t \left[ \min\left(r_t(\theta) \hat{A}_t,; \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \cdot \hat{A}_t\right) \right]$$<br>其中：</p>
<ul>
<li>$r_t(\theta) &#x3D; \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$：其中$\pi_\theta(a_t|s_t)$ 表示当前的新策略概率。 其中$\pi_{\theta_{old}}(a_t|s_t)$ 表示旧策略的概率（用来收集数据）。整体表示随着策略参数$\theta$的改变，新策略选这个动作的概率，相比旧策略高了多少，如果 $r_t$ &#x3D; 1.2，新策略更偏爱这个动作，如果 $r_t$ &#x3D; 0.7，新策略则不喜欢这个动作</li>
<li>$\hat{A}_t$：优势函数（动作好坏的估计），表示这个动作比平均水平好多少；$\hat{A}_t$ &gt; 0：这是一个“好动作”，我们希望新策略更倾向它。$\hat{A}_t$ &lt; 0：这是“坏动作”，我们希望新策略更少选它</li>
<li>$\epsilon$：限制更新幅度的一小常数，通常设为 0.1 或 0.2</li>
<li>$\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)$：如果 $r_t(\theta)$ 落在 $[1 - \epsilon, 1 + \epsilon]$，就不变；如果超过了，就限制到边界上；这就像给更新加了“保险绳”，<strong>不要一次跳太远</strong>，比如新策略一下把概率提高太多，反而不稳定</li>
<li>$\min\left( r_t(\theta)\hat{A}_t,\ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot \hat{A}_t \right)$ 表达的是一个 <strong>保守策略</strong>：如果没有超过边界，我们就正常优化；如果超过边界，我们就用“剪切后的版本”替代；哪个小用哪个，从而防止策略过度更新</li>
</ul>
<p>举个简单例子，假设，优势函数 $\hat{A}_t$ &#x3D; 5，旧策略选某动作概率是 0.2，新策略更新后变成 0.4，那么$r_t$ &#x3D; 0.4 &#x2F; 0.2 &#x3D; 2.0，设置 $\epsilon$ &#x3D; 0.2，则 clip 后为 1.2，计算得到：$min(2.0<em>5, 1.2</em>5)$ &#x3D; 6。</p>
<p>当策略改变幅度适中时按正常梯度优化；当策略改变太大（超出剪切阈值）时抑制过度优化，截断回报。<strong>这就是 PPO 名称中 “Proximal”（邻近）的含义</strong>：策略的变化始终控制在邻近区域，防止训练不稳定。</p>
<h5 id="3-简洁实用：适配现代深度学习系统"><a href="#3-简洁实用：适配现代深度学习系统" class="headerlink" title="3.简洁实用：适配现代深度学习系统"></a>3.简洁实用：适配现代深度学习系统</h5><p>PPO 在设计上强调“工程上好用”，这也是它能在工业界广泛落地的关键：</p>
<table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>TRPO</strong></th>
<th><strong>PPO</strong></th>
</tr>
</thead>
<tbody><tr>
<td>使用的优化器</td>
<td>共轭梯度法（二阶优化）</td>
<td>任意一阶优化器（Adam 等）</td>
</tr>
<tr>
<td>是否需要 KL 约束</td>
<td>是，显式添加 KL 散度限制</td>
<td>否，使用剪切函数近似控制</td>
</tr>
<tr>
<td>代码复杂度</td>
<td>高，难以调参</td>
<td>低，容易调试</td>
</tr>
<tr>
<td>样本效率</td>
<td>高</td>
<td>较高</td>
</tr>
<tr>
<td>稳定性</td>
<td>高</td>
<td>稳定且高效</td>
</tr>
</tbody></table>
<p>PPO 被称为“TRPO 的近似简化版”，但其实际表现往往与甚至超过 TRPO。</p>
<h5 id="4-PPO-的整体目标函数"><a href="#4-PPO-的整体目标函数" class="headerlink" title="4.PPO 的整体目标函数"></a>4.PPO 的整体目标函数</h5><p>PPO 的实际损失函数通常包括三项：<br>$$L^{\text{PPO}}(\theta) &#x3D; \mathbb{E}_t \left[ L^{\text{CLIP}}_t(\theta) - c_1 \cdot \text{VF Loss}_t + c_2 \cdot \text{Entropy Bonus}_t \right]$$</p>
<ul>
<li>$L^{\text{CLIP}}_t(\theta)$：剪切后的策略优化目标（核心）</li>
<li>$VF Loss$：值函数损失（用于训练 Critic）</li>
<li>$Entropy Bonus$：鼓励策略的探索性（增加随机性）</li>
</ul>
<p>这体现了 PPO 的三个核心关注点：</p>
<ol>
<li><strong>优化策略但别太过火</strong></li>
<li><strong>训练 Critic 提高估值准确性</strong></li>
<li><strong>鼓励探索避免陷入局部最优</strong></li>
</ol>
<p>PPO 的核心任务是<strong>稳定高效地优化策略函数</strong>，其设计理念是在保持性能提升的同时，通过“剪切目标函数”防止策略剧烈变化，从而实现训练过程中的稳健性、可控性与实用性。</p>
<h2 id="2-PPO-策略网络与环境交互"><a href="#2-PPO-策略网络与环境交互" class="headerlink" title="2.PPO 策略网络与环境交互"></a>2.PPO 策略网络与环境交互</h2><p>在 PPO算法中，策略网络的核心任务是通过与环境的持续交互，从状态中提取信息，采样动作，并依据经验数据不断优化策略。下面我们围绕“智能体如何感知、决策与收集经验”这条主线，分三部分：</p>
<h3 id="1-状态、动作与策略网络建模"><a href="#1-状态、动作与策略网络建模" class="headerlink" title="1.状态、动作与策略网络建模"></a>1.状态、动作与策略网络建模</h3><p>策略优化的前提是智能体必须能够<strong>准确理解当前所处的状态，并基于状态做出合理的动作决策</strong>。下面说说在 PPO 中，如何建模状态、动作以及策略网络，构建整个感知——决策链路。</p>
<h4 id="1-状态空间（State-Space）"><a href="#1-状态空间（State-Space）" class="headerlink" title="1.状态空间（State Space）"></a>1.状态空间（State Space）</h4><p><strong>状态</strong>是智能体感知环境的输入，是决策的基础。在不同的任务中，状态可以是：</p>
<table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>状态示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>经典控制</td>
<td>位置、速度、角度等数值数组（如 CartPole 的 4 维向量）</td>
</tr>
<tr>
<td>棋类游戏</td>
<td>当前棋盘的二维数组或编码</td>
</tr>
<tr>
<td>图像输入</td>
<td>原始像素矩阵（如 RGB 图像）</td>
</tr>
<tr>
<td>自定义环境</td>
<td>环境描述符、特征向量、传感器数据等</td>
</tr>
</tbody></table>
<p>在 PPO 中，我们通常将状态 $s$ 作为一个<strong>固定维度的向量输入策略网络</strong>，以便网络进行前向传播和策略输出。</p>
<h4 id="2-动作空间（Action-Space）"><a href="#2-动作空间（Action-Space）" class="headerlink" title="2.动作空间（Action Space）"></a>2.动作空间（Action Space）</h4><p>动作定义了智能体可以对环境采取的行为，其建模方式取决于环境类型：</p>
<h5 id="1-离散动作空间（Discrete）"><a href="#1-离散动作空间（Discrete）" class="headerlink" title="1. 离散动作空间（Discrete）"></a><strong>1. 离散动作空间（Discrete）</strong></h5><ul>
<li>例如：飞船落地case中动作只有两个（左或右）</li>
<li>PPO 中策略网络最后输出一组 <strong>softmax 概率分布</strong>，对所有可能动作进行建模</li>
<li>示例输出：$\pi_\theta(a|s)$ &#x3D; [0.4, 0.6]，表示选择动作 1 的概率为 60%</li>
</ul>
<h5 id="2-连续动作空间（Continuous）"><a href="#2-连续动作空间（Continuous）" class="headerlink" title="2.连续动作空间（Continuous）"></a><strong>2.连续动作空间（Continuous）</strong></h5><ul>
<li>例如：机器人控制中的加速度、关节角度等</li>
<li>PPO 中策略网络不直接输出动作，而是输出<strong>动作分布的参数</strong><ul>
<li>常见方式是：输出一个<strong>均值向量 μ 和对数标准差 logσ</strong></li>
<li>动作从该分布中采样：$a \sim \mathcal{N}(\mu, \sigma)$</li>
</ul>
</li>
<li>这种设计允许策略保持随机性，且便于求导</li>
</ul>
<h4 id="3-策略网络建模（Policy-Network）"><a href="#3-策略网络建模（Policy-Network）" class="headerlink" title="3.策略网络建模（Policy Network）"></a>3.策略网络建模（Policy Network）</h4><p>策略网络是 PPO 的核心部分，它是一个参数化函数 $\pi_\theta(a|s)$，用于对给定状态生成动作概率分布。</p>
<p><strong>PPO 中策略网络常用结构如下：</strong></p>
<table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>结构示意</strong></th>
<th><strong>应用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td>MLP（多层感知机）</td>
<td>state → Linear → ReLU → Linear → Softmax</td>
<td>状态为向量型任务，如控制类环境</td>
</tr>
<tr>
<td>CNN（卷积网络）</td>
<td>image → Conv → Pool → Flatten → Linear → Softmax</td>
<td>状态为图像，如 Atari 游戏</td>
</tr>
<tr>
<td>Actor-Critic 共用前缀</td>
<td>shared → actor head &#x2F; critic head</td>
<td>提高参数共享与训练效率</td>
</tr>
</tbody></table>
<p>PPO 通常采用 <strong>Actor-Critic 框架</strong>，即：</p>
<ul>
<li><strong>Actor 网络</strong>：策略网络 $\pi_\theta$，主要是根据当前状态 s 生成动作 a 的概率分布</li>
<li><strong>Critic 网络</strong>：值函数估计器 $V_\phi(s)$，用于估计当前状态的“好坏”，即预期的累计回报</li>
</ul>
<p>有时两者共享前几层网络，只在最后分出两个输出头（Head），分别输出动作分布参数与状态值。</p>
<h4 id="4-模型建模的关键要点"><a href="#4-模型建模的关键要点" class="headerlink" title="4.模型建模的关键要点"></a>4.模型建模的关键要点</h4><table>
<thead>
<tr>
<th><strong>模块</strong></th>
<th><strong>建模技巧</strong></th>
</tr>
</thead>
<tbody><tr>
<td>输入归一化</td>
<td>状态输入需归一化到相似尺度（如 [0, 1] 或 [-1, 1]）以稳定训练</td>
</tr>
<tr>
<td>输出处理</td>
<td>离散动作需加 softmax，连续动作需生成正态分布参数</td>
</tr>
<tr>
<td>初始化</td>
<td>用适当权重初始化策略输出层（如小权重、防止初期策略过于确定）</td>
</tr>
<tr>
<td>探索性</td>
<td>PPO 天然支持随机策略，但也可在输出中控制熵（entropy）来增强探索</td>
</tr>
</tbody></table>
<h4 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h4><p>PPO 中策略建模的三大核心：</p>
<ol>
<li><strong>状态建模</strong>：将环境状态编码为数值向量，作为策略网络输入</li>
<li><strong>动作建模</strong>：依据任务类型，输出动作概率分布或动作分布参数</li>
<li><strong>策略网络结构</strong>：采用深度神经网络建模 $\pi_\theta$，支持高维输入与复杂策略表达</li>
</ol>
<p>通过这套建模体系，PPO 的智能体才能在环境中感知状态、做出行动，并基于经验不断优化策略，实现强化学习的核心目标。</p>
<h3 id="2-游戏生命周期与轨迹采样"><a href="#2-游戏生命周期与轨迹采样" class="headerlink" title="2.游戏生命周期与轨迹采样"></a>2.游戏生命周期与轨迹采样</h3><p>在强化学习中，智能体的学习是建立在与环境反复交互所产生的“经验轨迹”之上的。下面介绍下PPO 中一个 episode（游戏生命周期）的全过程，以及如何通过与环境交互<strong>采样轨迹数据用于策略优化</strong>的。</p>
<h4 id="1-游戏生命周期（Episode-Lifecycle）"><a href="#1-游戏生命周期（Episode-Lifecycle）" class="headerlink" title="1.游戏生命周期（Episode Lifecycle）"></a>1.游戏生命周期（Episode Lifecycle）</h4><p>在 PPO 中，每一次与环境完整交互的过程称为一个 <strong>episode</strong>，也可以理解为一局“游戏”。每一局游戏的生命周期如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">初始化环境</span><br><span class="line">↓</span><br><span class="line">得到初始状态 s₀</span><br><span class="line">↓</span><br><span class="line">循环：</span><br><span class="line">    根据策略 πθ(s_t) 采样动作 a_t</span><br><span class="line">    环境执行动作，返回奖励 r_t 和新状态 s_&#123;t+1&#125;</span><br><span class="line">    存储 (s_t, a_t, r_t, s_&#123;t+1&#125;, done)</span><br><span class="line">    如果 done=True，终止 episode</span><br><span class="line">↓</span><br><span class="line">一轮结束</span><br></pre></td></tr></table></figure>
<p>这一过程体现了 RL 的核心闭环：<strong>状态 → 动作 → 奖励 + 新状态 → 再决策</strong>。</p>
<h4 id="2-轨迹与经验采样"><a href="#2-轨迹与经验采样" class="headerlink" title="2.轨迹与经验采样"></a>2.轨迹与经验采样</h4><p>PPO 是一种<strong>on-policy 策略梯度算法</strong>，它依赖于当前策略下采样的<strong>真实轨迹数据</strong>进行优化。因此，轨迹采样的质量直接影响训练效果。</p>
<h5 id="1-什么是轨迹（Trajectory）？"><a href="#1-什么是轨迹（Trajectory）？" class="headerlink" title="1.什么是轨迹（Trajectory）？"></a>1.什么是轨迹（Trajectory）？</h5><p>轨迹 τ​表示智能体与环境的一次完整交互序列，包含状态、动作、奖励三元组，一条轨迹通常表示为：$\tau &#x3D; { (s_0, a_0, r_0), (s_1, a_1, r_1), \dots, (s_T, a_T, r_T)}$<br>如果策略是随机的，轨迹本质上是从策略分布中采样得到的一条路径：$\tau \sim \pi_\theta$<br>概念：</p>
<ul>
<li>​<strong>初始 (s0​,a0​,r0​)​</strong>​：由环境初始化，策略选择第一个动作，环境返回第一个奖励</li>
<li><strong>终止 (sT​,aT​,rT​)</strong>：最后一个有效状态，可能影响最终奖励，之后环境终止</li>
</ul>
<p>轨迹是强化学习的<strong>核心数据单元</strong>，其质量直接影响训练效果。PPO 等 on-policy 算法尤其依赖<strong>当前策略采样的新鲜轨迹</strong>进行优化。</p>
<h5 id="2-PPO-中采样轨迹的目的"><a href="#2-PPO-中采样轨迹的目的" class="headerlink" title="2.PPO 中采样轨迹的目的"></a>2.PPO 中采样轨迹的目的</h5><p>采样轨迹（Trajectories）的核心目的是为策略优化提供<strong>真实交互数据</strong>，确保策略更新的<strong>稳定性</strong>和<strong>高效性</strong>。<br>采样的轨迹数据用于以下关键计算：</p>
<ul>
<li><strong>优势函数</strong> $\hat{A}_t$ ：动作好坏的估计。动作的惊喜程度（实际分 vs 预期分）</li>
<li><strong>目标值函数</strong> $\hat{V}_t$ 或 $R_t$：$\hat{V}_t$预测当前状态$st$​的<strong>未来总得分期望值</strong>(预测)，$R_t$实际从$st$​开始到结束的真实总得分（真实结果）。预期分 vs 真实分</li>
<li><strong>策略旧概率</strong> $\pi_{\theta_{\text{old}}}(a_t|s_t)$，用于构造剪切比值，记录<strong>更新策略前</strong>，在$st$​状态下选择$at$​的原始概率。更新策略时的”安全带”，防止AI瞎改</li>
</ul>
<h5 id="3-轨迹采样中的细节设计"><a href="#3-轨迹采样中的细节设计" class="headerlink" title="3.轨迹采样中的细节设计"></a>3.轨迹采样中的细节设计</h5><p>在 PPO 中，为了增强训练稳定性和样本利用效率，轨迹采样有以下几个常见设计点：</p>
<ul>
<li><strong>多环境并行采样</strong>：使用多个环境实例（如 vectorized Gym）同时生成轨迹，提高样本效率</li>
<li><strong>固定步长而非固定局数</strong>：PPO 通常采样固定步数（如 2048 步），不管中间 episode 是否终止</li>
<li><strong>状态标准化</strong>：对状态输入进行归一化处理，保持数值稳定</li>
<li><strong>记录旧策略概率</strong>：每一步记录旧策略下的动作概率，用于后续剪切目标计算</li>
<li><strong>缓存格式</strong>：使用 RolloutBuffer 等结构缓存状态、动作、奖励、done、log_prob、value 等信息</li>
</ul>
<p>智能体通过当前策略网络与环境反复交互，采样轨迹；利用采样数据估计策略梯度与值函数误差；进而优化策略网络，实现强化学习闭环。</p>
<h3 id="3-数据采集流程与交互代码示例"><a href="#3-数据采集流程与交互代码示例" class="headerlink" title="3.数据采集流程与交互代码示例"></a>3.数据采集流程与交互代码示例</h3><p>在 PPO 中，数据采集是策略优化的第一步。通过策略网络与环境的交互，采集大量高质量的状态-动作-奖励轨迹，是后续学习曲线收敛的关键。</p>
<p>一个完整的数据采样流程，通常包括以下核心步骤：</p>
<ol>
<li>初始化环境与策略网络</li>
<li>在当前策略下循环交互：<ul>
<li>根据状态 $s_t$，用策略网络输出动作 $a_t$</li>
<li>用该动作与环境交互，获得 $r_t$, $s_{t+1}$, done</li>
<li>将交互信息缓存至采样缓冲区</li>
</ul>
</li>
<li>重复以上过程直到采样足够时间步（如 2048）</li>
<li>结束本轮采样，开始策略优化阶段</li>
</ol>
<h4 id="1-采样结构设计：Rollout-Buffer"><a href="#1-采样结构设计：Rollout-Buffer" class="headerlink" title="1.采样结构设计：Rollout Buffer"></a>1.采样结构设计：Rollout Buffer</h4><p>为了高效组织采样的数据，我们通常使用一个数据结构 —— <strong>Rollout Buffer</strong>，用于存储每一轮采样的经验。<br><strong>典型缓存字段包括：</strong></p>
<ul>
<li>states：状态序列</li>
<li>actions：动作序列</li>
<li>rewards：即时奖励</li>
<li>dones：是否终止标志</li>
<li>log_probs：旧策略的动作概率（用于 Clip 比值）</li>
<li>values：值函数预测（用于计算优势）</li>
<li>next_states（可选）：用于 TD 估计</li>
</ul>
<h4 id="2-PyTorch-Gym-环境下的数据采样代码示例"><a href="#2-PyTorch-Gym-环境下的数据采样代码示例" class="headerlink" title="2.PyTorch + Gym 环境下的数据采样代码示例"></a>2.PyTorch + Gym 环境下的数据采样代码示例</h4><p>以下是一个简化的 PPO 数据采样代码流程，支持离散动作环境（如 CartPole）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RolloutBuffer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.clear()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clear</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.states, <span class="variable language_">self</span>.actions, <span class="variable language_">self</span>.rewards = [], [], []</span><br><span class="line">        <span class="variable language_">self</span>.dones, <span class="variable language_">self</span>.log_probs, <span class="variable language_">self</span>.values = [], [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, state, action, reward, done, log_prob, value</span>):</span><br><span class="line">        <span class="variable language_">self</span>.states.append(state)</span><br><span class="line">        <span class="variable language_">self</span>.actions.append(action)</span><br><span class="line">        <span class="variable language_">self</span>.rewards.append(reward)</span><br><span class="line">        <span class="variable language_">self</span>.dones.append(done)</span><br><span class="line">        <span class="variable language_">self</span>.log_probs.append(log_prob)</span><br><span class="line">        <span class="variable language_">self</span>.values.append(value)</span><br></pre></td></tr></table></figure>
<p>策略与环境交互采样过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collect_trajectories</span>(<span class="params">env, policy, buffer, rollout_steps</span>):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(rollout_steps):</span><br><span class="line">        state_tensor = torch.FloatTensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 策略网络输出动作分布</span></span><br><span class="line">        dist, value = policy(state_tensor)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        log_prob = dist.log_prob(action)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 与环境交互</span></span><br><span class="line">        next_state, reward, done, _ = env.step(action.item())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储经验</span></span><br><span class="line">        buffer.add(state, action.item(), reward, done, log_prob.item(), value.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 状态更新</span></span><br><span class="line">        state = next_state <span class="keyword">if</span> <span class="keyword">not</span> done <span class="keyword">else</span> env.reset()</span><br></pre></td></tr></table></figure>
<h4 id="3-一些细节"><a href="#3-一些细节" class="headerlink" title="3.一些细节"></a>3.一些细节</h4><table>
<thead>
<tr>
<th><strong>点位</strong></th>
<th><strong>描述</strong></th>
</tr>
</thead>
<tbody><tr>
<td>保持策略随机性</td>
<td>必须使用 sample() 而非 argmax，以保证策略梯度的可导性</td>
</tr>
<tr>
<td>记录 log_prob</td>
<td>是 PPO 中 $r_t &#x3D; π_θ(a_t)$</td>
</tr>
<tr>
<td>环境重置</td>
<td>遇到 done&#x3D;True 时应立即 env.reset()，否则状态将失效</td>
</tr>
<tr>
<td>状态归一化（可选）</td>
<td>统一状态维度、缩放数值区间，提高训练稳定性</td>
</tr>
<tr>
<td>采样步数设定</td>
<td>PPO 通常使用固定步数采样（如 2048）而非固定局数</td>
</tr>
</tbody></table>
<h2 id="3-PPO-的优化目标与策略梯度"><a href="#3-PPO-的优化目标与策略梯度" class="headerlink" title="3.PPO 的优化目标与策略梯度"></a>3.PPO 的优化目标与策略梯度</h2><p>在强化学习中，智能体的学习核心是<strong>策略优化</strong>，而策略优化的本质就是不断提升策略网络参数，使其在与环境的交互中获得更高的长期回报。本章将系统梳理强化学习中的优化目标，以及如何通过<strong>策略梯度方法</strong>实现对策略的可导性学习</p>
<h3 id="1-强化学习的目标函数定义"><a href="#1-强化学习的目标函数定义" class="headerlink" title="1.强化学习的目标函数定义"></a>1.强化学习的目标函数定义</h3><p>在RL中，智能体的目标并不是像监督学习那样最小化某个损失函数，而是<strong>最大化从环境中获得的长期累计奖励</strong>。下面将从基本原理出发，系统地推导并解释强化学习中策略优化的目标函数形式。</p>
<h4 id="1-策略优化的本质目标"><a href="#1-策略优化的本质目标" class="headerlink" title="1.策略优化的本质目标"></a>1.策略优化的本质目标</h4><p>强化学习中的策略由一个参数化的函数 $\pi_\theta(a|s)$ 表示，它给定当前状态 $s$ 时输出动作 $a$ 的概率分布。我们希望学习一组参数 $\theta$，使得该策略在环境中表现得尽可能好。</p>
<p>强化学习的优化目标是 <strong>最大化期望回报</strong>（Expected Return）：<br>$$J(\theta) &#x3D; \mathbb{E}{\tau \sim \pi\theta} \left[ \sum_{t&#x3D;0}^{T} \gamma^t r_t \right]$$<br>其中：</p>
<ul>
<li>$\theta$：策略网络的参数</li>
<li>$\tau &#x3D; (s_0, a_0, r_0, s_1, \dots)$：表示一条轨迹 trajectory</li>
<li>$\gamma \in (0, 1]$：折扣因子，用于权衡长期与短期奖励</li>
<li>$r_t$：智能体在第 t 步获得的即时奖励</li>
<li>$\mathbb{E}{\tau \sim \pi\theta}[\cdot]$：在当前策略下，对所有可能轨迹取期望</li>
</ul>
<h4 id="2-目标函数直观理解"><a href="#2-目标函数直观理解" class="headerlink" title="2.目标函数直观理解"></a>2.目标函数直观理解</h4><p>这个目标函数代表了智能体在当前策略下，预计可以获得的平均长期奖励。最大化它，就意味着让智能体倾向于采取能长期带来更高奖励的行为策略。<br>换句话说：</p>
<blockquote>
<p>强化学习 ≈ 找到一套动作决策规则，使得智能体在环境中获得的<strong>总奖励期望值最大化</strong>。</p>
</blockquote>
<h4 id="3-状态价值函数与动作价值函数"><a href="#3-状态价值函数与动作价值函数" class="headerlink" title="3.状态价值函数与动作价值函数"></a>3.状态价值函数与动作价值函数</h4><p>为了更细致地描述不同状态或动作的好坏，我们引入两个关键函数，它们都是目标函数的局部展开：</p>
<h5 id="1-状态价值函数（State-Value-Function）"><a href="#1-状态价值函数（State-Value-Function）" class="headerlink" title="1.状态价值函数（State Value Function）"></a><strong>1.状态价值函数（State Value Function）</strong></h5><p>$V^\pi(s) &#x3D; \mathbb{E}\pi \left[ \sum{t&#x3D;0}^\infty \gamma^t r_t ,\middle|, s_0 &#x3D; s \right]$<br>表示在状态 $s$ 下，使用策略 $\pi$ 所能获得的预期总奖励。</p>
<h5 id="2-动作价值函数（Action-Value-Function）"><a href="#2-动作价值函数（Action-Value-Function）" class="headerlink" title="2.动作价值函数（Action Value Function）"></a><strong>2.动作价值函数（Action Value Function）</strong></h5><p>$Q^\pi(s, a) &#x3D; \mathbb{E}\pi \left[ \sum{t&#x3D;0}^\infty \gamma^t r_t ,\middle|, s_0 &#x3D; s, a_0 &#x3D; a \right]$<br>表示在状态 $s$ 下选择动作 $a$，之后按照策略 $\pi$ 行动，能获得的期望回报。</p>
<blockquote>
<p>这两个函数会在后续优势函数和策略梯度中反复出现。</p>
</blockquote>
<h4 id="4-PPO-中使用的目标函数特点"><a href="#4-PPO-中使用的目标函数特点" class="headerlink" title="4.PPO 中使用的目标函数特点"></a>4.PPO 中使用的目标函数特点</h4><p>在 PPO 训练过程中，虽然最终优化的是剪切策略目标（Clip Objective），但仍然是围绕这个“期望回报最大化”的原始目标函数进行近似优化。这个原始目标提供了：</p>
<ul>
<li>理论基础：PPO 的损失项是从 $J(\theta)$ 推导出来的</li>
<li>导数依据：策略梯度就是对 $J(\theta)$ 求导数</li>
<li>稳定性参考：值函数分支用于估计状态回报 $V(s)$ 以减少方差</li>
</ul>
<h4 id="5-与监督学习目标的对比"><a href="#5-与监督学习目标的对比" class="headerlink" title="5.与监督学习目标的对比"></a>5.与监督学习目标的对比</h4><table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>监督学习</strong></th>
<th><strong>强化学习</strong></th>
</tr>
</thead>
<tbody><tr>
<td>目标函数</td>
<td>最小化预测误差（如 MSE）</td>
<td>最大化期望回报 $J(\theta)$</td>
</tr>
<tr>
<td>标签</td>
<td>明确给定的 ground truth</td>
<td>奖励由环境反馈，不稳定</td>
</tr>
<tr>
<td>样本依赖</td>
<td>样本独立 $i.i.d.$</td>
<td>样本依赖于策略，非独立</td>
</tr>
<tr>
<td>难点</td>
<td>特征建模</td>
<td>时序性、稀疏奖励、探索-利用权衡</td>
</tr>
</tbody></table>
<h3 id="2-期望计算与采样估计"><a href="#2-期望计算与采样估计" class="headerlink" title="2.期望计算与采样估计"></a>2.期望计算与采样估计</h3><p>在强化学习中，我们的目标是最大化如下策略目标函数：<br>$J(\theta) &#x3D; \mathbb{E}{\tau \sim \pi\theta} \left[ \sum_{t&#x3D;0}^{T} \gamma^t r_t \right]$<br>但由于轨迹分布$\tau \sim \pi_\theta$涉及环境动态与策略随机性，<strong>这个期望在实际中无法精确求解</strong>，因此必须通过<strong>采样近似</strong>来计算。</p>
<h4 id="1-为什么需要采样估计？"><a href="#1-为什么需要采样估计？" class="headerlink" title="1.为什么需要采样估计？"></a>1.为什么需要采样估计？</h4><p>强化学习不像监督学习那样拥有固定训练集。环境是一个黑箱，策略在其中不断与环境交互产生样本，因此：</p>
<ul>
<li><strong>分布不可知</strong>：轨迹 $\tau$ 的真实分布 $p(\tau)$ 无法直接获得</li>
<li><strong>动态生成数据</strong>：只能通过当前策略与环境互动收集数据</li>
<li><strong>期望不易解析</strong>：数学上无法求出对策略参数 $\theta$ 的解析梯度</li>
</ul>
<p>因此，我们必须依赖<strong>经验采样</strong>方式，用近似值替代期望与梯度。</p>
<h4 id="2-采样估计策略目标函数"><a href="#2-采样估计策略目标函数" class="headerlink" title="2.采样估计策略目标函数"></a>2.采样估计策略目标函数</h4><p>我们用<strong>经验平均值（Monte Carlo 估计）</strong> 来代替对轨迹的期望：<br>$J(\theta) \approx \frac{1}{N} \sum_{i&#x3D;1}^{N} \left[ \sum_{t&#x3D;0}^{T_i} \gamma^t r_t^{(i)} \right]$<br>其中：</p>
<ul>
<li>$N$：采样轨迹的数量</li>
<li>$T_i$：第 $i$ 条轨迹的终止时间步</li>
<li>$r_t^{(i)}$：第 i 条轨迹第 $t$ 步获得的奖励</li>
</ul>
<p>通过这种方式，我们可以估计在当前策略 $\pi_\theta$ 下，策略执行的表现。</p>
<h2 id="4-策略更新与训练流程"><a href="#4-策略更新与训练流程" class="headerlink" title="4.策略更新与训练流程"></a>4.策略更新与训练流程</h2><h3 id="1-多轮交互-多轮策略微调"><a href="#1-多轮交互-多轮策略微调" class="headerlink" title="1.多轮交互 + 多轮策略微调"></a>1.多轮交互 + 多轮策略微调</h3><p>一次完整的策略训练过程包含两大阶段：</p>
<ol>
<li><strong>与环境的交互采样阶段（rollout phase）</strong>：策略根据当前参数 $\theta$ 与环境互动，生成多个 episode 或 trajectory，每个时间步采集三类数据：状态 $s_t$、动作 $a_t$、回报&#x2F;优势 $A_t$。</li>
<li><strong>策略更新阶段（policy optimization phase）</strong>：使用采集到的样本数据，进行多次 epoch 的策略微调，即训练 PPO 的目标函数。</li>
</ol>
<h4 id="1-多轮交互的目的"><a href="#1-多轮交互的目的" class="headerlink" title="1.多轮交互的目的"></a>1.多轮交互的目的</h4><p>在传统的策略梯度方法（如 REINFORCE）中，通常每轮训练只使用一次采样数据，造成：</p>
<ul>
<li>样本利用率极低</li>
<li>更新方向受噪声影响大</li>
<li>不利于收敛与稳定性</li>
</ul>
<p>而 PPO 的改进之一是：<strong>一次采样数据可以用来进行多轮策略更新（Multiple Epochs）</strong>，极大提高训练效率。</p>
<h4 id="2-多轮策略微调"><a href="#2-多轮策略微调" class="headerlink" title="2.多轮策略微调"></a>2.多轮策略微调</h4><p>PPO 借助其 <strong>clip 剪切机制</strong>，可以在 <strong>不改变旧策略分布过快的前提下</strong>，使用同一批采样数据多次训练：</p>
<ul>
<li>将采样数据分为若干 minibatch（小批次）；</li>
<li>对每个小批次进行 3~10 轮更新（称为 multiple epochs）；</li>
<li>每次更新计算目标函数：$L^{\text{CLIP}}(\theta) &#x3D; \mathbb{E}_{(s,a) \sim \mathcal{D}} \left[ \min \left( r(\theta) A, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A \right) \right]$，其中 $\mathcal{D}$ 是采样的数据集。</li>
</ul>
<p>PPO 的训练采用 <strong>“一次采样 + 多轮更新”</strong> 的方式，在保持策略分布稳定的前提下，大大提高了样本利用效率，是其效率与稳定性兼得的核心优势。</p>
<h3 id="2-优势函数与稳定性优化"><a href="#2-优势函数与稳定性优化" class="headerlink" title="2.优势函数与稳定性优化"></a>2.优势函数与稳定性优化</h3><h4 id="1-什么是优势函数（Advantage-Function）？"><a href="#1-什么是优势函数（Advantage-Function）？" class="headerlink" title="1.什么是优势函数（Advantage Function）？"></a>1.什么是优势函数（Advantage Function）？</h4><p>优势函数 $A_t$ 是强化学习中用于衡量一个动作相对“平均水平”好坏的重要指标，定义为：<br>$A_t &#x3D; Q(s_t, a_t) - V(s_t)$<br>其中：</p>
<ul>
<li>$Q(s_t, a_t)$：表示在状态 $s_t$ 执行动作 $a_t$ 后的总期望回报</li>
<li>$V(s_t)$：表示在状态 $s_t$ 下采取任意策略的平均回报</li>
<li>所以 $A_t$ 表示「当前动作比平均水平好多少」</li>
</ul>
<p> 若 $A_t$ &gt; 0，说明当前动作优于平均水平；反之则劣于平均。</p>
<h4 id="2-为什么-PPO-要使用优势函数？"><a href="#2-为什么-PPO-要使用优势函数？" class="headerlink" title="2.为什么 PPO 要使用优势函数？"></a>2.为什么 PPO 要使用优势函数？</h4><p>PPO 的目标是最大化：<br>$L^{\text{CLIP}}(\theta) &#x3D; \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]$<br>可以看出：</p>
<ul>
<li>PPO 的策略梯度方向完全依赖于 $A_t$</li>
<li>准确估计 $A_t$ 是训练稳定性与收敛性的关键</li>
</ul>
<h4 id="3-如何估计优势函数？"><a href="#3-如何估计优势函数？" class="headerlink" title="3.如何估计优势函数？"></a>3.如何估计优势函数？</h4><p>在实践中，我们无法直接得到 $Q(s_t, a_t)$，所以通常用<strong>经验回报</strong> $R_t$ 与状态值函数 $V(s_t)$ 的差值近似：<br>$A_t &#x3D; R_t - V(s_t)$<br>但这样会带来<strong>高方差问题</strong>。于是，提出了一种更平滑的优势估计方法——<strong>广义优势估计（GAE）</strong>。</p>
<p>PPO 中的优势函数不仅决定了策略的更新方向，还通过 GAE、归一化等方法提升了训练的稳定性与收敛速度，是连接策略目标与实际更新之间的“稳定桥梁”。</p>
<h3 id="3-训练中的常见技巧与超参数设置"><a href="#3-训练中的常见技巧与超参数设置" class="headerlink" title="3.训练中的常见技巧与超参数设置"></a>3.训练中的常见技巧与超参数设置</h3><p>在强化学习中，尤其是 PPO 这样的 on-policy 策略优化方法，由于每一次更新都依赖新鲜采样数据，因此<strong>训练效率和稳定性尤为重要</strong>。本节总结了一系列实践中广泛使用的技巧与推荐超参数，帮助你在实现 PPO 时取得更好效果。常见训练技巧有：</p>
<h4 id="1-标准化优势函数（Advantage-Normalization）"><a href="#1-标准化优势函数（Advantage-Normalization）" class="headerlink" title="1.标准化优势函数（Advantage Normalization）"></a>1.标准化优势函数（Advantage Normalization）</h4><ul>
<li><strong>作用</strong>：消除优势值的数值尺度影响，避免策略梯度方向失衡，通常效果显著，建议始终启用。</li>
<li><strong>做法</strong>：每个训练批次上，将优势函数 A_t 标准化为：$A_t \leftarrow \frac{A_t - \mu}{\sigma + \epsilon}$</li>
</ul>
<h4 id="2-熵奖励（Entropy-Bonus）"><a href="#2-熵奖励（Entropy-Bonus）" class="headerlink" title="2.熵奖励（Entropy Bonus）"></a>2.熵奖励（Entropy Bonus）</h4><ul>
<li><strong>作用</strong>：增加策略的随机性，防止策略过早陷入确定性，提升探索能力。</li>
<li><strong>添加到目标函数</strong> 中，完整形式为：$L^{\text{PPO}} &#x3D; L^{\text{CLIP}} - c_1 \cdot \text{VF Loss} + c_2 \cdot \text{Entropy Bonus}$</li>
<li><strong>超参数</strong> $c_2$ 控制熵奖励权重，典型值为：0.001 ~ 0.02</li>
</ul>
<h4 id="3-梯度裁剪（Gradient-Clipping）"><a href="#3-梯度裁剪（Gradient-Clipping）" class="headerlink" title="3.梯度裁剪（Gradient Clipping）"></a>3.梯度裁剪（Gradient Clipping）</h4><ul>
<li><strong>作用</strong>：防止策略梯度爆炸，保证每步更新平稳</li>
<li><strong>做法</strong>：对所有梯度统一做 L2 范数裁剪</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># max_grad_norm 推荐值 0.5</span></span><br><span class="line">torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)</span><br></pre></td></tr></table></figure>
<h4 id="4-Reward-Normalization（回报归一化）"><a href="#4-Reward-Normalization（回报归一化）" class="headerlink" title="4.Reward Normalization（回报归一化）"></a>4.Reward Normalization（回报归一化）</h4><ul>
<li>特别适用于 reward 数值波动较大或不同任务 reward 分布差异大的情况</li>
<li>可提升学习稳定性，但在 reward 分布已稳定时可略去</li>
</ul>
<h4 id="5-周期性评估与-early-stopping"><a href="#5-周期性评估与-early-stopping" class="headerlink" title="5.周期性评估与 early stopping"></a>5.周期性评估与 early stopping</h4><ul>
<li>在训练周期中设定验证环境，用于评估策略性能趋势</li>
<li>若策略在评估环境中长期不提升，可早停或调整学习率策略</li>
</ul>
<h4 id="6-PPO-中关键超参数推荐"><a href="#6-PPO-中关键超参数推荐" class="headerlink" title="6.PPO 中关键超参数推荐"></a>6.PPO 中关键超参数推荐</h4><p>以下为 PPO 中常用的重要超参数及其典型建议范围（可依据任务微调）：</p>
<table>
<thead>
<tr>
<th><strong>超参数</strong></th>
<th><strong>描述</strong></th>
<th><strong>推荐值</strong></th>
</tr>
</thead>
<tbody><tr>
<td>γ</td>
<td>折扣因子</td>
<td>0.99</td>
</tr>
<tr>
<td>λ</td>
<td>GAE 参数，控制 bias-variance 平衡</td>
<td>0.95</td>
</tr>
<tr>
<td>ε</td>
<td>PPO 剪切阈值</td>
<td>0.1 ~ 0.3</td>
</tr>
<tr>
<td>K_epochs</td>
<td>每轮数据的微调轮数</td>
<td>4 ~ 10</td>
</tr>
<tr>
<td>mini_batch_size</td>
<td>小批次样本大小</td>
<td>64 ~ 1024</td>
</tr>
<tr>
<td>learning_rate</td>
<td>学习率</td>
<td>1e-4 ~ 3e-4</td>
</tr>
<tr>
<td>c1</td>
<td>值函数损失权重</td>
<td>0.5</td>
</tr>
<tr>
<td>c2</td>
<td>熵正则项权重</td>
<td>0.01</td>
</tr>
<tr>
<td>max_grad_norm</td>
<td>梯度裁剪上限</td>
<td>0.5</td>
</tr>
</tbody></table>
<p>这些超参数会根据任务复杂度、action space 维度和 reward scale 做适当调整。</p>
<h2 id="5-总结-1"><a href="#5-总结-1" class="headerlink" title="5.总结"></a>5.总结</h2><p>PPO（Proximal Policy Optimization）是一种高效稳定的策略优化算法，通过剪切目标函数（clip objective）限制策略更新幅度，在保持训练稳定性的同时简化实现。<br>其核心设计包括：</p>
<ol>
<li>用策略比值剪切替代复杂的KL约束，防止策略突变</li>
<li>采用广义优势估计（GAE）平衡偏差与方差</li>
<li>支持多轮数据复用提升样本效率</li>
</ol>
<p>PPO通过Actor-Critic框架交互采样，结合值函数损失和熵奖励项，实现策略的渐进式优化。典型超参数如剪切阈值ε&#x3D;0.2、折扣因子γ&#x3D;0.99、GAE参数λ&#x3D;0.95，配合梯度裁剪和优势归一化等技巧，使其在连续&#x2F;离散动作任务中均表现优异，成为强化学习领域的基准算法。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>KeyChan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/" title="强化学习 — PPO策略优化算法">https://www.keychan.xyz/2025/07/22/025-reinforcement-learning-ppo/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://x.com/keychankc">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96/" rel="tag"># 策略优化</a>
              <a href="/tags/PPO/" rel="tag"># PPO</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/14/024-reinforcement-learning-start/" rel="prev" title="强化学习 — 试错、策略与长期奖励">
                  <i class="fa fa-angle-left"></i> 强化学习 — 试错、策略与长期奖励
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/30/026-rl-ppo-discrete-continuous-case/" rel="next" title="PPO算法在连续与离散动作空间中的案例实践">
                  PPO算法在连续与离散动作空间中的案例实践 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KeyChan</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">237k</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/keychankc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="/js/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/ribbon.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://comment.mengyajia.com","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":false,"pageview":false,"placeholder":"欢迎评论~","emoji":["https://unpkg.com/@waline/emojis@1.1.0/qq"],"requiredMeta":["nick","mail"],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/2025/07/22/025-reinforcement-learning-ppo/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
